{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eager execution\n",
    "Tensorflow 2.0에 맞게 다시 Tensorflow를 살펴볼 필요가 있다고 느껴져서 <a href=\"https://www.tensorflow.org/?hl=ko\">Tensorflow 정식 홈페이지</a>에 나와있는 예제부터 전반적인 Tensorflow 사용법을 먼저 익히는 Post가 된다.  \n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 필요한 Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import os\n",
    "import time\n",
    "import cProfile\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.client import device_lib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eager execution\n",
    "Code 참조: <a href=\"https://www.tensorflow.org/guide/eager\">Tensorflow Core Eager execution</a>\n",
    "<br>\n",
    "\n",
    "#### What is Eager execution\n",
    "**Eager execution**은 imperative programing(명령형 프로그래밍) environment이다.  \n",
    "즉, Graph를 Define하고 RUN하기 전에 계산된 Value를 확인할 수 있는 환경인 것 이다.  \n",
    "Pytorch와 Tensorflow를 비교한 Post <a href=\"https://wjddyd66.github.io/pytorch/Pytorch-Basic/\">Pytorch-Basic</a>를 살펴보게 되면 Tensorflow 는 static Graph(Define and RUN), Pytorch는 dynamic Graph(Define by RUN)라고 설명하였다.  \n",
    "따라서 Pytorch의 경우에는 Graph를 돌리기 이전에 값을 확인하면서 사용자가 좀 더 쉽게 Debugging이 가능했던 이점을 Tensorflow 2.0에서는 Eager execution을 통하여 지원하는 모습이다.  \n",
    "\n",
    "이러한 **Eager execution**의 기능을 Tensorflow 2.0에서는 다음과 같이 설명하고 있다.  \n",
    ">1. An intuitive interface—Structure your code naturally and use Python data structures. Quickly iterate on small models and small data.\n",
    "2. Easier debugging—Call ops directly to inspect running models and test changes. Use standard Python debugging tools for immediate error reporting.\n",
    "3. Natural control flow—Use Python control flow instead of graph control flow, simplifying the specification of dynamic models.\n",
    ">\n",
    "\n",
    "1. Intuitive Interface: Python의 data Stucture를 사용가능하다.\n",
    "2. Easier debugging: Running중인 Model을 검토하거나 변경사항을 테스트하기 위하여 연산을 직접 호출하여 값을 확인 가능하다.(이전 Post에서 <code>.numpy()</code>를 통하여 값을 확인하면서 Data Pipeline을 잘 설정하였는지 확인하였다.)\n",
    "3. Natural control flow:Python의 control flow를 사용함으로 인하여 Tensorflow의 Graphic flow가 아니여서 사용자가 친숙하게 사용할 수 있다. (이전에서 Python의 Function을 설정하여 <code>@tf.function</code>을 통하여 Tensorflow Graph에서 사용가능하게 변경하거나 Tensor를 <code>.numpy()</code>로서 Numpy로 바꾸고 Function에서 연산처리를 하였다.)\n",
    "\n",
    "기본적으로 Tensorflow 2.0의 eager execution이 가능한지는 <code>tf.executing_eagerly()</code>로서 확인 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Easier debugging\n",
    "위에서 Running중인 Model을 검토하거나 변경사항을 테스트하기 위하여 연산을 직접 호출하여 값을 확인하는 것이 가능하다고 하였다.  \n",
    "아래 예제는 이러한 기능을 나타낸 것이다.  \n",
    "주요한 점은 이러한 Tensor를 연산하거나 값을 확인하기 위하여 기본적으로 Numpy를 사용하므로 Numpy에 관한 사전지식이 필요하다는 것 이다.  \n",
    "\n",
    "**참고사항**  \n",
    "Eager execution의 기능을 제공하는 Tensor의 종류는 EagerTensor이다.  \n",
    "이러한 Eager Tensor는 텐서플로 연산을 바로 평가(Intuitive Interface)하고 그 결과를 파이썬에게 알려주는 방식으로 동작을 변경한다.  \n",
    "또한 이렇나 EagerTensor를 확인하기 위한 <code>print</code>나 Debugging은 Gradinet를 계산하는 흐름을 방해하지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "tf.Tensor(\n",
      "[[1 2]\n",
      " [3 4]], shape=(2, 2), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[2 3]\n",
      " [4 5]], shape=(2, 2), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[ 2  6]\n",
      " [12 20]], shape=(2, 2), dtype=int32)\n",
      "[[ 2  6]\n",
      " [12 20]]\n",
      "[[1 2]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "source": [
    "# Eager Tensor 선언 및 확인\n",
    "a = tf.constant([[1, 2],\n",
    "                 [3, 4]])\n",
    "print(type(a))\n",
    "print(a)\n",
    "\n",
    "# 브로드캐스팅(Broadcasting) 지원\n",
    "b = tf.add(a, 1)\n",
    "print(b)\n",
    "\n",
    "# 연산자 오버로딩 지원\n",
    "print(a * b)\n",
    "\n",
    "# Numpy 지원\n",
    "c = np.multiply(a, b)\n",
    "print(c)\n",
    "\n",
    "# 텐서로부터 numpy 값 얻기:\n",
    "print(a.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dynamic control flow\n",
    "**Eager execution**의 가장 큰 이점은 Model이 실행하는 동안에 호스트 언어(Python)의 모든 기능을 활용할 수 있다는 것 이다.  \n",
    "아래에서는 Tensor를 Python Function안에서 Numpy로서 변경하여 연산을 수행하는 과정이다.  \n",
    "**Tensor값에 따라서 조건문을 실행하게 되고 실행중에 그 결과를 출력(Tensor와 연산을 확인하기 위하여 session을 열고 Run한 뒤 결과를 확인 안하여도 된다.)한다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "Fizz\n",
      "4\n",
      "Buzz\n",
      "Fizz\n",
      "7\n",
      "8\n",
      "Fizz\n",
      "Buzz\n",
      "11\n",
      "Fizz\n",
      "13\n",
      "14\n",
      "FizzBuzz\n"
     ]
    }
   ],
   "source": [
    "def fizzbuzz(max_num):\n",
    "    # Eager Tensor 선언\n",
    "    counter = tf.constant(0)\n",
    "    max_num = tf.convert_to_tensor(max_num)\n",
    "    # Eager Tensor를 .numpy()를 통하여 Python의 변수로서 변경\n",
    "    for num in range(1, max_num.numpy()+1):\n",
    "        # num을 Eager Tensor로서 변경\n",
    "        num = tf.constant(num)\n",
    "        # Eager Tensor를 조건문에 활용\n",
    "        if int(num % 3) == 0 and int(num % 5) == 0:\n",
    "            print('FizzBuzz')\n",
    "        elif int(num % 3) == 0:\n",
    "            print('Fizz')\n",
    "        elif int(num % 5) == 0:\n",
    "            print('Buzz')\n",
    "        else:\n",
    "            print(num.numpy())\n",
    "        counter += 1\n",
    "        \n",
    "fizzbuzz(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing gradients\n",
    "**Automatic differentiation**은 Backpropagation수행을 하여 Weight Update시에 매우 유용하다.  \n",
    "**Eager execution은 <code>tf.GradientTape</code>을 활용하여 미분을 바로 수행하게 된다.**  \n",
    "주요한 점은 tape는 Forward의 모든 연산을 담고 있고 tape를 거꾸로 돌려서 Gradient를 계산하고 폐기한다. 또한 **tf.Gradient는 one gradient만 가능하고(즉, <a href=\"https://wjddyd66.github.io/tnesorflow2.0/Tensorflow2.0(1)/\">Pix2Pix</a>에서는 Generator, Discriminator 2개의 Model이 존재하므로 tf.Gradient 2개 선언) 이를 어길시 runtime error가 발생한다.**  \n",
    "\n",
    "위의 해결방안으로서 <code>persistent=True</code>의 Option을 주어서 여러번 tape를 호출하고 tape를 삭제하는 방법으로 구성해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n",
      "tf.Tensor([[6.]], shape=(1, 1), dtype=float32)\n",
      "tf.Tensor([[108.]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Tensor 선언\n",
    "w = tf.Variable([[1.0]])\n",
    "\n",
    "# Model및 Gradient선언\n",
    "with tf.GradientTape() as tape:\n",
    "    loss = w * w\n",
    "\n",
    "# Loss를 w에 대하여 미분\n",
    "grad = tape.gradient(loss, w)\n",
    "# 결과 확인\n",
    "print(grad)\n",
    "\n",
    "# Tensor 선언\n",
    "w = tf.Variable([[3.0]])\n",
    "\n",
    "# Model및 Gradient선언\n",
    "# persistent=True Option 없을 시 Gradinet 1번 밖에 계산 안됨\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    tape.watch(w)\n",
    "    loss = w * w\n",
    "    loss2 = loss * loss\n",
    "    \n",
    "\n",
    "# Gradient 2번 호출\n",
    "grad = tape.gradient(loss, w)\n",
    "grad2 = tape.gradient(loss2, w)\n",
    "\n",
    "# 결과 확인\n",
    "print(grad)\n",
    "print(grad2)\n",
    "\n",
    "# Tape 삭제\n",
    "del tape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train a model\n",
    "**Eager execution**을 사용하여 Model을 Build하고 Training하기 전에 결과를 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits:  [[-0.00472712  0.00080639 -0.00118288  0.03038134  0.02856171 -0.01731389\n",
      "  -0.00965177 -0.03106915  0.01488576  0.03678003]]\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 다운로드\n",
    "(mnist_images, mnist_labels), _ = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# tf.data.Dataset Object로 변환 keras.layers.Conv2D를 위하여 tf.newaxis로서 Dimension 추가\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (tf.cast(mnist_images[...,tf.newaxis]/255, tf.float32),\n",
    "     tf.cast(mnist_labels,tf.int64)))\n",
    "\n",
    "# Shuffle 및 batch 처리\n",
    "dataset = dataset.shuffle(1000).batch(32)\n",
    "\n",
    "# Model 선언\n",
    "mnist_model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Conv2D(16,[3,3], activation='relu',\n",
    "                         input_shape=(None, None, 1)),\n",
    "  tf.keras.layers.Conv2D(16,[3,3], activation='relu'),\n",
    "  tf.keras.layers.GlobalAveragePooling2D(),\n",
    "  tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "'''\n",
    "Eager Execution을 통하여 Model 결과 확인\n",
    "위에서 Dataset Pipeline으로서 Batch를 32로서 선언하였기 때문에 하나의 결과를 확인하기 위해서는\n",
    "다음과 같이 images[0:1]로서 하나의 Image만은 Indexing하여 사용하여야 한다.\n",
    "'''\n",
    "for images,labels in dataset.take(1):\n",
    "    print(\"Logits: \", mnist_model(images[0:1]).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Customizing Training Step\n",
    "이전 <a href=\"https://wjddyd66.github.io/categories/#keras\">Keras 관련 Post</a>에서는 <code>model.fit()</code>을 통하여 Training Loop를 작동시켰다.  \n",
    "하지만 아래 Code는 Tensorflow의 **tf.GradientTape**를 통하여 전체적인 Training과정을 선언하고 원하는 형태로 Customizing을 실시한다.  \n",
    "최종적으로 BackPropagation을 실시하기 위한 과정은 다음과 같다.  \n",
    "\n",
    "1. Optimizer(<code>tf.keras.optimizers.Adam()</code>), LossFunction(<code>tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)</code>) 선언\n",
    "2. Model 결과와 Label LossFunction 계산(<code>loss_object(labels, logits)</code>)\n",
    "3. Gradient 계산(<code>tape.gradient(loss_value, mnist_model.trainable_variables)</code>)\n",
    "4. Weight Update(<code>optimizer.apply_gradients(zip(grads, mnist_model.trainable_variables))</code>)\n",
    "\n",
    "<br>\n",
    "\n",
    "**<code>tf.debugging.assert_equal</code>**: 2개의 값이 같은지 비교하는 것 이다. Python의 assert와 같은 기능을 Tensorflow Training과정에서 수행한다고 생각하면 된다.  \n",
    "참조: <a href=\"https://www.tensorflow.org/api_docs/python/tf/debugging\">tf.debugging 사용법</a><br>\n",
    "참조: <a href=\"https://www.tensorflow.org/api_docs/python/tf/debugging/assert_equal?version=stable\">tf.debugging.assert_equal 사용법</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 finished\n",
      "Epoch 1 finished\n",
      "Epoch 2 finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss [entropy]')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Optimizer 선언\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "# Loss Function 선언\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "# 결과 확인을 위한 History 선언\n",
    "loss_history = []\n",
    "\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = mnist_model(images, training=True)\n",
    "        \n",
    "        # Add asserts to check the shape of the output.\n",
    "        tf.debugging.assert_equal(logits.shape, (32, 10))\n",
    "        loss_value = loss_object(labels, logits)\n",
    "        \n",
    "    loss_history.append(loss_value.numpy().mean())\n",
    "    # 하나의 Backpropagation의 Set라고 생각하여도 된다. Gradient를 적용한 뒤 Backpropagation을 진행한다.\n",
    "    # 주요한점은 mnist_model.trainable_variables의 변수들을 Training하는 것 이므로 Model을 선언할 때\n",
    "    # Weight가 Update되지 않게 선언하면 Weight는 Training되지 않는다.\n",
    "    grads = tape.gradient(loss_value, mnist_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, mnist_model.trainable_variables))\n",
    "\n",
    "# Model Training\n",
    "def train(epochs):\n",
    "    for epoch in range(epochs):\n",
    "        for (batch, (images, labels)) in enumerate(dataset):\n",
    "            train_step(images, labels)\n",
    "        print ('Epoch {} finished'.format(epoch))        \n",
    "train(epochs = 3)\n",
    "\n",
    "# Loss History 시각화\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel('Batch #')\n",
    "plt.ylabel('Loss [entropy]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.Variable\n",
    "Tensorflow 1.x에서 Keras를 활용하지 않으면 하나하나의 Weight와 x, y를 <code>tf.placehoder()</code>와 같이 선언하였다.  \n",
    "이러한 변수는 목적에 따라서 placeholder or constant ... 등과 같이 나타내였다.  \n",
    "Tensorflow 2.0에서는 변수를 변수형 Tensor(<code>tf.Variable</code>), 상수형 Tensor로서 통일하였다.(개인적으로 가장 편리했던 점 중 하나이다.)  \n",
    "위의 2개의 차이점은 매우 간단하다.\n",
    "- 변수형 Tensor: Training 도중에 Update되는 값\n",
    "- 상수형 Tensor: Training 도중에 Update되지 않는 값\n",
    "\n",
    "이러한 tf.Variable은 Function을 제공하거나 <code>tf.device()</code>를 통하여 dtype에 호환되는 가장 빠른 장치에 위치하게 된다.(만약 GPU가 사용가능하면 대부분의 변수들이 GPU에 자동적으로 위치하게 된다.)  \n",
    "\n",
    "**tf.Variable에 적용할 수 있는 함수는 매우 많기때문에 링크로서 남겨둡니다.**  \n",
    "참조: <a href=\"https://www.tensorflow.org/api_docs/python/tf/Variable?version=stable\">tf.Variable 사용법</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 9664270224173875647\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 9037400715621209074\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 3306291200\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 7436821505498120560\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1050 Ti with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 4826601455677778066\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      "]\n",
      "\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "1.0\n",
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.0>\n",
      "read_vlaue 사용 tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "numpy 사용 1.0\n",
      "12\n",
      "<class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n"
     ]
    }
   ],
   "source": [
    "# 사용가능한 Device 출력\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "# tf.Variable 선언\n",
    "my_variable = tf.Variable(tf.zeros([1., 2., 3.]))\n",
    "\n",
    "print()\n",
    "# Variable GPU에 할당\n",
    "with tf.device(\"/gpu:0\"):\n",
    "    v = tf.Variable(tf.zeros([10, 10]))\n",
    "\n",
    "# tf.Variable 선언 및 연산 확인\n",
    "v = tf.Variable(0.0)\n",
    "w = v + 1\n",
    "print(type(w))\n",
    "print(w.numpy())\n",
    "\n",
    "# tf.Variable Function 적용\n",
    "v = tf.Variable(0.0)\n",
    "v.assign_add(1)\n",
    "print(v)\n",
    "\n",
    "# read_value를 사용하여 현재값 확인하기\n",
    "v = tf.Variable(0.0)\n",
    "v.assign_add(1)\n",
    "print('read_vlaue 사용',v.read_value())\n",
    "print('numpy 사용',v.numpy())\n",
    "\n",
    "# 변수 추적\n",
    "class MyModuleOne(tf.Module):\n",
    "    def __init__(self):\n",
    "        self.v0 = tf.Variable(1.0)\n",
    "        self.vs = [tf.Variable(x) for x in range(10)]\n",
    "    \n",
    "class MyOtherModule(tf.Module):\n",
    "    def __init__(self):\n",
    "        self.m = MyModuleOne()\n",
    "        self.v = tf.Variable(10.0)\n",
    "    \n",
    "m = MyOtherModule()\n",
    "print(len(m.variables))  # 12 = 1(self.v0) + 10(self.vs) + 1(self.v)\n",
    "\n",
    "\n",
    "# 변수형 Tensor(tf.Variable)\n",
    "s = tf.Variable(1.0)\n",
    "print(type(s))\n",
    "\n",
    "# 상수형 Tensor(Eager Tensor)\n",
    "s2 = tf.ones(2,1)\n",
    "print(type(s2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GradientTape2\n",
    "위에서 <code>tf.GradientTape</code>을 활용하여 미분을 바로 수행, Weight Update가 가능한 변수형 텐서, 불가능한 상수형 Tensor가 존재한다고 하였다.  \n",
    "만약 **상수형 텐서에 대해 미분하고 싶으면 <code>tape.watch()</code>함수를 사용하여 상수형 텐서를 변수형 텐서처럼 바꾸어야 한다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Variable Type  <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
      "tf.constant Type  <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "1.0\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# 변수형 Tensor\n",
    "x = tf.Variable(tf.constant(1.0))\n",
    "# 상수형 Tensor\n",
    "a = tf.constant(1.0)\n",
    "print('tf.Variable Type ',type(x))\n",
    "print('tf.constant Type ',type(a))\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(a)\n",
    "    y = tf.multiply(a, x)\n",
    "\n",
    "gradient = tape.gradient(y, a) \n",
    "print(gradient.numpy())\n",
    "\n",
    "# 아래 처럼 상수형 Tensor에 대해서 미분을 시도하면 Error 발생\n",
    "with tf.GradientTape() as tape:\n",
    "    y = tf.multiply(a, x)\n",
    "    \n",
    "gradient = tape.gradient(y, a) \n",
    "print(gradient is None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables and optimizer\n",
    "Tensorflow 2.0에서는 Keras를 활용하여 Model을 정의하게 된다.  \n",
    "아래 Code는 Keras에서 제공하는 Model을 선언하는 것이 아닌 사용자가 원하는 Model을 선언하는 방법이고 다음과 같은 순서로 진행된다.  \n",
    "**위에서 설명한 tf.Variable로서 선언하게 되면 Tensorflow의 1.x에서의 변수와 같이 Backpropagation을 쉽게 진행할 수 있다.**  \n",
    "\n",
    "1. Model 선언(tf.keras.Model을 상속받는다.)\n",
    "2. Optimizer, LossFunction선언\n",
    "3. Model에서 tf.variable로 선언한 변수를 Weight Update(<code>optimizer.apply_gradients(zip(grads, [model.W, model.B]))</code>)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "초기 손실: 69.547\n",
      "스텝 000에서 손실: 66.818\n",
      "스텝 050에서 손실: 9.644\n",
      "스텝 100에서 손실: 2.150\n",
      "스텝 150에서 손실: 1.167\n",
      "스텝 200에서 손실: 1.038\n",
      "스텝 250에서 손실: 1.021\n",
      "최종 손실: 1.019\n",
      "W = 3.035815715789795, B = 2.001500368118286\n",
      "Model output(input = 1, Model=3x+2+noise) 5.0373163\n"
     ]
    }
   ],
   "source": [
    "class Model(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.W = tf.Variable(5., name='weight')\n",
    "        self.B = tf.Variable(10., name='bias')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return inputs * self.W + self.B\n",
    "\n",
    "# Dataset은 대략 y=3x+2 로서 구성\n",
    "NUM_EXAMPLES = 2000\n",
    "training_inputs = tf.random.normal([NUM_EXAMPLES])\n",
    "noise = tf.random.normal([NUM_EXAMPLES])\n",
    "training_outputs = training_inputs * 3 + 2 + noise\n",
    "\n",
    "# 최적화할 손실함수\n",
    "def loss(model, inputs, targets):\n",
    "    error = model(inputs) - targets\n",
    "    return tf.reduce_mean(tf.square(error))\n",
    "\n",
    "def grad(model, inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(model, inputs, targets)\n",
    "    return tape.gradient(loss_value, [model.W, model.B])\n",
    "\n",
    "# 정의:\n",
    "# 1. 모델\n",
    "# 2. 모델 파라미터에 대한 손실 함수의 미분\n",
    "# 3. 미분에 기초한 변수 업데이트 전략\n",
    "model = Model()\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "print(\"초기 손실: {:.3f}\".format(loss(model, training_inputs, training_outputs)))\n",
    "\n",
    "# 반복 훈련\n",
    "for i in range(300):\n",
    "    grads = grad(model, training_inputs, training_outputs)\n",
    "    optimizer.apply_gradients(zip(grads, [model.W, model.B]))\n",
    "    \n",
    "    if i % 50 == 0:\n",
    "        print(\"스텝 {:03d}에서 손실: {:.3f}\".format(i, loss(model, training_inputs, training_outputs)))\n",
    "\n",
    "print(\"최종 손실: {:.3f}\".format(loss(model, training_inputs, training_outputs)))\n",
    "print(\"W = {}, B = {}\".format(model.W.numpy(), model.B.numpy()))\n",
    "\n",
    "print('Model output(input = 1, Model=3x+2+noise)',model(1.).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### object-based saving\n",
    "사용자가 Customizing한 Model또한 tf.keras.Model을 상속받아 구현한 Model이므로 Training된 Model의 Wieghts(<code>tf.Variable</code>) 저장하고 불러올 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Training Model(y=3x+2+noise) 15.0\n",
      "Training Model(y=3x+2+noise) 5.0373163\n",
      "checkpoint\t\t     weights.data-00000-of-00002  weights.index\r\n",
      "weights.data-00000-of-00001  weights.data-00001-of-00002\r\n"
     ]
    }
   ],
   "source": [
    "# Customizing Model의 Weights(tf.Variable) 저장 및 불러오기\n",
    "model.save_weights('./Model/weights')\n",
    "model = Model()\n",
    "print('Not Training Model(y=3x+2+noise)',model(1.).numpy())\n",
    "model.load_weights('./Model/weights')\n",
    "print('Training Model(y=3x+2+noise)',model(1.).numpy())\n",
    "!ls ./Model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Object-oriented metrics\n",
    "<code>tf.keras.metrics</code>는 하나의 객체로서 사용자가 지정한 형태로의 지표로서 출력한다.  \n",
    "이전 Keras에서는 <code>model.compile()</code>의 Option중 하나로서 사용하였고 아래 Code는 Customizing한 Model에서 사용하는 방식이다.\n",
    "참조: <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/metrics\">tf.keras.metrics 사용법</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "초기 손실: 69.547\n",
      "스텝 000에서 손실: 66.818\n",
      "스텝 050에서 손실: 9.644\n",
      "스텝 100에서 손실: 2.150\n",
      "스텝 150에서 손실: 1.167\n",
      "스텝 200에서 손실: 1.038\n",
      "스텝 250에서 손실: 1.021\n",
      "최종 손실: 1.019\n",
      "W = 3.035815715789795, B = 2.001500368118286\n",
      "Model output(input = 1, Model=3x+2+noise) 5.0373163\n",
      "tf.Tensor(6.5259175, shape=(), dtype=float32)\n",
      "6.5259175\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "m = tf.keras.metrics.Mean(\"loss\")\n",
    "\n",
    "print(\"초기 손실: {:.3f}\".format(loss(model, training_inputs, training_outputs)))\n",
    "\n",
    "# 반복 훈련\n",
    "for i in range(300):\n",
    "    grads = grad(model, training_inputs, training_outputs)\n",
    "    optimizer.apply_gradients(zip(grads, [model.W, model.B]))\n",
    "    m(loss(model, training_inputs, training_outputs))\n",
    "    if i % 50 == 0:\n",
    "        print(\"스텝 {:03d}에서 손실: {:.3f}\".format(i, loss(model, training_inputs, training_outputs)))\n",
    "\n",
    "print(\"최종 손실: {:.3f}\".format(loss(model, training_inputs, training_outputs)))\n",
    "print(\"W = {}, B = {}\".format(model.W.numpy(), model.B.numpy()))\n",
    "\n",
    "print('Model output(input = 1, Model=3x+2+noise)',model(1.).numpy())\n",
    "\n",
    "\n",
    "print(m.result())\n",
    "print(m.result().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summaries and Tensorboard\n",
    "<code>tf.summary</code>를 통하여 Tensorboard를 통하여 Visualization할 변수를 저장한다.  \n",
    "확인하게 되면 events.out.tfevents File들이 선언되고 Tensorboard의 출력을 살펴보면 다음과 같디 Metrics의 결과가 저장되는 것을 살펴볼 수 있다.  \n",
    "<div><img src=\"https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/Tensorflow/30.png\" height=\"100%\" width=\"100%\" /></div><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "초기 손실: 69.547\n",
      "스텝 000에서 손실: 66.818\n",
      "스텝 050에서 손실: 9.644\n",
      "스텝 100에서 손실: 2.150\n",
      "스텝 150에서 손실: 1.167\n",
      "스텝 200에서 손실: 1.038\n",
      "스텝 250에서 손실: 1.021\n",
      ".\n",
      "..\n",
      "events.out.tfevents.1577333357.jyhwang-XPS-15-9570.32222.793404.v2\n",
      "events.out.tfevents.1577337029.jyhwang-XPS-15-9570.3949.685214.v2\n",
      "events.out.tfevents.1577337741.jyhwang-XPS-15-9570.7379.685521.v2\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.0.0 at http://localhost:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "m = tf.keras.metrics.Mean(\"loss\")\n",
    "logdir = \"./tb/\"\n",
    "writer = tf.summary.create_file_writer(logdir)\n",
    "\n",
    "\n",
    "\n",
    "print(\"초기 손실: {:.3f}\".format(loss(model, training_inputs, training_outputs)))\n",
    "\n",
    "# 반복 훈련\n",
    "with writer.as_default():\n",
    "    for i in range(300):\n",
    "        grads = grad(model, training_inputs, training_outputs)\n",
    "        optimizer.apply_gradients(zip(grads, [model.W, model.B]))\n",
    "        m(loss(model, training_inputs, training_outputs))\n",
    "        if i % 50 == 0:\n",
    "            print(\"스텝 {:03d}에서 손실: {:.3f}\".format(i, loss(model, training_inputs, training_outputs)))\n",
    "            tf.summary.scalar('loss', m.result().numpy(), step=i)\n",
    "\n",
    "!ls -a {logdir}\n",
    "!tensorboard --logdir={logdir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Gradients\n",
    "Gradinet를 Tensorflow에서 제공하는 것이 아닌 <code>@tf.custom_gradient</code>의 Decorator를 통하여 나타낼 수 있다.  \n",
    "중요한 점은 2가지 이다.  \n",
    "- 처음 Function은 Forward를 의미한다.\n",
    "- 두번째 Function은 Gradient를 Customizing할 수 있다. 이러한 Function은 (dy)의 값을 입력 받는다.\n",
    "\n",
    "식으로서 나타내면 다음과 같다.  \n",
    "**Not Customize Gradient**  \n",
    "처음은 <code>tf.GradientTape()</code>를 Customizing하지 않는다.  \n",
    "그러면 식은 다음과 같이 정의된다.  \n",
    "<p>$$y=3x^2, \\frac{\\partial y}{\\partial x} = 6x$$</p>\n",
    "따라서 x의 값에 100을 넣은 600의 값이 출력될 것이다.  \n",
    "\n",
    "**Customize Gradient**  \n",
    "<code>@tf.custom_graident</code>를 통하여 Gradient를 Customizing 한다.  \n",
    "그러면 식은 다음과 같이 정의된다.  \n",
    "<p>$$\\text{Forward:} y=3x^2$$</p>\n",
    "<code>grad(dy)</code>에서 dy를 Argument를 입력으로 받아서 Gradient를 계산한다.  \n",
    "\n",
    "**grad_example_1**  \n",
    "아래 grad_example_1은 다음과 같은 순서로 작동된다.\n",
    "- <code>tape.watch(x)</code>: 입력변수가 상수형 Tensor이므로 필요한 Code\n",
    "- <code>value = custom(x)</code>: custom Function에서 Forward를 수행한다.\n",
    "- <code>tape.gradient(value, x)</code>: custom Function에서 grad를 수행한다. dy의 값은 <span>$dy = \\frac{\\partial y}{\\partial y} = 1$</span>이다.\n",
    "\n",
    "**grad_example_2**  \n",
    "아래 grad_example_2은 다음과 같은 순서로 작동된다.\n",
    "- <code>tape.watch(x)</code>: 입력변수가 상수형 Tensor이므로 필요한 Code\n",
    "- <code>value = custom(x)</code>: custom Function에서 Forward를 수행한다.\n",
    "- <code>value = value*value</code>: <span>$y = y^2$</span>\n",
    "- <code>tape.gradient(value, x)</code>: custom Function에서 grad를 수행한다. dy의 값은 <span>$dy = \\frac{\\partial y^2}{\\partial y} = 2y = 2*300 = 600$</span>이다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = tf.constant(100.)\n",
      "Y = 3x^2\n",
      "\n",
      "Not Customize Gradient\n",
      "600.0\n",
      "\n",
      "Custom_gradient\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Start\n",
      "Forward Type:  <class 'tensorflow.python.framework.ops.EagerTensor'> Forward value 300.0\n",
      "Value Type:  <class 'tensorflow.python.framework.ops.EagerTensor'> Value value 300.0\n",
      "Grad Start\n",
      "dy Type:  <class 'tensorflow.python.framework.ops.EagerTensor'> dy value 1.0\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "300.0\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Start\n",
      "Forward Type:  <class 'tensorflow.python.framework.ops.EagerTensor'> Forward value 300.0\n",
      "Value Type:  <class 'tensorflow.python.framework.ops.EagerTensor'> Value value 90000.0\n",
      "Grad Start\n",
      "dy Type:  <class 'tensorflow.python.framework.ops.EagerTensor'> dy value 600.0\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "180000.0\n"
     ]
    }
   ],
   "source": [
    "print('X = tf.constant(100.)')\n",
    "print('Y = 3x^2')\n",
    "print()\n",
    "print('Not Customize Gradient')\n",
    "\n",
    "x = tf.constant(100.)\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    y = 3*x*x\n",
    "    print(tape.gradient(y,x).numpy())\n",
    "\n",
    "print()\n",
    "print('Custom_gradient')\n",
    "\n",
    "@tf.custom_gradient\n",
    "def custom(x):\n",
    "    print('Start')\n",
    "    forward = 3*x*x\n",
    "    print('Forward Type: ', type(forward),'Forward value', forward.numpy())\n",
    "    \n",
    "    def grad(dy):\n",
    "        print('Grad Start')\n",
    "        print('dy Type: ', type(dy),'dy value', dy.numpy())\n",
    "        return dy * forward\n",
    "    return forward, grad\n",
    "\n",
    "def grad_example_1(x):\n",
    "    print(type(x))\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(x)\n",
    "        value = custom(x)\n",
    "        #value = value*value\n",
    "        print('Value Type: ', type(value),'Value value', value.numpy())\n",
    "    return tape.gradient(value, x)\n",
    "\n",
    "def grad_example_2(x):\n",
    "    print(type(x))\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(x)\n",
    "        value = log1pexp(x)\n",
    "        value = value*value\n",
    "        print('Value Type: ', type(value),'Value value', value.numpy())\n",
    "    return tape.gradient(value, x)\n",
    "\n",
    "grad1 = grad_example_1(tf.constant(10.))\n",
    "print(type(grad1))\n",
    "print(grad1.numpy())\n",
    "\n",
    "grad2 = grad_example_2(tf.constant(10.))\n",
    "print(type(grad2))\n",
    "print(grad2.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance\n",
    "Eager Execution을 수행할때 CPU와 GPU속도 차이를 계산한다.\n",
    "각각의 Device는 <code>tf.device()</code>로서 선언할 수 있고 여러개의 CPU나 GPU를 사용하는 경우 각각의 GPU와 CPU에게 작업을 할당할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1000) 크기 행렬을 자기 자신과 200번 곱했을 때 걸리는 시간:\n",
      "CPU: 0.8723616600036621 초\n",
      "GPU: 0.24282073974609375 초\n"
     ]
    }
   ],
   "source": [
    "def measure(x, steps):\n",
    "    tf.matmul(x, x)\n",
    "    start = time.time()\n",
    "    for i in range(steps):\n",
    "        x = tf.matmul(x, x)\n",
    "    _ = x.numpy()\n",
    "    end = time.time()\n",
    "    return end - start\n",
    "\n",
    "shape = (1000, 1000)\n",
    "steps = 200\n",
    "print(\"{} 크기 행렬을 자기 자신과 {}번 곱했을 때 걸리는 시간:\".format(shape, steps))\n",
    "\n",
    "# CPU에서 실행:\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    print(\"CPU: {} 초\".format(measure(tf.random.normal(shape), steps)))\n",
    "\n",
    "# GPU에서 실행이 가능하다면\n",
    "if tf.test.is_gpu_available():\n",
    "    # GPU 처음 Device를 사용하여 계산\n",
    "    with tf.device(\"/gpu:0\"):\n",
    "        print(\"GPU: {} 초\".format(measure(tf.random.normal(shape), steps)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
