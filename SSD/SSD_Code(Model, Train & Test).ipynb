{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SSD 구현 (Model, Train & Test)\n",
    "코드 참조: <a href=\"https://github.com/ChunML/ssd-tf2\">ChunML GitHub</a><br>\n",
    "위의 Code를 참조하여 수정한 SSD의 현재 Directory의 구조는 다음과 같습니다.(위의 Code는 SSD 300,512를 둘 다 구현하였지만, 현재 Code는 논문에서 예제로 보여준 SSD300을 고정으로서 사용하였습니다.)  \n",
    "<br>\n",
    "\n",
    "### Model\n",
    "Model에 대한 전반적인 구조는 VGG16(Imagenet)으로서 Transfer Learning된 Model을 사용하여 구조를 형성하게 된다.  \n",
    "따라서 VGG16의 Network의 Architecture와 SSD가 어떻게 연결되는지 알아보면 다음과 같다.  \n",
    "<div><img src=\"https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/Tensorflow/78.png\" height=\"100%\" width=\"100%\" /></div><br>\n",
    "<br>\n",
    "\n",
    "#### layer.py\n",
    "기본적으로 Network를 구성하는 Layer들을 정의하는 곳 이다.  \n",
    "1) create_vgg16_layers()  \n",
    "- vgg16_conv4: PreTrainning된 VGG16 Model에서 Conv5_3 Layer까지 지정하는 곳 이다.  \n",
    "- vgg16_conv7: PreTrainning된 VGG16 Model에서 FcLayer6(Dense1), FcLaye7(Dense2)를 통과한 Layer를 지정하는 곳 이다. \n",
    "\n",
    "2) create_extra_layers()  \n",
    "논문에서 다양한 Scale의 FeatureMap에서 ObjectDetection을 하기위한 Extra Feature Layers를 선언하는 곳 이다.  \n",
    "\n",
    "3) conf_head_layers()\n",
    "Object의 Class를 확인하기 위한 Layer이다.  \n",
    "각각은 Default Box의 개수 * Class로서 Dimension을 이루고 논문과 같이 Convolution의 Filter의 Size는 3x3이다.  \n",
    "\n",
    "4) create_loc_head_layers()  \n",
    "Object의 Localization을 확인하기 위한 Layer이다.  \n",
    "각각은 Default Box의 개수 * (cx,cy,w,h) Dimension을 이루고 논문과 같이 Convolution의 Filter의 Size는 3x3이다.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as layers\n",
    "from tensorflow.keras import Sequential\n",
    "\n",
    "\n",
    "def create_vgg16_layers():\n",
    "    vgg16_conv4 = [\n",
    "        layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "        layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "        layers.MaxPool2D(2, 2, padding='same'),\n",
    "\n",
    "        layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "        layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "        layers.MaxPool2D(2, 2, padding='same'),\n",
    "\n",
    "        layers.Conv2D(256, 3, padding='same', activation='relu'),\n",
    "        layers.Conv2D(256, 3, padding='same', activation='relu'),\n",
    "        layers.Conv2D(256, 3, padding='same', activation='relu'),\n",
    "        layers.MaxPool2D(2, 2, padding='same'),\n",
    "\n",
    "        layers.Conv2D(512, 3, padding='same', activation='relu'),\n",
    "        layers.Conv2D(512, 3, padding='same', activation='relu'),\n",
    "        layers.Conv2D(512, 3, padding='same', activation='relu'),\n",
    "        layers.MaxPool2D(2, 2, padding='same'),\n",
    "\n",
    "        layers.Conv2D(512, 3, padding='same', activation='relu'),\n",
    "        layers.Conv2D(512, 3, padding='same', activation='relu'),\n",
    "        layers.Conv2D(512, 3, padding='same', activation='relu'),\n",
    "    ]\n",
    "\n",
    "    x = layers.Input(shape=[None, None, 3])\n",
    "    out = x\n",
    "    for layer in vgg16_conv4:\n",
    "        out = layer(out)\n",
    "    # PreTrainning된 VGG16 Model에서 Conv5_3 Layer까지 지정하는 곳 이다.  \n",
    "    vgg16_conv4 = tf.keras.Model(x, out)\n",
    "    # PreTrainning된 VGG16 Model에서 FcLayer6(Dense1), FcLaye7(Dense2)를 통과한 Layer를 지정하는 곳 이다.\n",
    "    vgg16_conv7 = [\n",
    "        # Difference from original VGG16:\n",
    "        # 5th maxpool layer has kernel size = 3 and stride = 1\n",
    "        layers.MaxPool2D(3, 1, padding='same'),\n",
    "        # atrous conv2d for 6th block\n",
    "        layers.Conv2D(1024, 3, padding='same',\n",
    "                      dilation_rate=6, activation='relu'),\n",
    "        layers.Conv2D(1024, 1, padding='same', activation='relu'),\n",
    "    ]\n",
    "\n",
    "    x = layers.Input(shape=[None, None, 512])\n",
    "    out = x\n",
    "    for layer in vgg16_conv7:\n",
    "        out = layer(out)\n",
    "\n",
    "    vgg16_conv7 = tf.keras.Model(x, out)\n",
    "    return vgg16_conv4, vgg16_conv7\n",
    "\n",
    "# 논문에서 다양한 Scale의 FeatureMap에서 ObjectDetection을 하기위한 \n",
    "# Extra Feature Layers를 선언하는 곳 이다.\n",
    "def create_extra_layers():\n",
    "    \"\"\" Create extra layers\n",
    "        8th to 11th blocks\n",
    "    \"\"\"\n",
    "    extra_layers = [\n",
    "        # 8th block output shape: B, 512, 10, 10\n",
    "        Sequential([\n",
    "            layers.Conv2D(256, 1, activation='relu'),\n",
    "            layers.Conv2D(512, 3, strides=2, padding='same',\n",
    "                          activation='relu'),\n",
    "        ]),\n",
    "        # 9th block output shape: B, 256, 5, 5\n",
    "        Sequential([\n",
    "            layers.Conv2D(128, 1, activation='relu'),\n",
    "            layers.Conv2D(256, 3, strides=2, padding='same',\n",
    "                          activation='relu'),\n",
    "        ]),\n",
    "        # 10th block output shape: B, 256, 3, 3\n",
    "        Sequential([\n",
    "            layers.Conv2D(128, 1, activation='relu'),\n",
    "            layers.Conv2D(256, 3, activation='relu'),\n",
    "        ]),\n",
    "        # 11th block output shape: B, 256, 1, 1\n",
    "        Sequential([\n",
    "            layers.Conv2D(128, 1, activation='relu'),\n",
    "            layers.Conv2D(256, 3, activation='relu'),\n",
    "        ])\n",
    "    ]\n",
    "\n",
    "    return extra_layers\n",
    "\n",
    "# Object의 Class를 확인하기 위한 Layer이다.  \n",
    "# 각각은 Default Box의 개수 * Class로서 Dimension을 이루고 \n",
    "# 논문과 같이 Convolution의 Filter의 Size는 3x3이다.\n",
    "def create_conf_head_layers(num_classes):\n",
    "    \"\"\" Create layers for classification\n",
    "    \"\"\"\n",
    "    conf_head_layers = [\n",
    "        layers.Conv2D(4 * num_classes, kernel_size=3,\n",
    "                      padding='same'),  # for 4th block\n",
    "        layers.Conv2D(6 * num_classes, kernel_size=3,\n",
    "                      padding='same'),  # for 7th block\n",
    "        layers.Conv2D(6 * num_classes, kernel_size=3,\n",
    "                      padding='same'),  # for 8th block\n",
    "        layers.Conv2D(6 * num_classes, kernel_size=3,\n",
    "                      padding='same'),  # for 9th block\n",
    "        layers.Conv2D(4 * num_classes, kernel_size=3,\n",
    "                      padding='same'),  # for 10th block\n",
    "        layers.Conv2D(4 * num_classes, kernel_size=3,\n",
    "                      padding='same')  # for 11th block\n",
    "    ]\n",
    "\n",
    "    return conf_head_layers\n",
    "\n",
    "# Object의 Localization을 확인하기 위한 Layer이다.  \n",
    "# 각각은 Default Box의 개수 * (cx,cy,w,h) Dimension을 이루고 \n",
    "# 논문과 같이 Convolution의 Filter의 Size는 3x3이다.\n",
    "def create_loc_head_layers():\n",
    "    \"\"\" Create layers for regression\n",
    "    \"\"\"\n",
    "    loc_head_layers = [\n",
    "        layers.Conv2D(4 * 4, kernel_size=3, padding='same'),\n",
    "        layers.Conv2D(6 * 4, kernel_size=3, padding='same'),\n",
    "        layers.Conv2D(6 * 4, kernel_size=3, padding='same'),\n",
    "        layers.Conv2D(6 * 4, kernel_size=3, padding='same'),\n",
    "        layers.Conv2D(4 * 4, kernel_size=3, padding='same'),\n",
    "        layers.Conv2D(4 * 4, kernel_size=3, padding='same')\n",
    "    ]\n",
    "\n",
    "    return loc_head_layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### network.py\n",
    "위에서 선언한 Layers.py를 통하여 SSD Network를 구성한다.  \n",
    "기본적으로 layers.py에서 다 선언한 것을 이어주는 것이 대부분이다.  \n",
    "조금 중요하게 살펴보아야 할 부분을 살펴보면 다음과 같다.  \n",
    "\n",
    "```python\n",
    "fc1_weights, fc1_biases = origin_vgg.get_layer(index=-3).get_weights()\n",
    "fc2_weights, fc2_biases = origin_vgg.get_layer(index=-2).get_weights()\n",
    "\n",
    "conv6_weights = np.random.choice(np.reshape(fc1_weights, (-1,)), (3, 3, 512, 1024))\n",
    "conv6_biases = np.random.choice(fc1_biases, (1024,))\n",
    "\n",
    "conv7_weights = np.random.choice(np.reshape(fc2_weights, (-1,)), (1, 1, 1024, 1024))\n",
    "conv7_biases = np.random.choice(fc2_biases, (1024,))\n",
    "\n",
    "self.vgg16_conv7.get_layer(index=2).set_weights([conv6_weights, conv6_biases])\n",
    "self.vgg16_conv7.get_layer(index=3).set_weights([conv7_weights, conv7_biases])\n",
    "```\n",
    "\n",
    "VGG16의 마지막 단을 살펴보면 FC6 -> FC7 -> Softmax로서 Classify를 한다.  \n",
    "따라서 PreTraining된 VGG16 Model에서 마지막에서 3번째를 FC6 -> conv6, FC7 -> conv7로서 선언하는 것이 중요한다.  \n",
    "<br>\n",
    "\n",
    "```python\n",
    "for i in range(len(self.vgg16_conv4.layers)):\n",
    "    x = self.vgg16_conv4.get_layer(index=i)(x)\n",
    "    if i == len(self.vgg16_conv4.layers) - 5:\n",
    "        conf, loc = self.compute_heads(self.batch_norm(x), head_idx)\n",
    "```\n",
    "논문에서 살펴보면 VGG16의 모델을 Conv5_3 Layer까지 사용하나 마지막에 Detection을 하는 부분은 Conv4_3 Layer에서 가져온다.  \n",
    "따라서 위와 같은 과정을 거쳐서 FeatureMap을 생성해야 한다.  \n",
    "\n",
    "최종적인 결과로서 모든 FeaturMap을 하나로 연결(confs, locs)하여 Network의 Output이 생성된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.applications import VGG16\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from layers import create_vgg16_layers, create_extra_layers, create_conf_head_layers, create_loc_head_layers\n",
    "\n",
    "\n",
    "class SSD(Model):\n",
    "    \"\"\" Class for SSD model\n",
    "    Attributes:\n",
    "        num_classes: number of classes\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super(SSD, self).__init__()\n",
    "        self.arch = 'ssd300'\n",
    "        self.num_classes = num_classes\n",
    "        self.vgg16_conv4, self.vgg16_conv7 = create_vgg16_layers()\n",
    "        self.batch_norm = layers.BatchNormalization(\n",
    "            beta_initializer='glorot_uniform',\n",
    "            gamma_initializer='glorot_uniform'\n",
    "        )\n",
    "        self.extra_layers = creatcompute_headse_extra_layers()\n",
    "        self.conf_head_layers = create_conf_head_layers(num_classes)\n",
    "        self.loc_head_layers = create_loc_head_layers()\n",
    "\n",
    "    def compute_heads(self, x, idx):\n",
    "        \"\"\" Compute outputs of classification and regression heads\n",
    "        Args:\n",
    "            x: the input feature map\n",
    "            idx: index of the head layer\n",
    "        Returns:\n",
    "            conf: output of the idx-th classification head\n",
    "            loc: output of the idx-th regression head\n",
    "        \"\"\"\n",
    "        conf = self.conf_head_layers[idx](x)\n",
    "        conf = tf.reshape(conf, [conf.shape[0], -1, self.num_classes])\n",
    "\n",
    "        loc = self.loc_head_layers[idx](x)\n",
    "        loc = tf.reshape(loc, [loc.shape[0], -1, 4])\n",
    "\n",
    "        return conf, loc\n",
    "\n",
    "    def init_vgg16(self):\n",
    "        \"\"\" Initialize the VGG16 layers from pretrained weights\n",
    "            and the rest from scratch using xavier initializer\n",
    "        \"\"\"\n",
    "        origin_vgg = VGG16(weights='imagenet')\n",
    "        for i in range(len(self.vgg16_conv4.layers)):\n",
    "            self.vgg16_conv4.get_layer(index=i).set_weights(\n",
    "                origin_vgg.get_layer(index=i).get_weights())\n",
    "\n",
    "        fc1_weights, fc1_biases = origin_vgg.get_layer(index=-3).get_weights()\n",
    "        fc2_weights, fc2_biases = origin_vgg.get_layer(index=-2).get_weights()\n",
    "\n",
    "        conv6_weights = np.random.choice(\n",
    "            np.reshape(fc1_weights, (-1,)), (3, 3, 512, 1024))\n",
    "        conv6_biases = np.random.choice(\n",
    "            fc1_biases, (1024,))\n",
    "\n",
    "        conv7_weights = np.random.choice(\n",
    "            np.reshape(fc2_weights, (-1,)), (1, 1, 1024, 1024))\n",
    "        conv7_biases = np.random.choice(\n",
    "            fc2_biases, (1024,))\n",
    "\n",
    "        self.vgg16_conv7.get_layer(index=2).set_weights(\n",
    "            [conv6_weights, conv6_biases])\n",
    "        self.vgg16_conv7.get_layer(index=3).set_weights(\n",
    "            [conv7_weights, conv7_biases])\n",
    "\n",
    "    def call(self, x):\n",
    "        \"\"\" The forward pass\n",
    "        Args:\n",
    "            x: the input image\n",
    "        Returns:\n",
    "            confs: list of outputs of all classification heads\n",
    "            locs: list of outputs of all regression heads\n",
    "        \"\"\"\n",
    "        confs = []\n",
    "        locs = []\n",
    "        head_idx = 0\n",
    "        for i in range(len(self.vgg16_conv4.layers)):\n",
    "            x = self.vgg16_conv4.get_layer(index=i)(x)\n",
    "            if i == len(self.vgg16_conv4.layers) - 5:\n",
    "                conf, loc = self.compute_heads(self.batch_norm(x), head_idx)\n",
    "                confs.append(conf)\n",
    "                locs.append(loc)\n",
    "                head_idx += 1\n",
    "\n",
    "        x = self.vgg16_conv7(x)\n",
    "\n",
    "        conf, loc = self.compute_heads(x, head_idx)\n",
    "\n",
    "        confs.append(conf)\n",
    "        locs.append(loc)\n",
    "        head_idx += 1\n",
    "\n",
    "        for layer in self.extra_layers:\n",
    "            x = layer(x)\n",
    "            conf, loc = self.compute_heads(x, head_idx)\n",
    "            confs.append(conf)\n",
    "            locs.append(loc)\n",
    "            head_idx += 1\n",
    "\n",
    "        confs = tf.concat(confs, axis=1)\n",
    "        locs = tf.concat(locs, axis=1)\n",
    "\n",
    "        return confs, locs\n",
    "\n",
    "\n",
    "def create_ssd(num_classes, pretrained_type,\n",
    "               checkpoint_dir=None,\n",
    "               checkpoint_path=None):\n",
    "    \"\"\" Create SSD model and load pretrained weights\n",
    "    Args:\n",
    "        num_classes: number of classes\n",
    "        pretrained_type: type of pretrained weights, can be either 'VGG16' or 'ssd'\n",
    "        weight_path: path to pretrained weights\n",
    "    Returns:\n",
    "        net: the SSD model\n",
    "    \"\"\"\n",
    "    net = SSD(num_classes)\n",
    "    net(tf.random.normal((1, 512, 512, 3)))\n",
    "    if pretrained_type == 'base':\n",
    "        net.init_vgg16()\n",
    "    elif pretrained_type == 'latest':\n",
    "        try:\n",
    "            paths = [os.path.join(checkpoint_dir, path)\n",
    "                     for path in os.listdir(checkpoint_dir)]\n",
    "            latest = sorted(paths, key=os.path.getmtime)[-1]\n",
    "            net.load_weights(latest)\n",
    "        except AttributeError as e:\n",
    "            print('Please make sure there is at least one checkpoint at {}'.format(\n",
    "                checkpoint_dir))\n",
    "            print('The model will be loaded from base weights.')\n",
    "            net.init_vgg16()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            raise ValueError('Please check if checkpoint_dir is specified')\n",
    "    elif pretrained_type == 'specified':\n",
    "        if not os.path.isfile(checkpoint_path):\n",
    "            raise ValueError(\n",
    "                'Not a valid checkpoint file: {}'.format(checkpoint_path))\n",
    "\n",
    "        try:\n",
    "            net.load_weights(checkpoint_path)\n",
    "        except Exception as e:\n",
    "            raise ValueError(\n",
    "                'Please check the following\\n1./ Is the path correct: {}?\\n2./ Is the model architecture correct: {}?'.format(\n",
    "                    checkpoint_path, 'ssd300'))\n",
    "    else:\n",
    "        raise ValueError('Unknown pretrained type: {}'.format(pretrained_type))\n",
    "    return net\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loss.py\n",
    "최종적인 SSD Model의 Loss를 구하는 방법이다.  \n",
    "**hard_negative_mining()**  \n",
    "논문에서는 Image의 Pixel에서 Default Box가 Background인 것이 많아서 LossFunction에서 Positive:Negative의 비율을 High confidence기준으로 1:3으로서 뽑게 하였다.  \n",
    "따라서 Loss기준으로 Highconfidence로서 정렬하고 num_neg = num_pos * neg_ratio(=3)으로서 정의하였다.  \n",
    "<br>\n",
    "\n",
    "**class SSDLosses()**  \n",
    "실제 최종적인 Loss를 구하는 방법이다.  \n",
    "**Localization Loss**  \n",
    "<p>$$L_{loc}(x,l,g) = \\sum_{i \\in Pos}^N \\sum_{m \\in cx,cy,w,h} x_{ij}^k smooth_{L1}(l_i^m-\\hat{g}_j^m)$$</p>\n",
    "<p>$$\\hat{g}_j^{cx}=(g_j^{cx}-d_i^{cx})/d_i^w, \\hat{g}_j^{cy}=(g_j^{cy}-d_i^{cy})/d_i^h$$</p>\n",
    "<p>$$\\hat{g}_j^{w} = log(\\frac{g_j^w}{d_i^w}),  \\hat{g}_j^{h} = log(\\frac{g_j^h}{d_i^h})$$</p>\n",
    "<p>$$\n",
    "x_{ij}^p=\n",
    "\\begin{cases}\n",
    "1, & \\mbox{if } IOU > 0.5 \\mbox{ between default box i and ground true box j on class p} \\\\\n",
    "0, & \\mbox{otherwise}\n",
    "\\end{cases}\n",
    "$$</p>\n",
    "실제 Code에서도 SmoothL1(<code>smooth_l1_loss = tf.keras.losses.Huber(reduction='sum')</code>)로서 구현\n",
    "\n",
    "**Confidence Loss**  \n",
    "<p>$$L_{conf}(x,c) = -\\sum_{i \\in Pos}^N x_{ij}^p log(\\hat{c}_i^p)-\\sum_{i \\in Neg} log(\\hat{c}_i^0) \\text{,  where  } \\hat{c}_i^p = \\frac{exp(c_i^p)}{\\sum_p c_i^p}$$</p>\n",
    "COnfidence loss에서 중요한 점은 0(Background)의 값이 높기 때문에 hard_negative_mining를 적용시킨다.\n",
    "실제 Code에서도 CrossEntropy(<code>cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='sum')</code>)로서 구현\n",
    "\n",
    "**Final Loss**  \n",
    "<p>$$L(x,c,l,g) = \\frac{1}{N}(L_{conf}(x,c) + \\alpha L_{loc}(x,l,g))$$</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def hard_negative_mining(loss, gt_confs, neg_ratio=3):\n",
    "    \"\"\" Hard negative mining algorithm\n",
    "        to pick up negative examples for back-propagation\n",
    "        base on classification loss values\n",
    "    Args:\n",
    "        loss: list of classification losses of all default boxes (B, num_default)\n",
    "        gt_confs: classification targets (B, num_default)\n",
    "        neg_ratio: negative / positive ratio\n",
    "    Returns:\n",
    "        conf_loss: classification loss\n",
    "        loc_loss: regression loss\n",
    "    \"\"\"\n",
    "    # loss: B x N\n",
    "    # gt_confs: B x N\n",
    "    pos_idx = gt_confs > 0\n",
    "    num_pos = tf.reduce_sum(tf.dtypes.cast(pos_idx, tf.int32), axis=1)\n",
    "    num_neg = num_pos * neg_ratio\n",
    "\n",
    "    rank = tf.argsort(loss, axis=1, direction='DESCENDING')\n",
    "    rank = tf.argsort(rank, axis=1)\n",
    "    neg_idx = rank < tf.expand_dims(num_neg, 1)\n",
    "\n",
    "    return pos_idx, neg_idx\n",
    "\n",
    "\n",
    "class SSDLosses(object):\n",
    "    \"\"\" Class for SSD Losses\n",
    "    Attributes:\n",
    "        neg_ratio: negative / positive ratio\n",
    "        num_classes: number of classes\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, neg_ratio, num_classes):\n",
    "        self.neg_ratio = neg_ratio\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def __call__(self, confs, locs, gt_confs, gt_locs):\n",
    "        \"\"\" Compute losses for SSD\n",
    "            regression loss: smooth L1\n",
    "            classification loss: cross entropy\n",
    "        Args:\n",
    "            confs: outputs of classification heads (B, num_default, num_classes)\n",
    "            locs: outputs of regression heads (B, num_default, 4)\n",
    "            gt_confs: classification targets (B, num_default)\n",
    "            gt_locs: regression targets (B, num_default, 4)\n",
    "        Returns:\n",
    "            conf_loss: classification loss\n",
    "            loc_loss: regression loss\n",
    "        \"\"\"\n",
    "        cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=True, reduction='none')\n",
    "\n",
    "        # compute classification losses\n",
    "        # without reduction\n",
    "        temp_loss = cross_entropy(\n",
    "            gt_confs, confs)\n",
    "        pos_idx, neg_idx = hard_negative_mining(\n",
    "            temp_loss, gt_confs, self.neg_ratio)\n",
    "\n",
    "        # classification loss will consist of positive and negative examples\n",
    "\n",
    "        cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=True, reduction='sum')\n",
    "        smooth_l1_loss = tf.keras.losses.Huber(reduction='sum')\n",
    "\n",
    "        conf_loss = cross_entropy(\n",
    "            gt_confs[tf.math.logical_or(pos_idx, neg_idx)],\n",
    "            confs[tf.math.logical_or(pos_idx, neg_idx)])\n",
    "\n",
    "        # regression loss only consist of positive examples\n",
    "        loc_loss = smooth_l1_loss(\n",
    "            # tf.boolean_mask(gt_locs, pos_idx),\n",
    "            # tf.boolean_mask(locs, pos_idx))\n",
    "            gt_locs[pos_idx],\n",
    "            locs[pos_idx])\n",
    "\n",
    "        num_pos = tf.reduce_sum(tf.dtypes.cast(pos_idx, tf.float32))\n",
    "        conf_loss = conf_loss / num_pos\n",
    "        loc_loss = loc_loss / num_pos\n",
    "        return conf_loss, loc_loss\n",
    "\n",
    "\n",
    "def create_losses(neg_ratio, num_classes):\n",
    "    criterion = SSDLosses(neg_ratio, num_classes)\n",
    "\n",
    "    return criterion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Test\n",
    "\n",
    "#### train.py\n",
    "위에서 설명하였던 Utils,Dataset,Model을 활용하여 실제 SSD Model을 Trainning하는 방법이다.  \n",
    "아래와 같은 Code로서 실행 가능하고 다양한 Option은 바꿔서 실행가능하다.  \n",
    "**train.py 실행 에시**  \n",
    "<code>python train.py --batch-size 4 --gpu-id 0</code><br>\n",
    "\n",
    "**train.py Option List**  \n",
    "```code\n",
    "usage: train.py [-h] [--data-dir DATA_DIR] [--batch-size BATCH_SIZE]\n",
    "                [--num-batches NUM_BATCHES] [--neg-ratio NEG_RATIO]\n",
    "                [--initial-lr INITIAL_LR] [--momentum MOMENTUM]\n",
    "                [--weight-decay WEIGHT_DECAY] [--num-epochs NUM_EPOCHS]\n",
    "                [--checkpoint-dir CHECKPOINT_DIR]\n",
    "```\n",
    "<br>\n",
    "\n",
    "**train_step()**  \n",
    "실제 LossFunction을 계산하고 Backpropagation을 진행하는 곳 이다.  \n",
    "Paper와 동일하게 다음과 같이 Hyperparameter를 선언하였다.  \n",
    ">We fine-tune the resulting model using SGD with initial learning rate 10−3\n",
    ", 0.9 momentum, 0.0005 weight decay, and batch size 32.  \n",
    "The learning rate decay policy is slightly different for each dataset, and we will describe details later\n",
    "\n",
    "- learning rate(<code>parser.add_argument('--initial-lr', default=1e-3, type=float)</code>): 10-3\n",
    "- momentum(<code>parser.add_argument('--momentum', default=0.9, type=float)</code>): 0.9\n",
    "- weight decay(<code>parser.add_argument('--weight-decay', default=5e-4, type=float)</code>): 0.0005\n",
    "- batch size(<code>parser.add_argument('--batch-size', default=32, type=int)</code>): 32\n",
    "- <code>NUM_CLASSES = 21</code>: # 20개의 Class + 1개의 Background\n",
    "\n",
    "나머지의 사항은 기본적으로 해왔던 Model을 Training하는 과정과 같다.  \n",
    "자세한 사항은 Code에 주석으로서 첨부하였다.  \n",
    "\n",
    "**참고사항(batch-size)**  \n",
    "현재 Local Notebook에서는 Batch Size 32는 Training되지 않아서 Batch Size를 4로서 현저히 줄인뒤 Training하였다.  \n",
    "\n",
    "**참고사항(PiecewiseConstantDecay)**  \n",
    "논문에서 재시한 Learning Rate를 변경하는 방법이다.  \n",
    "현재 Code는 <code>tensorflow.keras.optimizers.schedules.PiecewiseConstantDecay</code>를 활용하여 Learning Rate를 서서히 낮추는 방법을 사용하였다.  \n",
    ">Applies exponential decay to the learning rate.  \n",
    "When training a model, it is often recommended to lower the learning rate as the training progresses. This schedule applies an exponential decay function to an optimizer step, given a provided initial learning rate.\n",
    "\n",
    "```code\n",
    "__init__(\n",
    "    initial_learning_rate,\n",
    "    decay_steps,\n",
    "    decay_rate,\n",
    "    staircase=False,\n",
    "    name=None\n",
    ")\n",
    "```\n",
    "<br>\n",
    "참조: <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/ExponentialDecay\">PiecewiseConstantDecay 사용법</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import yaml\n",
    "\n",
    "from tensorflow.keras.optimizers.schedules import PiecewiseConstantDecay\n",
    "from voc_data import create_batch_generator\n",
    "from anchor import generate_default_boxes\n",
    "from network import create_ssd\n",
    "from losses import create_losses\n",
    "\n",
    "# Paper와 같이 Hyperparameter를 Default로서 설정하였다.\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--batch-size', default=32, type=int)\n",
    "parser.add_argument('--num-batches', default=-1, type=int)\n",
    "parser.add_argument('--neg-ratio', default=3, type=int)\n",
    "parser.add_argument('--initial-lr', default=1e-3, type=float)\n",
    "parser.add_argument('--momentum', default=0.9, type=float)\n",
    "parser.add_argument('--weight-decay', default=5e-4, type=float)\n",
    "parser.add_argument('--num-epochs', default=120, type=int)\n",
    "parser.add_argument('--checkpoint-dir', default='checkpoints')\n",
    "parser.add_argument('--pretrained-type', default='base')\n",
    "parser.add_argument('--gpu-id', default='0')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "# 사용가능한 GPU Device를 설정한다.\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu_id\n",
    "\n",
    "# 20개의 Class + 1개의 Background\n",
    "NUM_CLASSES = 21\n",
    "\n",
    "\n",
    "# LossFunction과 Backpropagation을 징행한다.\n",
    "@tf.function\n",
    "def train_step(imgs, gt_confs, gt_locs, ssd, criterion, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        confs, locs = ssd(imgs)\n",
    "        conf_loss, loc_loss = criterion(\n",
    "            confs, locs, gt_confs, gt_locs)\n",
    "        loss = conf_loss + loc_loss\n",
    "        # l2_loss = [tf.nn.l2_loss(t) for t in ssd.trainable_variables]\n",
    "        # l2_loss = args.weight_decay * tf.math.reduce_sum(l2_loss)\n",
    "        # loss += l2_loss\n",
    "\n",
    "    gradients = tape.gradient(loss, ssd.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, ssd.trainable_variables))\n",
    "    \n",
    "    return loss, conf_loss, loc_loss\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Model의 Checkpoints를 저장할 Directory가 없을 경우 생성한다.\n",
    "    os.makedirs(args.checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    # 실제 SSD 300에 미리 저장되어 있는 Setting값을 가져와서 적용한다.(Anchor, FeatureMapSize 등)\n",
    "    with open('./config.yml') as f:\n",
    "        cfg = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "    config = cfg['SSD300']\n",
    "    default_boxes = generate_default_boxes(config)\n",
    "    \n",
    "    # voc_data.py에서 설정한 Dataset을 Batch형태로서 가져온다.\n",
    "    batch_generator, val_generator, info = create_batch_generator(default_boxes,\n",
    "        args.batch_size, args.num_batches,\n",
    "        mode='train')\n",
    "    \n",
    "    # 실제 SSD Model을 설정한다. 만약, Training중이던 Model이 있으면 그대로 가져가서 사용할 수 있다.\n",
    "    try:\n",
    "        ssd = create_ssd(NUM_CLASSES,\n",
    "                        args.pretrained_type,\n",
    "                        checkpoint_dir=args.checkpoint_dir)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('The program is exiting...')\n",
    "        sys.exit()\n",
    "    \n",
    "    # Hard negative mining을 적용하여 Loss를 구한다.\n",
    "    criterion = create_losses(args.neg_ratio, NUM_CLASSES)\n",
    "    steps_per_epoch = info['length'] // args.batch_size\n",
    "\n",
    "    # 해당 논문에서는 The learning rate decay policy is slightly different for each dataset\n",
    "    # 로서 설명하였다. 정확한 방법은 나와있지 않아서 아마 원본 Code를 참고하여 만든 것 같다.\n",
    "    lr_fn = PiecewiseConstantDecay(\n",
    "        boundaries=[int(steps_per_epoch * args.num_epochs * 2 / 3),\n",
    "                    int(steps_per_epoch * args.num_epochs * 5 / 6)],\n",
    "        values=[args.initial_lr, args.initial_lr * 0.1, args.initial_lr * 0.01])\n",
    "    \n",
    "    # Optimizer 선언\n",
    "    optimizer = tf.keras.optimizers.SGD(\n",
    "        learning_rate=lr_fn,\n",
    "        momentum=args.momentum)\n",
    "\n",
    "    # Training의 과정을 저장할 tf.summary를 선언한다.\n",
    "    train_log_dir = 'logs/train'\n",
    "    val_log_dir = 'logs/val'\n",
    "    train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "    val_summary_writer = tf.summary.create_file_writer(val_log_dir)\n",
    "\n",
    "    # 지정한 Epoch 만큼 Model을 Training한다.\n",
    "    for epoch in range(args.num_epochs):\n",
    "        avg_loss = 0.0\n",
    "        avg_conf_loss = 0.0\n",
    "        avg_loc_loss = 0.0\n",
    "        start = time.time()\n",
    "        for i, (_, imgs, gt_confs, gt_locs) in enumerate(batch_generator):\n",
    "            loss, conf_loss, loc_loss = train_step(\n",
    "                imgs, gt_confs, gt_locs, ssd, criterion, optimizer)\n",
    "            avg_loss = (avg_loss * i + loss.numpy()) / (i + 1)\n",
    "            avg_conf_loss = (avg_conf_loss * i + conf_loss.numpy()) / (i + 1)\n",
    "            avg_loc_loss = (avg_loc_loss * i + loc_loss.numpy()) / (i + 1)\n",
    "            # print(i)\n",
    "            \n",
    "            # Batch 도중에 Loss를 확인한다.\n",
    "            if (i + 1) % 50 == 0:\n",
    "                print('Epoch: {} Batch {} Time: {:.2}s | Loss: {:.4f} Conf: {:.4f} Loc: {:.4f}'.format(\n",
    "                    epoch + 1, i + 1, time.time() - start, avg_loss, avg_conf_loss, avg_loc_loss))\n",
    "\n",
    "        avg_val_loss = 0.0\n",
    "        avg_val_conf_loss = 0.0\n",
    "        avg_val_loc_loss = 0.0\n",
    "        \n",
    "        # Training Data가 아닌 Validation으로서 확인한다.\n",
    "        for i, (_, imgs, gt_confs, gt_locs) in enumerate(val_generator):\n",
    "            val_confs, val_locs = ssd(imgs)\n",
    "            val_conf_loss, val_loc_loss = criterion(\n",
    "                val_confs, val_locs, gt_confs, gt_locs)\n",
    "            val_loss = val_conf_loss + val_loc_loss\n",
    "            avg_val_loss = (avg_val_loss * i + val_loss.numpy()) / (i + 1)\n",
    "            avg_val_conf_loss = (avg_val_conf_loss * i + val_conf_loss.numpy()) / (i + 1)\n",
    "            avg_val_loc_loss = (avg_val_loc_loss * i + val_loc_loss.numpy()) / (i + 1)\n",
    "\n",
    "        # Training Loss에 관하여 tf.summary를 이용하여 저장\n",
    "        with train_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', avg_loss, step=epoch)\n",
    "            tf.summary.scalar('conf_loss', avg_conf_loss, step=epoch)\n",
    "            tf.summary.scalar('loc_loss', avg_loc_loss, step=epoch)\n",
    "\n",
    "        # Validation Loss에 관하여 tf.summary를 이용하여 저장\n",
    "        with val_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', avg_val_loss, step=epoch)\n",
    "            tf.summary.scalar('conf_loss', avg_val_conf_loss, step=epoch)\n",
    "            tf.summary.scalar('loc_loss', avg_val_loc_loss, step=epoch)\n",
    "\n",
    "        # 일정 Epoch마다 Model을 Keras의 .h5형태로서 저장\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            ssd.save_weights(\n",
    "                os.path.join(args.checkpoint_dir, 'ssd_epoch_{}.h5'.format(epoch + 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test.py\n",
    "실제 Test Image로서 만들어진 Model의 결과를 확인하는 방법이다.  \n",
    "기본적으로 train.py와 구조가 같지만 실제 Image에 Detection한 결과를 겹치게 Image로서 저장하고 또한, 해당 Label을 저장하여 결과를 나타내게 된다.  \n",
    "실행 예시는 다음과 같다.  \n",
    "\n",
    "**test.py 실행 에시**  \n",
    "<code>python test.py --checkpoint-path ./checkpoints/ssd_epoch_110.h5 --num-examples 40</code><br>\n",
    "\n",
    "**test.py Option List**  \n",
    "```code\n",
    "usage: test.py [-h] [--data-dir DATA_DIR] [--num-examples NUM_EXAMPLES]\n",
    "               [--pretrained-type PRETRAINED_TYPE]\n",
    "               [--checkpoint-dir CHECKPOINT_DIR]\n",
    "               [--checkpoint-path CHECKPOINT_PATH] [--gpu-id GPU_ID]\n",
    "```\n",
    "<br>\n",
    "\n",
    "**실형 결과**  \n",
    "최종적인 실행 결과를 살펴보면 다음과 같다.  \n",
    "\n",
    "<div><img src=\"https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/Tensorflow/79.png\" height=\"100%\" width=\"100%\" /></div><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "\n",
    "from anchor import generate_default_boxes\n",
    "from box_utils import decode, compute_nms\n",
    "from voc_data import create_batch_generator\n",
    "from image_utils import ImageVisualizer\n",
    "from losses import create_losses\n",
    "from network import create_ssd\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data-dir', default='./data/preprocessing_test')\n",
    "parser.add_argument('--num-examples', default=-1, type=int)\n",
    "parser.add_argument('--pretrained-type', default='specified')\n",
    "parser.add_argument('--checkpoint-dir', default='')\n",
    "parser.add_argument('--checkpoint-path', default='')\n",
    "parser.add_argument('--gpu-id', default='0')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu_id\n",
    "\n",
    "NUM_CLASSES = 21\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "# 실제 Image를 넣고 Object의 Localization과 Label의 Prediction한다.\n",
    "def predict(imgs, default_boxes):\n",
    "    confs, locs = ssd(imgs)\n",
    "\n",
    "    confs = tf.squeeze(confs, 0)\n",
    "    locs = tf.squeeze(locs, 0)\n",
    "\n",
    "    confs = tf.math.softmax(confs, axis=-1)\n",
    "    classes = tf.math.argmax(confs, axis=-1)\n",
    "    scores = tf.math.reduce_max(confs, axis=-1)\n",
    "\n",
    "    boxes = decode(default_boxes, locs)\n",
    "\n",
    "    out_boxes = []\n",
    "    out_labels = []\n",
    "    out_scores = []\n",
    "\n",
    "    for c in range(1, NUM_CLASSES):\n",
    "        cls_scores = confs[:, c]\n",
    "\n",
    "        score_idx = cls_scores > 0.6\n",
    "        # cls_boxes = tf.boolean_mask(boxes, score_idx)\n",
    "        # cls_scores = tf.boolean_mask(cls_scores, score_idx)\n",
    "        cls_boxes = boxes[score_idx]\n",
    "        cls_scores = cls_scores[score_idx]\n",
    "\n",
    "        nms_idx = compute_nms(cls_boxes, cls_scores, 0.45, 200)\n",
    "        cls_boxes = tf.gather(cls_boxes, nms_idx)\n",
    "        cls_scores = tf.gather(cls_scores, nms_idx)\n",
    "        cls_labels = [c] * cls_boxes.shape[0]\n",
    "\n",
    "        out_boxes.append(cls_boxes)\n",
    "        out_labels.extend(cls_labels)\n",
    "        out_scores.append(cls_scores)\n",
    "\n",
    "    out_boxes = tf.concat(out_boxes, axis=0)\n",
    "    out_scores = tf.concat(out_scores, axis=0)\n",
    "\n",
    "    boxes = tf.clip_by_value(out_boxes, 0.0, 1.0).numpy()\n",
    "    classes = np.array(out_labels)\n",
    "    scores = out_scores.numpy()\n",
    "\n",
    "    return boxes, classes, scores\n",
    "\n",
    "# Model을 정의하게 되고 실제 Detection한 Image의 결과와 Localization, Label 등을 저장하게 된다.\n",
    "if __name__ == '__main__':\n",
    "    with open('./config.yml') as f:\n",
    "        cfg = yaml.load(f)\n",
    "\n",
    "    config = cfg['SSD300']\n",
    "    default_boxes = generate_default_boxes(config)\n",
    "\n",
    "    batch_generator, info = create_batch_generator(\n",
    "        args.data_dir, default_boxes,\n",
    "        BATCH_SIZE, args.num_examples, mode='test')\n",
    "\n",
    "    try:\n",
    "        ssd = create_ssd(NUM_CLASSES,\n",
    "                         args.pretrained_type,\n",
    "                         args.checkpoint_dir,\n",
    "                         args.checkpoint_path)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('The program is exiting...')\n",
    "        sys.exit()\n",
    "\n",
    "    os.makedirs('outputs/images', exist_ok=True)\n",
    "    os.makedirs('outputs/detects', exist_ok=True)\n",
    "    visualizer = ImageVisualizer(info['idx_to_name'], save_dir='outputs/images')\n",
    "\n",
    "    for i, (filename, imgs, gt_confs, gt_locs) in enumerate(\n",
    "        tqdm(batch_generator, total=info['length'],\n",
    "             desc='Testing...', unit='images')):\n",
    "        boxes, classes, scores = predict(imgs, default_boxes)\n",
    "        filename = filename.numpy()[0].decode()\n",
    "        original_image = Image.open(\n",
    "            os.path.join(info['image_dir'], '{}.jpg'.format(filename)))\n",
    "        boxes *= original_image.size * 2\n",
    "        visualizer.save_image(\n",
    "            original_image, boxes, classes, '{}.jpg'.format(filename))\n",
    "\n",
    "        log_file = os.path.join('outputs/detects', '{}.txt')\n",
    "\n",
    "        for cls, box, score in zip(classes, boxes, scores):\n",
    "            cls_name = info['idx_to_name'][cls - 1]\n",
    "            with open(log_file.format(cls_name), 'a') as f:\n",
    "                f.write('{} {} {} {} {} {}\\n'.format(\n",
    "                    filename,\n",
    "                    score,\n",
    "                    *[coord for coord in box]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
