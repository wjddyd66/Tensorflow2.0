{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SSD 구현 (Dataset, Utils)\n",
    "코드 참조: <a href=\"https://github.com/ChunML/ssd-tf2\">ChunML GitHub</a><br>\n",
    "위의 Code를 참조하여 수정한 SSD의 현재 Directory의 구조는 다음과 같습니다.(위의 Code는 SSD 300,512를 둘 다 구현하였지만, 현재 Code는 논문에서 예제로 보여준 SSD300을 고정으로서 사용하였습니다.)  \n",
    "- Training Data\n",
    " - data/train/JPEGImages\n",
    " - data/train/Annotations\n",
    "- Test Data\n",
    " - data/test/JPEGImages\n",
    " - data/test/Annotations\n",
    " - preprocess_test.py: Test Dataset을 사용하기 위하여 데이터 전처리\n",
    " - data/preprocessing_test/JPEGImages: 실제 Test에 사용할 Image Directory\n",
    " - data/preprocessing_test/Annotations: 실제 Test에 사용할 Label Directory\n",
    "- Dataset(Dataset의 Batch처리를 위한 Code)\n",
    " - voc_data.py: Data Batch 처리\n",
    "- Utils(전체적인 Code의 utils를 모아둔 Code)\n",
    " - config.yml: 미리 Image Size, ratios, scales, fm_size(Feature Map의 크기)를 정의\n",
    " - anchor.py: Default Box를 생성\n",
    " - box_utils.py: IOU측정등 box를 위한 utils\n",
    " - image_utils.py: 논문에서 제시한 Data Augmentation, ImageVisualization을 정의\n",
    "- Model(SSD의 Model을 위한 Code)\n",
    " - layers.py: SSD의 Layer를 선언\n",
    " - network.py: SSD의 Network를 선언\n",
    " - losses.py: SSD의 Loss를 선언\n",
    "- Train & Test(Model의 Train 및 Test를 위한 Code)\n",
    " - train.py: Model의 Train을 위한 Code\n",
    " - test.py: Model의 Output을 위한 Code\n",
    "\n",
    "#### 사전사항 1(requirement)\n",
    "실제 SSD를 구현하기 위하여 필요한 사전설치사항은 requirement.txt에 저장해두었다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 사전사항 2(Dataset-Trainning, Validation)\n",
    "사용한 Data는 YOLO와 SSD에서 공통적으로 사용한 PASCAL 2012 Data를 사용하였다.  \n",
    "Directory의 Size는 2GB로서 다음링크에서 다운받을 수 있다.  \n",
    "참조: <a href=\"http://host.robots.ox.ac.uk:8080/pascal/VOC/voc2012/index.html#devkit\">PASCAL 2012 DataSet</a>  \n",
    "\n",
    "해당 링크에서 Data를 다운로드 받으면 다음과 같이 되어있다.  \n",
    "- VOCdevkit/VOC2012\n",
    "  - Annotations\n",
    "  - ImageSets\n",
    "  - JPEGImages\n",
    "  - SegmentationClass\n",
    "  - SegmentationObject\n",
    "  \n",
    "위의 5개의 Directory에서 2가지만 사용한다.  \n",
    "- JPEGImages: .jpg Image가 들어있다.\n",
    "- Annotation: .xml형식으로 해당 Image에 대한 Box의 위치가 적혀있다. (xmin,ymin,xmax,ymax)로서 구성되어있다.\n",
    "\n",
    "<div><img src=\"https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/Tensorflow/76.png\" height=\"100%\" width=\"100%\" /></div><br>\n",
    "<br>\n",
    "\n",
    "#### 사전사항 3(Dataset-Test)\n",
    "사용한 Data는 YOLO와 SSD에서 공통적으로 사용한 PASCAL 2012 Test Data를 사용(<a href=\"http://host.robots.ox.ac.uk:8080/eval/challenges/voc2012/\">PASCAL 2012 Test DataSet</a>)하였다.  \n",
    "Directory의 Size는 500MB이고 위에서 다운받은 링크에서 회원가입을 해야지 다운받을 수 있다.    \n",
    "해당 링크에서 회원가입 후 Data를 다운로드 받으면 다음과 같이 되어있다.  \n",
    "- VOCdevkit/VOC2012\n",
    "  - Annotations\n",
    "  - ImageSets\n",
    "  - JPEGImages\n",
    "  \n",
    "위의 3개의 Directory에서 2가지만 사용한다.  \n",
    "- JPEGImages: .jpg Image가 들어있다.\n",
    "- Annotation: .xml형식으로 해당 Image에 대한 Box의 위치가 적혀있다. (xmin,ymin,xmax,ymax)로서 구성되어있다.\n",
    "<br>\n",
    "\n",
    "현재 TestDataset을 다운받아 사용하면 문제점이 Image File인 .jpg와 Label File인 .xml의 수가 일치하지 않는다는 것 이다. 따라서 아래의 Code를 통하여 서로 Mapping되는 File만을 preprocessing_test Directory에 위치하게 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "os.chdir('./data/test')\n",
    "image_dir = './JPEGImages/'\n",
    "ano_dir ='./Annotations/'\n",
    "preprocessing_dir = '../preprocessing_test'\n",
    "preprocessing_image = preprocessing_dir+'/JPEGImages/'\n",
    "preprocessing_ano = preprocessing_dir+'/Annotations/'\n",
    "\n",
    "def make_dir(dir_path):\n",
    "    if not os.path.isdir(dir_path):\n",
    "        os.mkdir(dir_path)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "make_dir(preprocessing_dir)\n",
    "make_dir(preprocessing_image)\n",
    "make_dir(preprocessing_ano)\n",
    "\n",
    "image_file = list(map(lambda x: x[:-4], os.listdir(image_dir)))\n",
    "ano_file = list(map(lambda x: x[:-4], os.listdir(ano_dir)))\n",
    "\n",
    "for image in image_file:\n",
    "    for ano in ano_file:\n",
    "        if image == ano:\n",
    "            shutil.move(image_dir+image+'.jpg',preprocessing_image)\n",
    "            shutil.move(ano_dir+ano+'.xml',preprocessing_ano)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils\n",
    "#### config.yaml\n",
    "기본적인 SSD300에 대한 Parameter를 미리 정의한다.  \n",
    "```code\n",
    "SSD300:\n",
    "  ratios: [[2], [2, 3], [2, 3], [2, 3], [2], [2]]\n",
    "  scales: [0.1, 0.2, 0.375, 0.55, 0.725, 0.9, 1.075]\n",
    "  fm_sizes: [38, 19, 10, 5, 3, 1]\n",
    "  image_size: 300\n",
    "```\n",
    "<br>\n",
    "\n",
    "- image_size: 해당 논문과 동일하게 300 x 300 pixel Size로서 선언하였다.\n",
    "- ratiosn: Aspect Rations는 <span>$1,2,3,\\frac{1}{2},\\frac{1}{3}$</span>으로서 선언되었다. 1인경우에 특별한 <span>$s_k^{'}=\\sqrt{s_k s_{k+1}}$</span>이 적용되어야 하므로 따로 anchor.py에서 추가적인 작업을 한다.\n",
    "- scales: <span>$m=6, s_{min}=0.1, s_{max}=0.9$</span>,  <span>$s_k = s_{min}+\\frac{s_{max}-s_{min}}{m}(k-1) \\rightarrow 0.1+\\frac{0.8}{5}(k-1)$</span>로서 값을 대입하여 계산하였다.\n",
    "- fm_size: 해당 논문의 Model구조에 맞는 Feature Map의 Size를 정의하였다.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/Tensorflow/77.png\" height=\"100%\" width=\"100%\"><br>\n",
    "\n",
    "**조심하여야 하는 부분이다. 제일 이해가 되지 않고 많이 해매었던 부분이기도 하다. 위의 Network의 결과로서 8732 Dimension의 Tensor가 나오게 된다. 각각의 Featuremap의 결과를 합치면 다음과 같이 나오게 되어야 한다. 하지만 Aspect Rations는 <span>$1,2,3,\\frac{1}{2},\\frac{1}{3}$</span>를 전부 다 적용하면 11640 Dimension의 결과가 나오게 된다.(<span>$11640=(38*38*6)+(19*19*6)+(10*10*6)+(5*5*6)+(3*3*6)+(1*1*6)$</span>) 논문을 자세히 보면 각각의 Featurmap의 결과에 <span>$s_k$</span>로서 표시한 것을 볼 수 있다. 이것이 의미하는 것은 Scale의 개수를 의미하는 것으로 다음과 같이 정리될 수 있다.**  \n",
    "- <span>$s_1 = 1,2,\\frac{1}{2}$</span>\n",
    "- <span>$s_2 = 1,2,\\frac{1}{2},3,\\frac{1}{3}$</span>\n",
    "\n",
    "따라서 위의 ratios처럼 정의하게 되면 최종적으로 8738(=<span>$(38*38*4)+(19*19*6)+(10*10*6)+(5*5*6)+(3*3*4)+(1*1*4)$</span>)를 얻을 수 있다.  \n",
    "\n",
    "#### anchor.py\n",
    "Default Box를 정의하는 곳 이다.  \n",
    "각각의 Default Box는 Feature Map에 따라서 Ratios와 Scale을 변화시키면서 (cx,cy,w,h)를 생성하게 된다.  \n",
    "<p>$$a_r \\in {1,2,3,\\frac{1}{2},\\frac{1}{3}} (w_k^a = s_k\\sqrt{a_r}) (h_k^a = s_k/\\sqrt{a_r})$$</p>\n",
    "<p>$$s_k^{'}=\\sqrt{s_k s_{k+1}}\\text{,  }a_r=1 \\text{ 인 경우}$$</p>\n",
    "<p>$$(cx,cy) = (\\frac{i+0.5}{|f_k|},\\frac{j+0.5}{|f_k|}), f_k\\text{는 k번째 Feature Map의 크기}$$</p>\n",
    "위의 식을 그대로 적용한 것을 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import math\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def generate_default_boxes(config):\n",
    "    \"\"\" Generate default boxes for all feature maps\n",
    "\n",
    "    Args:\n",
    "        config: information of feature maps\n",
    "        scales: boxes' size relative to image's size\n",
    "        fm_sizes: sizes of feature maps\n",
    "        ratios: box ratios used in each feature maps\n",
    "\n",
    "    Returns:\n",
    "        default_boxes: tensor of shape (num_default, 4) with format (cx, cy, w, h)\n",
    "    \"\"\"\n",
    "\n",
    "    # Config를 Argument로 받아 미리 지정되어있는 config.yaml File의 Parameter값을 가져오게 된다.\n",
    "    default_boxes = []\n",
    "    scales = config['scales']\n",
    "    fm_sizes = config['fm_sizes']\n",
    "    ratios = config['ratios']\n",
    "\n",
    "    for m, fm_size in enumerate(fm_sizes):\n",
    "        for i, j in itertools.product(range(fm_size), repeat=2):\n",
    "            # cx, cy 정의\n",
    "            cx = (j + 0.5) / fm_size\n",
    "            cy = (i + 0.5) / fm_size\n",
    "            # Aspect ratio가 1인경우\n",
    "            default_boxes.append([\n",
    "                cx,\n",
    "                cy,\n",
    "                scales[m],\n",
    "                scales[m]\n",
    "            ])\n",
    "\n",
    "            default_boxes.append([\n",
    "                cx,\n",
    "                cy,\n",
    "                math.sqrt(scales[m] * scales[m + 1]),\n",
    "                math.sqrt(scales[m] * scales[m + 1])\n",
    "            ])\n",
    "            \n",
    "            # Aspect ratio가 1이 아닌경우 (2,3)\n",
    "            for ratio in ratios:\n",
    "                r = math.sqrt(ratio)\n",
    "                default_boxes.append([\n",
    "                    cx,\n",
    "                    cy,\n",
    "                    scales[m] * r,\n",
    "                    scales[m] / r\n",
    "                ])\n",
    "\n",
    "                default_boxes.append([\n",
    "                    cx,\n",
    "                    cy,\n",
    "                    scales[m] / r,\n",
    "                    scales[m] * r\n",
    "                ])\n",
    "\n",
    "    # Defult Boxes는 N*(cx,cy,w,h)로서 정의되게 된다.\n",
    "    default_boxes = tf.constant(default_boxes)\n",
    "    # 0~1 사이의 값으로서 정규화\n",
    "    default_boxes = tf.clip_by_value(default_boxes, 0.0, 1.0)\n",
    "\n",
    "    return default_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### box_utils.py\n",
    "실제 Box에 관한 Utils를 모아두는 곳 이다. 다양한 Function이 존재하게 된다.  \n",
    "1) transform_corner_to_center(boxes)  \n",
    "원래 Label Dataset Format을 논문에서의 Format으로 바꾼다. (xmin,ymin,xmax,ymax) -> (cx,cy,w,h)  \n",
    "\n",
    "2) transform_center_to_corner(boxes)  \n",
    "논문에서의 Format을 원래 Label Dataset Format으로 바꾼다. (cx,cy,w,h) -> (xmin,ymin,xmax,ymax)  \n",
    "\n",
    "3) compute_area(top_left,bot_right)  \n",
    "iou를 계산하기 위해서 (top_left(x_min,y_min),bot_right(x_max,y_max))를 통하여 Bounding Box의 width, height를 통하여 Area를 구하는 과정이다.  \n",
    "\n",
    "4) compute_iou(boxes_a,boxes_b)  \n",
    "실제 Ground Truth Box와 Predicted Box를 jaccard overlap을 통하여 Matching strategy를 계산하는 과정이다.   \n",
    "<p>$$J(A,B) = \\frac{|A \\cap B|}{|A \\cup B|} = \\frac{|A \\cap B|}{|A|+|B|-|A \\cap B|}$$</p>\n",
    "\n",
    "5) encode(default_boxes, boxes)  \n",
    "Localization Loss를 위하여 식을 좌표를 변경하는 단계이다.  \n",
    "<p>$$\\hat{g}_j^{cx}=(g_j^{cx}-d_i^{cx})/d_i^w, \\hat{g}_j^{cy}=(g_j^{cy}-d_i^{cy})/d_i^h$$</p>\n",
    "<p>$$\\hat{g}_j^{w} = log(\\frac{g_j^w}{d_i^w}),  \\hat{g}_j^{h} = log(\\frac{g_j^h}{d_i^h})$$</p>\n",
    "\n",
    "6) decode(default_boxes, locs)  \n",
    "Encode의 반대 과정  \n",
    "\n",
    "7) compute_target(default_boxes, gt_boxes,gt_labels,iou_threshold=0.5)  \n",
    "2가지의 Confidence Loss와 Localization Loss를 위한 과정이다.  \n",
    "현재 Function의 절차는 다음과 같다.  \n",
    "1. Matching strategy를 계산\n",
    "2. 가장 iou가 높은 best_default_iou, best_gt_iou를 계산한다.\n",
    "3. 가장 iou가 높은 Ground Truth Box의 Class를 측정한다.\n",
    "3. Ground Truth Box의 Classify의 Label중 해당 Box에 대한 Class를 구한다. Threshold(논문과 같은 0.5의 값)를 적용하여 해당 클래스를 구한다.(gt_confs=<span>$x_{ij}^p log(\\hat{c}_i^p)$</span>)\n",
    "5. Default Box에서 가장 IOU가 높은 Ground Truth Box의 좌표를 Encode를 통하여 논문에 맞게 식을 변형한다.(gt_locs= GroundTruthBox(<span>$(\\hat{g}_j^{cx},\\hat{g}_j^{cy},\\hat{g}_j^{w},\\hat{g}_j^{h})$</span>))\n",
    "\n",
    "<p>$$\n",
    "x_{ij}^p=\n",
    "\\begin{cases}\n",
    "1, & \\mbox{if } IOU > 0.5 \\mbox{ between default box i and ground true box j on class p} \\\\\n",
    "0, & \\mbox{otherwise}\n",
    "\\end{cases}\n",
    "$$</p>\n",
    "\n",
    "8) compute_nms(boxes, scores, nms_threshold, limit=200)  \n",
    "일정영역 이상의 겹친 Boxes를 제거하는 부분이다. 가장 많이 겹친 부분만 살려둔다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# iou를 계산하기 위해서 (top_left(x_min,y_min),bot_right(x_max,y_max))를 통하여 \n",
    "# Bounding Box의 width, height를 통하여 Area를 구하는 과정이다.\n",
    "def compute_area(top_left, bot_right):\n",
    "    \"\"\" Compute area given top_left and bottom_right coordinates\n",
    "    Args:\n",
    "        top_left: tensor (num_boxes, 2)\n",
    "        bot_right: tensor (num_boxes, 2)\n",
    "    Returns:\n",
    "        area: tensor (num_boxes,)\n",
    "    \"\"\"\n",
    "    # top_left: N x 2\n",
    "    # bot_right: N x 2\n",
    "    hw = tf.clip_by_value(bot_right - top_left, 0.0, 300.0)\n",
    "    area = hw[..., 0] * hw[..., 1]\n",
    "\n",
    "    return area\n",
    "\n",
    "# 실제 Ground Truth Box와 Predicted Box를 jaccard overlap을 통하여 Matching strategy를 계산하는 과정이다.\n",
    "def compute_iou(boxes_a, boxes_b):\n",
    "    \"\"\" Compute overlap between boxes_a and boxes_b\n",
    "    Args:\n",
    "        boxes_a: tensor (num_boxes_a, 4)\n",
    "        boxes_b: tensor (num_boxes_b, 4)\n",
    "    Returns:\n",
    "        overlap: tensor (num_boxes_a, num_boxes_b)\n",
    "    \"\"\"\n",
    "    # boxes_a => num_boxes_a, 1, 4\n",
    "    boxes_a = tf.expand_dims(boxes_a, 1)\n",
    "\n",
    "    # boxes_b => 1, num_boxes_b, 4\n",
    "    boxes_b = tf.expand_dims(boxes_b, 0)\n",
    "    top_left = tf.math.maximum(boxes_a[..., :2], boxes_b[..., :2])\n",
    "    bot_right = tf.math.minimum(boxes_a[..., 2:], boxes_b[..., 2:])\n",
    "\n",
    "    overlap_area = compute_area(top_left, bot_right)\n",
    "    area_a = compute_area(boxes_a[..., :2], boxes_a[..., 2:])\n",
    "    area_b = compute_area(boxes_b[..., :2], boxes_b[..., 2:])\n",
    "\n",
    "    overlap = overlap_area / (area_a + area_b - overlap_area)\n",
    "\n",
    "    return overlap\n",
    "\n",
    "# 2가지의 Confidence Loss와 Localization Loss를 위한 과정이다.\n",
    "def compute_target(default_boxes, gt_boxes, gt_labels, iou_threshold=0.5):\n",
    "    \"\"\" Compute regression and classification targets\n",
    "    Args:\n",
    "        default_boxes: tensor (num_default, 4)\n",
    "                       of format (cx, cy, w, h)\n",
    "        gt_boxes: tensor (num_gt, 4)\n",
    "                  of format (xmin, ymin, xmax, ymax)\n",
    "        gt_labels: tensor (num_gt,)\n",
    "    Returns:\n",
    "        gt_confs: classification targets, tensor (num_default,)\n",
    "        gt_locs: regression targets, tensor (num_default, 4)\n",
    "    \"\"\"\n",
    "    # Convert default boxes to format (xmin, ymin, xmax, ymax)\n",
    "    # in order to compute overlap with gt boxes\n",
    "\n",
    "    transformed_default_boxes = transform_center_to_corner(default_boxes)\n",
    "    \n",
    "    # Matching strategy를 계산\n",
    "    iou = compute_iou(transformed_default_boxes, gt_boxes)\n",
    "    \n",
    "    # 가장 iou가 높은 best_default_iou, best_gt_iou를 계산한다.\n",
    "    best_gt_iou = tf.math.reduce_max(iou, 1)\n",
    "    best_gt_idx = tf.math.argmax(iou, 1)\n",
    "\n",
    "    best_default_iou = tf.math.reduce_max(iou, 0)\n",
    "    best_default_idx = tf.math.argmax(iou, 0)\n",
    "    \n",
    "    \n",
    "    best_gt_idx = tf.tensor_scatter_nd_update(\n",
    "        best_gt_idx,\n",
    "        tf.expand_dims(best_default_idx, 1),\n",
    "        tf.range(best_default_idx.shape[0], dtype=tf.int64))\n",
    "\n",
    "    # Normal way: use a for loop\n",
    "    # for gt_idx, default_idx in enumerate(best_default_idx):\n",
    "    #     best_gt_idx = tf.tensor_scatter_nd_update(\n",
    "    #         best_gt_idx,\n",
    "    #         tf.expand_dims([default_idx], 1),\n",
    "    #         [gt_idx])\n",
    "\n",
    "    \n",
    "    best_gt_iou = tf.tensor_scatter_nd_update(\n",
    "        best_gt_iou,\n",
    "        tf.expand_dims(best_default_idx, 1),\n",
    "        tf.ones_like(best_default_idx, dtype=tf.float32))\n",
    "\n",
    "    # Ground Truth Box의 Classify의 Label중 해당 Box에 대한 Class를 구한다. \n",
    "    # Threshold(논문과 같은 0.5의 값)를 적용하여 해당 클래스를 구한다.\n",
    "    gt_confs = tf.gather(gt_labels, best_gt_idx)\n",
    "    gt_confs = tf.where(\n",
    "        tf.less(best_gt_iou, iou_threshold),\n",
    "        tf.zeros_like(gt_confs),\n",
    "        gt_confs)\n",
    "    \n",
    "    # Default Box에서 가장 IOU가 높은 Ground Truth Box의 \n",
    "    # 좌표를 Encode를 통하여 논문에 맞게 식을 변형한다.\n",
    "    gt_boxes = tf.gather(gt_boxes, best_gt_idx)\n",
    "    gt_locs = encode(default_boxes, gt_boxes)\n",
    "\n",
    "    return gt_confs, gt_locs\n",
    "\n",
    "# Localization Loss를 위하여 식을 좌표를 변경하는 단계이다.\n",
    "def encode(default_boxes, boxes):\n",
    "    \"\"\" Compute regression values\n",
    "    Args:\n",
    "        default_boxes: tensor (num_default, 4)\n",
    "                       of format (cx, cy, w, h)\n",
    "        boxes: tensor (num_default, 4)\n",
    "               of format (xmin, ymin, xmax, ymax)\n",
    "        variance: variance for center point and size\n",
    "    Returns:\n",
    "        locs: regression values, tensor (num_default, 4)\n",
    "    \"\"\"\n",
    "    # Convert boxes to (cx, cy, w, h) format\n",
    "    transformed_boxes = transform_corner_to_center(boxes)\n",
    "\n",
    "    locs = tf.concat([\n",
    "        (transformed_boxes[..., :2] - default_boxes[:, :2]\n",
    "         ) / (default_boxes[:, 2:]),\n",
    "        tf.math.log(transformed_boxes[..., 2:] / default_boxes[:, 2:])],\n",
    "        axis=-1)\n",
    "\n",
    "    return locs\n",
    "\n",
    "# Encode의 반대 과정\n",
    "def decode(default_boxes, locs):\n",
    "    \"\"\" Decode regression values back to coordinates\n",
    "    Args:\n",
    "        default_boxes: tensor (num_default, 4)\n",
    "                       of format (cx, cy, w, h)\n",
    "        locs: tensor (batch_size, num_default, 4)\n",
    "              of format (cx, cy, w, h)\n",
    "        variance: variance for center point and size\n",
    "    Returns:\n",
    "        boxes: tensor (num_default, 4)\n",
    "               of format (xmin, ymin, xmax, ymax)\n",
    "    \"\"\"\n",
    "    locs = tf.concat([\n",
    "        locs[..., :2] *\n",
    "        default_boxes[:, 2:] + default_boxes[:, :2],\n",
    "        tf.math.exp(locs[..., 2:]) * default_boxes[:, 2:]], axis=-1)\n",
    "\n",
    "    boxes = transform_center_to_corner(locs)\n",
    "\n",
    "    return boxes\n",
    "\n",
    "# 원래 Label Dataset Format을 논문에서의 Format으로 바꾼다. (xmin,ymin,xmax,ymax) -> (cx,cy,w,h)\n",
    "def transform_corner_to_center(boxes):\n",
    "    \"\"\" Transform boxes of format (xmin, ymin, xmax, ymax)\n",
    "        to format (cx, cy, w, h)\n",
    "    Args:\n",
    "        boxes: tensor (num_boxes, 4)\n",
    "               of format (xmin, ymin, xmax, ymax)\n",
    "    Returns:\n",
    "        boxes: tensor (num_boxes, 4)\n",
    "               of format (cx, cy, w, h)\n",
    "    \"\"\"\n",
    "    center_box = tf.concat([\n",
    "        (boxes[..., :2] + boxes[..., 2:]) / 2,\n",
    "        boxes[..., 2:] - boxes[..., :2]], axis=-1)\n",
    "\n",
    "    return center_box\n",
    "\n",
    "# 논문에서의 Format을 원래 Label Dataset Format으로 바꾼다. (cx,cy,w,h) -> (xmin,ymin,xmax,ymax)\n",
    "def transform_center_to_corner(boxes):\n",
    "    \"\"\" Transform boxes of format (cx, cy, w, h)\n",
    "        to format (xmin, ymin, xmax, ymax)\n",
    "    Args:\n",
    "        boxes: tensor (num_boxes, 4)\n",
    "               of format (cx, cy, w, h)\n",
    "    Returns:\n",
    "        boxes: tensor (num_boxes, 4)\n",
    "               of format (xmin, ymin, xmax, ymax)\n",
    "    \"\"\"\n",
    "    corner_box = tf.concat([\n",
    "        boxes[..., :2] - boxes[..., 2:] / 2,\n",
    "        boxes[..., :2] + boxes[..., 2:] / 2], axis=-1)\n",
    "\n",
    "    return corner_box\n",
    "\n",
    "# 일정영역 이상의 겹친 Boxes를 제거하는 부분이다. 가장 많이 겹친 부분만 살려둔다.\n",
    "def compute_nms(boxes, scores, nms_threshold, limit=200):\n",
    "    \"\"\" Perform Non Maximum Suppression algorithm\n",
    "        to eliminate boxes with high overlap\n",
    "\n",
    "    Args:\n",
    "        boxes: tensor (num_boxes, 4)\n",
    "               of format (xmin, ymin, xmax, ymax)\n",
    "        scores: tensor (num_boxes,)\n",
    "        nms_threshold: NMS threshold\n",
    "        limit: maximum number of boxes to keep\n",
    "\n",
    "    Returns:\n",
    "        idx: indices of kept boxes\n",
    "    \"\"\"\n",
    "    if boxes.shape[0] == 0:\n",
    "        return tf.constant([], dtype=tf.int32)\n",
    "    selected = [0]\n",
    "    idx = tf.argsort(scores, direction='DESCENDING')\n",
    "    idx = idx[:limit]\n",
    "    boxes = tf.gather(boxes, idx)\n",
    "\n",
    "    iou = compute_iou(boxes, boxes)\n",
    "\n",
    "    while True:\n",
    "        row = iou[selected[-1]]\n",
    "        next_indices = row <= nms_threshold\n",
    "        # iou[:, ~next_indices] = 1.0\n",
    "        iou = tf.where(\n",
    "            tf.expand_dims(tf.math.logical_not(next_indices), 0),\n",
    "            tf.ones_like(iou, dtype=tf.float32),\n",
    "            iou)\n",
    "\n",
    "        if not tf.math.reduce_any(next_indices):\n",
    "            break\n",
    "\n",
    "        selected.append(tf.argsort(\n",
    "            tf.dtypes.cast(next_indices, tf.int32), direction='DESCENDING')[0].numpy())\n",
    "\n",
    "    return tf.gather(idx, selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### image_utils.py\n",
    "Image에 관련된 Utils모아둔 Code이다.  \n",
    "1) class ImageVisualizer(object)  \n",
    "Model이 Predicted한 Object Class와 Box Localization, Image를 입력받아 해당되는 Image에 Box를 표시한 뒤, Class를 표시하고 저장한다.  \n",
    "2) horizontal_flip(img, boxes, labels)  \n",
    "Data Augmentation방법이다. 논문에서는 다음과 같이 3가지 방법을 사용하였다.  \n",
    "- Use the original\n",
    "- Using Patch(Sample a patch with IOU of 0.1, 0.3, 0.5, 0.7 or 0.9)\n",
    "- Resize and flipped with probablity of 0.5\n",
    "\n",
    "위의 방식대로 3가지로 진행하려고 하였으나, IOU가 0.1, 0.3, 0.5 에 맞게 계속해서 무한루프를 돌게되는 경우 많은 시간을 소비하여 제외하고 Original, Flipped의 2가지 방법을 사용하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from box_utils import compute_iou\n",
    "\n",
    "# Model이 Predicted한 Object Class와 Box Localization, \n",
    "# Image를 입력받아 해당되는 Image에 Box를 표시한 뒤, Class를 표시하고 저장한다.\n",
    "class ImageVisualizer(object):\n",
    "    \"\"\" Class for visualizing image\n",
    "\n",
    "    Attributes:\n",
    "        idx_to_name: list to convert integer to string label\n",
    "        class_colors: colors for drawing boxes and labels\n",
    "        save_dir: directory to store images\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, idx_to_name, class_colors=None, save_dir=None):\n",
    "        self.idx_to_name = idx_to_name\n",
    "        if class_colors is None or len(class_colors) != len(self.idx_to_name):\n",
    "            self.class_colors = [[0, 255, 0]] * len(self.idx_to_name)\n",
    "        else:\n",
    "            self.class_colors = class_colors\n",
    "\n",
    "        if save_dir is None:\n",
    "            self.save_dir = './'\n",
    "        else:\n",
    "            self.save_dir = save_dir\n",
    "\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "\n",
    "    def save_image(self, img, boxes, labels, name):\n",
    "        \"\"\" Method to draw boxes and labels\n",
    "            then save to dir\n",
    "\n",
    "        Args:\n",
    "            img: numpy array (width, height, 3)\n",
    "            boxes: numpy array (num_boxes, 4)\n",
    "            labels: numpy array (num_boxes)\n",
    "            name: name of image to be saved\n",
    "        \"\"\"\n",
    "        plt.figure()\n",
    "        fig, ax = plt.subplots(1)\n",
    "        ax.imshow(img)\n",
    "        save_path = os.path.join(self.save_dir, name)\n",
    "\n",
    "        for i, box in enumerate(boxes):\n",
    "            idx = labels[i] - 1\n",
    "            cls_name = self.idx_to_name[idx]\n",
    "            top_left = (box[0], box[1])\n",
    "            bot_right = (box[2], box[3])\n",
    "            ax.add_patch(patches.Rectangle(\n",
    "                (box[0], box[1]),\n",
    "                box[2] - box[0], box[3] - box[1],\n",
    "                linewidth=2, edgecolor=(0., 1., 0.),\n",
    "                facecolor=\"none\"))\n",
    "            plt.text(\n",
    "                box[0],\n",
    "                box[1],\n",
    "                s=cls_name,\n",
    "                color=\"white\",\n",
    "                verticalalignment=\"top\",\n",
    "                bbox={\"color\": (0., 1., 0.), \"pad\": 0},\n",
    "            )\n",
    "\n",
    "        plt.axis(\"off\")\n",
    "        plt.savefig(save_path, bbox_inches=\"tight\", pad_inches=0.0)\n",
    "        plt.close('all')\n",
    "\n",
    "# Image를 Flipped한 뒤 Box의 위치를 조정한다.\n",
    "def horizontal_flip(img, boxes, labels):\n",
    "    \"\"\" Function to horizontally flip the image\n",
    "        The gt boxes will be need to be modified accordingly\n",
    "\n",
    "    Args:\n",
    "        img: the original PIL Image\n",
    "        boxes: gt boxes tensor (num_boxes, 4)\n",
    "        labels: gt labels tensor (num_boxes,)\n",
    "\n",
    "    Returns:\n",
    "        img: the horizontally flipped PIL Image\n",
    "        boxes: horizontally flipped gt boxes tensor (num_boxes, 4)\n",
    "        labels: gt labels tensor (num_boxes,)\n",
    "    \"\"\"\n",
    "    img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "    boxes = tf.stack([\n",
    "        1 - boxes[:, 2],\n",
    "        boxes[:, 1],\n",
    "        1 - boxes[:, 0],\n",
    "        boxes[:, 3]], axis=1)\n",
    "\n",
    "    return img, boxes, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "#### voc_data.py\n",
    "실질적인 Dataset을 Training Data와 Validation Data로서 나눈 뒤, Batch 처리까지하여 Model에 넣을 수 있게 하는 Preprocessing 단계이다.  \n",
    "1) init():\n",
    "Data초기에 필요한 Argument들을 정의하는 부분이다.  \n",
    "- idx_to_name: 미리 정해져있는 20개의 Label을 정의한 것\n",
    "- name_to_idx: {'aeroplane': 0, 'bicycle': 1,...} 형식으로 Label의 이름과 Index를 Dict Type으로 선언\n",
    "- image_dir: Image 경로\n",
    "- anno_dir: Annotations(Bounding Box의 Label 및 (xmin,ymin,xmax,ymax)) 경로\n",
    "- ids: Image와 해당되는 Annotation이 맞는지 확인하기 위한 것.\n",
    " - image_dir_example: 2008_000200.jpg\n",
    " - anno_dir_example: 2008_000200.xml\n",
    "- default_boxes: 입력 받는 Default Boxes\n",
    "- new_size: Model Input으로 들어가는 Image의 Size\n",
    "- train_ids: Trainning Dataset, 전체 Dataset의 75%\n",
    "- val_ids: Validation Dataset, 전체 Dataset의 25%\n",
    "- augmentation: 위의 image_utils.py를 활용하여 Dataset을 원래대로 사용할지 Flip한 Dataset을 사용할지 결정하기 위해서\n",
    "\n",
    "2) len(): 전체 데이터의 개수 파악  \n",
    "3) get_image(): 해당되는 Index의 Image를 반환  \n",
    "4) get_annotation(): 해당되는 Index의 Annotation.xml을 통하여 Label,(xmin, ymin, xmax, ymax)을 반환-> 0~1사이의 값으로서 정규화  \n",
    "5) generate()\n",
    "1. Input Image의 Size를 받는다.\n",
    "2. get_annotation()을 통하여 Label과 Bounding Box의 Location을 입력받는다.\n",
    "3. Random하게 Original Image를 사용할지 Flip을 실행할 Image를 사용할지 결정한다.\n",
    "4. Image의 Size를 Model Input에 맞게 (300,300)으로 바꾼뒤 0 ~ 1 사이의 값으로서 정규화를 한다.\n",
    "5. Utils -> box_utils -> compute_target를 통하여 실제 Label을 Model에 맞는 Label로서 변경한다.\n",
    "6. Filename, Image, Ground Truth Label, Ground Truth Location을 반환한다.\n",
    "6) create_batch_generator(): Batch_Size를 입력받아 Dataset을 생성한다. 만약 아래 Code가 이해되지 않으면 링크 참조  \n",
    " - <a href=\"https://wjddyd66.github.io/tnesorflow2.0/Tensorflow2.0(3)/\">Load and preprocess Data</a>\n",
    " - <a href=\"https://wjddyd66.github.io/tnesorflow2.0/Tensorflow2.0(4)/\">Load and preprocess Data2</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "from box_utils import compute_target\n",
    "from image_utils import horizontal_flip\n",
    "from functools import partial\n",
    "\n",
    "# 실질적인 Dataset을 Training Data와 Validation Data로서 나눈 뒤, \n",
    "# Batch 처리까지하여 Model에 넣을 수 있게 하는 Preprocessing 단계이다. \n",
    "\n",
    "# Data초기에 필요한 Argument들을 정의하는 부분이다. \n",
    "class VOCDataset():\n",
    "    def __init__(self, data_dir, default_boxes,num_examples=-1):\n",
    "        super(VOCDataset, self).__init__()\n",
    "        # 미리 정해져있는 20개의 Label을 정의한 것\n",
    "        self.idx_to_name = [\n",
    "            'aeroplane', 'bicycle', 'bird', 'boat',\n",
    "            'bottle', 'bus', 'car', 'cat', 'chair',\n",
    "            'cow', 'diningtable', 'dog', 'horse',\n",
    "            'motorbike', 'person', 'pottedplant',\n",
    "            'sheep', 'sofa', 'train', 'tvmonitor']\n",
    "        \n",
    "        # {'aeroplane': 0, 'bicycle': 1,...} 형식으로 Label의 이름과 Index를 Dict Type으로 선언\n",
    "        self.name_to_idx = dict([(v, k)\n",
    "                                 for k, v in enumerate(self.idx_to_name)])\n",
    "        # Image 경로\n",
    "        self.image_dir = data_dir+'/JPEGImages'\n",
    "        # Annotations(Bounding Box의 Label 및 (xmin,ymin,xmax,ymax)) 경로\n",
    "        self.anno_dir = data_dir+'/Annotations'\n",
    "        \n",
    "        \n",
    "        # Image와 해당되는 Annotation이 맞는지 확인하기 위한 것.\n",
    "        self.ids = list(map(lambda x: x[:-4], os.listdir(self.image_dir)))\n",
    "        # 입력 받는 Default Boxes\n",
    "        self.default_boxes = default_boxes\n",
    "        # Model Input으로 들어가는 Image의 Size\n",
    "        self.new_size = 300\n",
    "\n",
    "        if num_examples != -1:\n",
    "            self.ids = self.ids[:num_examples]\n",
    "        # Trainning Dataset, 전체 Dataset의 75%\n",
    "        self.train_ids = self.ids[:int(len(self.ids) * 0.75)]\n",
    "        # Validation Dataset, 전체 Dataset의 25%\n",
    "        self.val_ids = self.ids[int(len(self.ids) * 0.75):]\n",
    "        # 위의 image_utils.py를 활용하여 Dataset을 원래대로 사용할지 Flip한 Dataset을 사용할지 결정하기 위해서\n",
    "        self.augmentation = ['original','flip']\n",
    "    \n",
    "    # 전체 데이터의 개수 파악  \n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "    \n",
    "    # 해당되는 Index의 Image를 반환\n",
    "    def _get_image(self, index):\n",
    "        \"\"\" Method to read image from file\n",
    "            then resize to (300, 300)\n",
    "            then subtract by ImageNet's mean\n",
    "            then convert to Tensor\n",
    "\n",
    "        Args:\n",
    "            index: the index to get filename from self.ids\n",
    "\n",
    "        Returns:\n",
    "            img: tensor of shape (3, 300, 300)\n",
    "        \"\"\"\n",
    "        filename = self.ids[index]\n",
    "        img_path = os.path.join(self.image_dir, filename + '.jpg')\n",
    "        img = Image.open(img_path)\n",
    "\n",
    "        return img\n",
    "    \n",
    "    # 해당되는 Index의 Annotation.xml을 통하여 \n",
    "    # Label,(xmin, ymin, xmax, ymax)을 반환-> 0~1사이의 값으로서 정규화 \n",
    "    def _get_annotation(self, index, orig_shape):\n",
    "        \"\"\" Method to read annotation from file\n",
    "            Boxes are normalized to image size\n",
    "            Integer labels are increased by 1\n",
    "\n",
    "        Args:\n",
    "            index: the index to get filename from self.ids\n",
    "            orig_shape: image's original shape\n",
    "\n",
    "        Returns:\n",
    "            boxes: numpy array of shape (num_gt, 4)\n",
    "            labels: numpy array of shape (num_gt,)\n",
    "        \"\"\"\n",
    "        h, w = orig_shape\n",
    "        filename = self.ids[index]\n",
    "        anno_path = os.path.join(self.anno_dir, filename + '.xml')\n",
    "        objects = ET.parse(anno_path).findall('object')\n",
    "        boxes = []\n",
    "        labels = []\n",
    "\n",
    "        for obj in objects:\n",
    "            name = obj.find('name').text.lower().strip()\n",
    "            bndbox = obj.find('bndbox')\n",
    "            xmin = (float(bndbox.find('xmin').text) - 1) / w\n",
    "            ymin = (float(bndbox.find('ymin').text) - 1) / h\n",
    "            xmax = (float(bndbox.find('xmax').text) - 1) / w\n",
    "            ymax = (float(bndbox.find('ymax').text) - 1) / h\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "            labels.append(self.name_to_idx[name] + 1)\n",
    "\n",
    "        return np.array(boxes, dtype=np.float32), np.array(labels, dtype=np.int64)\n",
    "    \n",
    "    # 실질적인 Dataset 생성을 위하여 필요\n",
    "    def generate(self, subset=None):\n",
    "        \"\"\" The __getitem__ method\n",
    "            so that the object can be iterable\n",
    "\n",
    "        Args:\n",
    "            index: the index to get filename from self.ids\n",
    "\n",
    "        Returns:\n",
    "            img: tensor of shape (300, 300, 3)\n",
    "            boxes: tensor of shape (num_gt, 4)\n",
    "            labels: tensor of shape (num_gt,)\n",
    "        \"\"\"\n",
    "        \n",
    "        # 만약 Train인 경우 File적용 Test라면 Filp 적용 X\n",
    "        if subset == 'train':\n",
    "            indices = self.train_ids\n",
    "            # 3. Random하게 Original Image를 사용할지 Flip을 실행할 Image를 사용할지 결정한다.\n",
    "            augmentation_method = np.random.choice(self.augmentation)\n",
    "            if augmentation_method == 'flip':\n",
    "                img, boxes, labels = horizontal_flip(img, boxes, labels)\n",
    "                \n",
    "        elif subset == 'val':\n",
    "            indices = self.val_ids\n",
    "        else:\n",
    "            indices = self.ids\n",
    "        for index in range(len(indices)):\n",
    "            # img, orig_shape = self._get_image(index)\n",
    "            filename = indices[index]\n",
    "            img = self._get_image(index)\n",
    "            \n",
    "            # 1. Input Image의 Size를 받는다.\n",
    "            w, h = img.size\n",
    "            \n",
    "            # 2. get_annotation()을 통하여 Label과 Bounding Box의 Location을 입력받는다.\n",
    "            boxes, labels = self._get_annotation(index, (h, w))\n",
    "            boxes = tf.constant(boxes, dtype=tf.float32)\n",
    "            labels = tf.constant(labels, dtype=tf.int64)\n",
    "            \n",
    "            # 4. Image의 Size를 Model Input에 맞게 (300,300)으로 바꾼뒤 0 ~ 1 사이의 값으로서 정규화를 한다.\n",
    "            img = np.array(img.resize(\n",
    "                (self.new_size, self.new_size)), dtype=np.float32)\n",
    "            img = (img / 127.0) - 1.0\n",
    "            img = tf.constant(img, dtype=tf.float32)\n",
    "            \n",
    "            # 5. Utils -> box_utils -> compute_target를 통하여 실제 Label을 Model에 맞는 Label로서 변경한다.\n",
    "            gt_confs, gt_locs = compute_target(\n",
    "                self.default_boxes, boxes, labels)\n",
    "\n",
    "            # 6. Filename, Image, Ground Truth Label, Ground Truth Location을 반환한다\n",
    "            # Generator로서 특정 Index후 다음 Index로 반환하기 위하여 Return 값을 yield로서 선언\n",
    "            yield filename, img, gt_confs, gt_locs\n",
    "\n",
    "# create_batch_generator(): Batch_Size를 입력받아 Dataset을 생성한다.\n",
    "def create_batch_generator(data_dir,default_boxes,batch_size, num_batches,\n",
    "                           mode):\n",
    "    num_examples = batch_size * num_batches if num_batches > 0 else -1\n",
    "    voc = VOCDataset(data_dir,default_boxes,num_examples)\n",
    "\n",
    "    info = {\n",
    "        'idx_to_name': voc.idx_to_name,\n",
    "        'name_to_idx': voc.name_to_idx,\n",
    "        'length': len(voc),\n",
    "        'image_dir': voc.image_dir,\n",
    "        'anno_dir': voc.anno_dir\n",
    "    }\n",
    "\n",
    "    if mode == 'train':\n",
    "        train_gen = partial(voc.generate, subset='train')\n",
    "        train_dataset = tf.data.Dataset.from_generator(\n",
    "            train_gen, (tf.string, tf.float32, tf.int64, tf.float32))\n",
    "        val_gen = partial(voc.generate, subset='val')\n",
    "        val_dataset = tf.data.Dataset.from_generator(\n",
    "            val_gen, (tf.string, tf.float32, tf.int64, tf.float32))\n",
    "\n",
    "        train_dataset = train_dataset.shuffle(40).batch(batch_size)\n",
    "        val_dataset = val_dataset.batch(batch_size)\n",
    "\n",
    "        return train_dataset.take(num_batches), val_dataset.take(-1), info\n",
    "    else:\n",
    "        dataset = tf.data.Dataset.from_generator(\n",
    "            voc.generate, (tf.string, tf.float32, tf.int64, tf.float32))\n",
    "        dataset = dataset.batch(batch_size)\n",
    "        return dataset.take(num_batches), info"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
