{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor\n",
    "Tensorflow 2.0에 맞게 다시 Tensorflow를 살펴볼 필요가 있다고 느껴져서 <a href=\"https://www.tensorflow.org/?hl=ko\">Tensorflow 정식 홈페이지</a>에 나와있는 예제부터 전반적인 Tensorflow 사용법을 먼저 익히는 Post가 된다.  \n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 필요한 Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is Tensor?\n",
    "Tnesorflow란 이름에서 알 수 있듯이 Tensor를 포함한 계산을 정의하고 실행하는 Framework이다.  \n",
    "**Tensor란 벡터와 행렬을 이차원한 것이고 고차원으로 확장 가능하다. 내부적으로 Tensorflow는 기본적으로 제공되는 자료형을 사용해 n-차원 배열로 나타낸다.**  \n",
    "\n",
    "Tensorflow는 다음과 같은 속성을 가지고 있다.\n",
    "- 자료형 ex) float32, int32, string, ...\n",
    "- 형태(shape)\n",
    "\n",
    "**Tensor안의 각각 원소는 동일한 자료형이다.**  \n",
    "Tensor의 대표적인 종류는 다음과 같다.\n",
    "- tf.Variable: Training으로서 Weight Update가 가능한 Tensor\n",
    "- tf.constant: Training으로서 Weight Update가 불가능한 Tensor (Weight Update시 tape.watch() 필요)\n",
    "- tf.SparseTensor: Sparse한 구조의 Tensor. Data의 대부분이 Sparse한 구조를 가지고 있기 때문에(Image 혹은 Lidar 형태의 Data인 경우) 지원한는 기능. Sparse하다는 것은 일반적으로 0(의미없는 데이터 ex) One-Hot-Encoding: Sparse, Embedding: Dense)을 많이 표현하고 있다고 생각하면 된다.\n",
    "- tf.ragged.: 비정형 데이터를 다루기 위하여 variable-length 한 Tensor 형태\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Rank\n",
    "Tensor의 Rank란 Tensor의 Dimension을 의미하게 되고 각각의 랭크는 다음과 같은 의미를 가집니다.  \n",
    "<table class=\"table\">\n",
    "\t<tr>\n",
    "\t\t<td>Rank</td>\n",
    "\t\t<td>Math entity</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>0</td>\n",
    "\t\t<td>Scalar(magnitude(값) only)</td>\n",
    "\t</tr>\t\n",
    "\t<tr>\n",
    "\t\t<td>1</td>\n",
    "\t\t<td>Vector(magnitude(값) and direction(방향))</td>\n",
    "\t</tr>\n",
    "    <tr>\n",
    "\t\t<td>2</td>\n",
    "\t\t<td>Matrix(table of numbers)</td>\n",
    "\t</tr>\n",
    "    <tr>\n",
    "\t\t<td>3</td>\n",
    "\t\t<td>3-Tensor(cube of numbers)</td>\n",
    "\t</tr>\n",
    "    <tr>\n",
    "\t\t<td>n</td>\n",
    "\t\t<td>n-Tensor</td>\n",
    "\t</tr>\n",
    "</table>\n",
    "\n",
    "이러한 Tensor의 Rank는 <code>tf.rank</code>로서 확인 가능합니다.  \n",
    "다차원의 Tensor의 경우에는 Indexing을 통하여 접근 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 0\n",
      "Rank of Rank 0 Tensor:  tf.Tensor(0, shape=(), dtype=int32)\n",
      "Value of Rank 0 Tensor:  b'Elephant' 451 3.1415927 (12.3-4.85j)\n",
      "\n",
      "Rank 1\n",
      "Rank of Rank 1 Tensor:  tf.Tensor(1, shape=(), dtype=int32)\n",
      "Value of Rank 0 Tensor:  [b'Hello'] [3.14159 2.71828] [ 2  3  5  7 11] [12.3-4.85j  7.5-6.23j]\n",
      "\n",
      "Higher Rank\n",
      "Rank of Higher Rank Tensor:  tf.Tensor(2, shape=(), dtype=int32) tf.Tensor(2, shape=(), dtype=int32) tf.Tensor(2, shape=(), dtype=int32) tf.Tensor(2, shape=(), dtype=int32) tf.Tensor(0, shape=(), dtype=int32) tf.Tensor(2, shape=(), dtype=int32)\n",
      "Value of Higher Rank Tensor:  [[ 7]\n",
      " [11]] [[False  True]\n",
      " [ True False]] [[ 4]\n",
      " [ 9]\n",
      " [16]\n",
      " [25]] [[ 4  9]\n",
      " [16 25]] 2 [[ 7]\n",
      " [11]]\n",
      "\n",
      "Indexing\n",
      "Rank of Rank my_image Tensor:  tf.Tensor(4, shape=(), dtype=int32)\n",
      "Value of Rank my_image Tensor\n",
      "[[[[0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]]]]\n",
      "my_image Tensor index: 0, value: [[[0. 0. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 0. 0.]]]\n",
      "my_image Tensor index: 1, value: [[[0. 0. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# Rank 0\n",
    "mammal = tf.Variable(\"Elephant\", tf.string)\n",
    "ignition = tf.Variable(451, tf.int16)\n",
    "floating = tf.Variable(3.14159265359, tf.float64)\n",
    "its_complicated = tf.Variable(12.3 - 4.85j, tf.complex64)\n",
    "print('Rank 0')\n",
    "print('Rank of Rank 0 Tensor: ',tf.rank(mammal))\n",
    "print('Value of Rank 0 Tensor: ',mammal.numpy(),ignition.numpy(),floating.numpy(),\n",
    "      its_complicated.numpy())\n",
    "print()\n",
    "\n",
    "# Rank 1\n",
    "mystr = tf.Variable([\"Hello\"], tf.string)\n",
    "cool_numbers  = tf.Variable([3.14159, 2.71828], tf.float32)\n",
    "first_primes = tf.Variable([2, 3, 5, 7, 11], tf.int32)\n",
    "its_very_complicated = tf.Variable([12.3 - 4.85j, 7.5 - 6.23j], tf.complex64)\n",
    "print('Rank 1')\n",
    "print('Rank of Rank 1 Tensor: ',tf.rank(mystr))\n",
    "print('Value of Rank 0 Tensor: ',mystr.numpy(),cool_numbers.numpy(),\n",
    "      first_primes.numpy(),its_very_complicated.numpy())\n",
    "print()\n",
    "\n",
    "# Higher Rank\n",
    "mymat = tf.Variable([[7],[11]], tf.int16)\n",
    "myxor = tf.Variable([[False, True],[True, False]], tf.bool)\n",
    "linear_squares = tf.Variable([[4], [9], [16], [25]], tf.int32)\n",
    "squarish_squares = tf.Variable([ [4, 9], [16, 25] ], tf.int32)\n",
    "rank_of_squares = tf.rank(squarish_squares)\n",
    "mymatC = tf.Variable([[7],[11]], tf.int32)\n",
    "print('Higher Rank')\n",
    "print('Rank of Higher Rank Tensor: ',tf.rank(mymat),tf.rank(myxor),\n",
    "      tf.rank(linear_squares),tf.rank(squarish_squares),tf.rank(rank_of_squares),\n",
    "      tf.rank(mymatC))\n",
    "print('Value of Higher Rank Tensor: ',mymat.numpy(),myxor.numpy(),linear_squares.numpy(),\n",
    "      squarish_squares.numpy(),rank_of_squares.numpy(),mymatC.numpy())\n",
    "print()\n",
    "\n",
    "# Indexing\n",
    "my_image = tf.zeros([2, 2, 2, 3])\n",
    "print('Indexing')\n",
    "print('Rank of Rank my_image Tensor: ',tf.rank(my_image))\n",
    "print('Value of Rank my_image Tensor')\n",
    "print(my_image.numpy())\n",
    "\n",
    "for i,value in enumerate(my_image):\n",
    "    print('my_image Tensor index: {}, value: {}'.format(i,value.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shape\n",
    "Shape란 Tensor에서 각각의 Dimension을 의미하게 된다.  \n",
    "이러한 Shape는 특정한 값으로서 선언하거나 None으로서 자동으로 생성 가능하게 지정할 수 있다.  \n",
    "각각의 Rank와 Shape는 다음과 같다.  \n",
    "<table class=\"table\">\n",
    "\t<tr>\n",
    "\t\t<td>Rank</td>\n",
    "\t\t<td>Shape</td>\n",
    "        <td>Dimension number</td>\n",
    "        <td>Example</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>0</td>\n",
    "\t\t<td>[]</td>\n",
    "        <td>0-D</td>\n",
    "        <td>A 0-D tensor. A scalar</td>\n",
    "\t</tr>\t\n",
    "\t<tr>\n",
    "\t\t<td>1</td>\n",
    "\t\t<td>[D0]</td>\n",
    "        <td>1-D</td>\n",
    "        <td>A 1-D tensor. tensor with shape[5]</td>\n",
    "\t</tr>\n",
    "    <tr>\n",
    "\t\t<td>2</td>\n",
    "\t\t<td>[D0, D1]</td>\n",
    "        <td>0-D</td>\n",
    "        <td>A 2-D tensor. tensor with shape[3,4]</td>\n",
    "\t</tr>\n",
    "    <tr>\n",
    "\t\t<td>3</td>\n",
    "\t\t<td>[D0, D1, D2]</td>\n",
    "        <td>3-D</td>\n",
    "        <td>A 3-D tensor. tensor with shape[1,4,3]</td>\n",
    "\t</tr>\n",
    "    <tr>\n",
    "\t\t<td>n</td>\n",
    "\t\t<td>[D0, D1, ..., Dn-1]</td>\n",
    "        <td>n-D</td>\n",
    "        <td>A n-D tensor. tensor with shape[D0,D1, ...,Dn-1]</td>\n",
    "\t</tr>\n",
    "</table>\n",
    "위와같은 Tensor의 Shape는 <code>.shape()</code>로서 접근 가능하고 Indexing이 가능하다.  \n",
    "또한 <code>tf.reshape</code>를 통하여 Tensor의 Shape가 변경 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5,)\n",
      "(3, 4)\n",
      "zeros_i value:  [0. 0. 0. 0.]\n",
      "zeros_i shape:  (4,)\n",
      "Original shape=(3,4) Tensor\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "Change shape=(4,3) Tensor\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Rank 1 Tensor\n",
    "zeros_1 = tf.zeros(shape=[5])\n",
    "print(zeros_1.shape)\n",
    "\n",
    "# Rank 2 Tensor\n",
    "zeros_2 = tf.zeros(shape=[3,4])\n",
    "print(zeros_2.shape)\n",
    "\n",
    "# Shape Indexing\n",
    "zeros_i = tf.zeros_like(zeros_2[1])\n",
    "print('zeros_i value: ',zeros_i.numpy())\n",
    "print('zeros_i shape: ',zeros_i.shape)\n",
    "\n",
    "# Reshape\n",
    "zeros_re = tf.reshape(zeros_2,[4,3])\n",
    "print('Original shape=(3,4) Tensor')\n",
    "print(zeros_2.numpy())\n",
    "print('Change shape=(4,3) Tensor')\n",
    "print(zeros_re.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 자료형\n",
    "Tensor는 2가지 속성을 가지고 있다고 설명하였다.  \n",
    "Shape와 자료형이다.  \n",
    "위에서 Shape는 살펴보았으니 자료형에 대해서 알아보자.  \n",
    "**기본적으로 Tensor는 한 개이상의 자료형을 가지는 것은 불가능하다.(임의의 데이터 구조를 직렬화한 string 제외)**  \n",
    "- <code>tf.Dtype</code> or <code>.dtype</code>: Tensor 자료형 확인\n",
    "- <code>tf.cast</code>: Tensor 자료형 변경\n",
    "\n",
    "Tensorflow에서 지원하는 자료형은 다음과 같다.  \n",
    "<table>\n",
    "    <tbody><tr><th style=\"width:432;height:24;border-bottom:1px solid #ccc;border-right:1px solid #ccc;border-top:1px solid #ccc;border-left:1px solid #ccc;;\"><p>&nbsp;자료형</p></th>\n",
    "<th style=\"width:432;height:24;border-bottom:1px solid #ccc;border-right:1px solid #ccc;border-top:1px solid #ccc;;\"><p>&nbsp;상세</p></th>\n",
    "</tr>\n",
    "<tr><td style=\"width:432;height:24;border-bottom:1px solid #ccc;border-right:1px solid #ccc;border-left:1px solid #ccc;;\"><p style=\"text-align: center;\">tensorflow.int8</p></td>\n",
    "<td style=\"width:432;height:24;border-bottom:1px solid #ccc;border-right:1px solid #ccc;;\"><p style=\"text-align: center;\">8비트 정수&nbsp;</p></td>\n",
    "</tr>\n",
    "<tr><td style=\"width:432;height:24;border-bottom:1px solid #ccc;border-right:1px solid #ccc;border-left:1px solid #ccc;;\"><p style=\"text-align: center;\">tensorflow.int16</p></td>\n",
    "<td style=\"width:432;height:24;border-bottom:1px solid #ccc;border-right:1px solid #ccc;;\"><p style=\"text-align: center;\">16비트 정수&nbsp;</p></td>\n",
    "</tr>\n",
    "<tr><td style=\"width:432;height:24;border-bottom:1px solid #ccc;border-right:1px solid #ccc;border-left:1px solid #ccc;;\"><p style=\"text-align: center;\">tensorflow.int32</p></td>\n",
    "<td style=\"width:432;height:24;border-bottom:1px solid #ccc;border-right:1px solid #ccc;;\"><p style=\"text-align: center;\">&nbsp;32비트 정수</p></td>\n",
    "</tr>\n",
    "<tr><td style=\"width:432;height:24;border-bottom:1px solid #ccc;border-right:1px solid #ccc;border-left:1px solid #ccc;;\"><p style=\"text-align: center;\">tensorflow.int64</p></td>\n",
    "<td style=\"width:432;height:24;border-bottom:1px solid #ccc;border-right:1px solid #ccc;;\"><p style=\"text-align: center;\">&nbsp;64비트 정수</p></td>\n",
    "</tr>\n",
    "<tr><td style=\"width:432;height:24;border-bottom:1px solid #ccc;border-right:1px solid #ccc;border-left:1px solid #ccc;;\"><p style=\"text-align: center;\">tensorflow.uint8</p></td>\n",
    "<td style=\"width:432;height:24;border-bottom:1px solid #ccc;border-right:1px solid #ccc;;\"><p style=\"text-align: center;\">8비트 0을 포함한 자연수&nbsp;</p></td>\n",
    "</tr>\n",
    "<tr><td style=\"width:432;height:24;border-bottom:1px solid #ccc;border-right:1px solid #ccc;border-left:1px solid #ccc;;\"><p style=\"text-align: center;\">&nbsp;tensorflow.string</p></td>\n",
    "<td style=\"width:432;height:24;border-bottom:1px solid #ccc;border-right:1px solid #ccc;;\"><p style=\"text-align: center;\">&nbsp;문자열</p></td>\n",
    "</tr>\n",
    "<tr><td style=\"width:432;height:24;border-bottom:1px solid #ccc;border-right:1px solid #ccc;border-left:1px solid #ccc;;\"><p style=\"text-align: center;\">&nbsp;tensorflow.bool</p></td>\n",
    "<td style=\"width:432;height:24;border-bottom:1px solid #ccc;border-right:1px solid #ccc;;\"><p style=\"text-align: center;\">&nbsp;부울린값(True,False)</p></td>\n",
    "</tr>\n",
    "<tr><td style=\"width:432;height:24;border-bottom:1px solid #ccc;border-right:1px solid #ccc;border-left:1px solid #ccc;;\"><p style=\"text-align: center;\">&nbsp;tensorflow.complex64</p></td>\n",
    "<td style=\"width:432;height:24;border-bottom:1px solid #ccc;border-right:1px solid #ccc;;\"><p style=\"text-align: center;\">&nbsp;복소수</p></td>\n",
    "</tr>\n",
    "<tr><td style=\"width:432;height:24;border-bottom:1px solid #ccc;border-right:1px solid #ccc;border-left:1px solid #ccc;;\"><p style=\"text-align: center;\">&nbsp;tensorflow.qint8</p></td>\n",
    "<td style=\"width:432;height:24;border-bottom:1px solid #ccc;border-right:1px solid #ccc;;\"><p style=\"text-align: center;\">&nbsp;양자화 명령어용 8비트 정수</p></td>\n",
    "</tr>\n",
    "\n",
    "<tr><td style=\"width:432;height:24;border-bottom:1px solid #ccc;border-right:1px solid #ccc;border-left:1px solid #ccc;;\"><p style=\"text-align: center;\">&nbsp;tensorflow.qint32</p></td>\n",
    "<td style=\"width:432;height:24;border-bottom:1px solid #ccc;border-right:1px solid #ccc;;\"><p style=\"text-align: center;\">&nbsp;양자화 명령어용 21비트 정수</p></td>\n",
    "</tr>\n",
    "\n",
    "<tr><td style=\"width:432;height:24;border-bottom:1px solid #ccc;border-right:1px solid #ccc;border-left:1px solid #ccc;;\"><p style=\"text-align: center;\">&nbsp;tensorflow.quint8</p></td>\n",
    "<td style=\"width:432;height:24;border-bottom:1px solid #ccc;border-right:1px solid #ccc;;\"><p style=\"text-align: center;\">&nbsp;양자화 명령어용 8비트 0을 포함한&nbsp;자연수</p></td>\n",
    "</tr>\n",
    "\n",
    "<tr><td style=\"width:432;height:24;border-bottom:1px solid #ccc;border-right:1px solid #ccc;border-left:1px solid #ccc;;\"><p style=\"text-align: center;\">&nbsp;tensorflow.float32</p></td>\n",
    "<td style=\"width:432;height:24;border-bottom:1px solid #ccc;border-right:1px solid #ccc;;\"><p style=\"text-align: center;\">&nbsp;32비트 실수</p></td>\n",
    "</tr>\n",
    "<tr><td style=\"width:432;height:24;border-bottom:1px solid #ccc;border-right:1px solid #ccc;border-left:1px solid #ccc;;\"><p style=\"text-align: center;\">&nbsp;tensorflow.float64</p></td>\n",
    "<td style=\"width:432;height:24;border-bottom:1px solid #ccc;border-right:1px solid #ccc;;\"><p style=\"text-align: center;\">&nbsp;64비트 실수</p></td>\n",
    "</tr>\n",
    "</tbody></table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor_int 값:  3\n",
      "tensor_int 자료형:  <dtype: 'int16'>\n",
      "tensor_float 값:  3.0\n",
      "tensor_float 자료형:  <dtype: 'int32'>\n"
     ]
    }
   ],
   "source": [
    "# int 형 자료형 선언\n",
    "tensor_int = tf.Variable(3,dtype=tf.int16)\n",
    "print('tensor_int 값: ',tensor_int.numpy())\n",
    "print('tensor_int 자료형: ',tensor_int.dtype)\n",
    "\n",
    "# int -> float으로 DType변경\n",
    "tensor_float = tf.cast(tensor_int,dtype=tf.float16)\n",
    "print('tensor_float 값: ',tensor_float.numpy())\n",
    "print('tensor_float 자료형: ',tf.DType(tensor_float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate tensors\n",
    "Graph를 생성하면 텐서에 할당된 값을 가져오는 계산이 가능하다.  \n",
    "Tensor를 계산하는 가장 간단한 방법은 <code>Tensor.eval</code> 메서드를 사용하는 것 이다.(Numpy값을 Return한다. => Tensor 1.x Version) \n",
    "\n",
    "<span style=\"color:red\">Tensorflow 2.0에서는 Eager Tensor가 생겼기 때문에 <code>Tensor.eval</code>이 아닌 바로 <code>.numpy()</code>로서 계산하고 값을 확인 가능하다. <code>Tensor.eval</code>사용시 Error가 발생하면서 <code>.numpy()</code>를 사용하라고 한다.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 4 9]\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "eval is not supported when eager execution is enabled, is .numpy() what you're looking for?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-3f0c04ebc21c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Tensorflow 2.0이상인 경우 tensor.eval() Error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m   1113\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m     raise NotImplementedError(\n\u001b[0;32m-> 1115\u001b[0;31m         \u001b[0;34m\"eval is not supported when eager execution is enabled, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1116\u001b[0m         \"is .numpy() what you're looking for?\")\n\u001b[1;32m   1117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: eval is not supported when eager execution is enabled, is .numpy() what you're looking for?"
     ]
    }
   ],
   "source": [
    "# Tensor 선언\n",
    "constant = tf.constant([1,2,3])\n",
    "# Graph 선언\n",
    "tensor = constant * constant\n",
    "# .numpy을 통하여 Tensor 계산 확인\n",
    "print(tensor.numpy())\n",
    "# Tensorflow 2.0이상인 경우 tensor.eval() Error\n",
    "print(tensor.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ragged tensors\n",
    "비정형 Data를 다루기 위하여 Variable Length가 지원되는 Tensor(Tensor Array라고 생각하면 편하다.)이다.  \n",
    "이러한 Tensor는 <code>tf.ragged.Tensor()</code>로서 선언되고 <code>tf.add()</code>, <code>tf.reduce_mean()</code>와 같이 math operation 그리고 <code>tf.concat()</code>, <code>tf.tile()</code>와 같이 array operation외에 많은 기능이 제공된다.  \n",
    "\n",
    "**<code>tf.ragged.map_flat_values()</code>**를 사용하게 되면 RaggedTensor에 Fuctnio을 적용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Math Operation\n",
      "<tf.RaggedTensor [[6, 4, 7, 4], [], [8, 12, 5], [9], []]>\n",
      "tf.Tensor([2.25              nan 5.33333333 6.                nan], shape=(5,), dtype=float64)\n",
      "\n",
      "Array Operation\n",
      "<tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9, 2], [6], [], [5, 3]]>\n",
      "<tf.RaggedTensor [[3, 1, 4, 1, 3, 1, 4, 1], [], [5, 9, 2, 5, 9, 2], [6, 6], []]>\n",
      "<tf.RaggedTensor [[b'So', b'lo'], [b'th', b'fo', b'al', b'th', b'fi']]>\n",
      "\n",
      "Indexing\n",
      "tf.Tensor([3 1 4 1], shape=(4,), dtype=int32)\n",
      "<tf.RaggedTensor [[3, 1], [], [5, 9], [6], []]>\n",
      "<tf.RaggedTensor [[3, 1], [], [5], [], []]>\n",
      "\n",
      "Original\n",
      "<tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9, 2], [6], []]>\n",
      "Function: 2x+1\n",
      "tf.ragged.map_flat_values() Result\n",
      "<tf.RaggedTensor [[7, 3, 9, 3], [], [11, 19, 5], [13], []]>\n"
     ]
    }
   ],
   "source": [
    "# Ragged Tensor 선언\n",
    "digits = tf.ragged.constant([[3, 1, 4, 1], [], [5, 9, 2], [6], []])\n",
    "words = tf.ragged.constant([[\"So\", \"long\"], [\"thanks\", \"for\", \"all\", \"the\", \"fish\"]])\n",
    "\n",
    "# Math Operation 확인\n",
    "print('Math Operation')\n",
    "print(tf.add(digits, 3))\n",
    "print(tf.reduce_mean(digits, axis=1))\n",
    "print()\n",
    "\n",
    "# Array Operation 확인\n",
    "print('Array Operation')\n",
    "print(tf.concat([digits, [[5, 3]]], axis=0))\n",
    "print(tf.tile(digits, [1, 2]))\n",
    "print(tf.strings.substr(words, 0, 2))\n",
    "print()\n",
    "\n",
    "# Indexing\n",
    "print('Indexing')\n",
    "print(digits[0])\n",
    "\n",
    "# First two values in each row\n",
    "print(digits[:, :2])\n",
    "\n",
    "# Last two values in each row\n",
    "print(digits[:, :-2])\n",
    "print()\n",
    "\n",
    "# tf.ragged.map_flat_values\n",
    "print('Original')\n",
    "print(digits)\n",
    "print('Function: 2x+1')\n",
    "times_two_plus_one = lambda x: x * 2 + 1\n",
    "print('tf.ragged.map_flat_values() Result')\n",
    "print(tf.ragged.map_flat_values(times_two_plus_one, digits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construction a ragged tensor\n",
    "**1. tf.ragged.constant**  \n",
    "RaggedTensor를 정의하는 가장 쉬운 방법이다. Python의 List와 같이 정의하면 된다.  \n",
    "참조: <a href=\"https://www.tensorflow.org/api_docs/python/tf/ragged/constant?version=stable\">tf.ragged.constant 사용법</a>\n",
    "<br>\n",
    "\n",
    "**2. tf.RaggedTensor.from_value_rowids**  \n",
    "Python의 List에 각각의 List의 원소와 그 원소의 Row를 아는 경우 사용하는 방법이다.  \n",
    "그림으로 나타내면 다음과 같다.  \n",
    "<img src=\"https://www.tensorflow.org/images/ragged_tensors/value_rowids.png\"><br>\n",
    "사진 참조: <a href=\"https://www.tensorflow.org/guide/ragged_tensor\">ragged tensor guide</a><br>\n",
    "<br>\n",
    "\n",
    "**3. tf.RaggedTensor.from_row_lengths**  \n",
    "Python의 List에 각각의 List의 원소와 Row당 원소의 개수가 알 때 사용한다.  \n",
    "<img src=\"https://www.tensorflow.org/images/ragged_tensors/row_lengths.png\"><br>\n",
    "사진 참조: <a href=\"https://www.tensorflow.org/guide/ragged_tensor\">ragged tensor guide</a><br>\n",
    "<br>\n",
    "\n",
    "**4. tf.RaggedTensor.from_row_splits**  \n",
    "Python의 List에 각각의 List의 원소와 각 Row의 시작 Index를 알 때 사용한다.    \n",
    "<img src=\"https://www.tensorflow.org/images/ragged_tensors/row_splits.png\"><br>\n",
    "사진 참조: <a href=\"https://www.tensorflow.org/guide/ragged_tensor\">ragged tensor guide</a><br> \n",
    "참조: <a href=\"https://www.tensorflow.org/api_docs/python/tf/RaggedTensor?version=stable#from_value_rowids\">tf.ragged 사용법</a>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[[b'I', b'have', b'a', b'cat'], [b'His', b'name', b'is', b'Mat']], [[b'Do', b'you', b'want', b'to', b'come', b'visit'], [b\"I'm\", b'free', b'tomorrow']]]>\n",
      "<tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9], [2]]>\n",
      "<tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9], [2]]>\n",
      "<tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9], [2]]>\n"
     ]
    }
   ],
   "source": [
    "# tf.ragged.constant\n",
    "print(tf.ragged.constant([\n",
    "    [['I', 'have', 'a', 'cat'], ['His', 'name', 'is', 'Mat']],\n",
    "    [['Do', 'you', 'want', 'to', 'come', 'visit'], [\"I'm\", 'free', 'tomorrow']],\n",
    "]))\n",
    "\n",
    "# tf.RaggedTensor.from_value_rowids\n",
    "print(tf.RaggedTensor.from_value_rowids(\n",
    "    values=[3, 1, 4, 1, 5, 9, 2],\n",
    "    value_rowids=[0, 0, 0, 0, 2, 2, 3]))\n",
    "\n",
    "# tf.RaggedTensor.from_row_lengths\n",
    "print(tf.RaggedTensor.from_row_lengths(\n",
    "    values=[3, 1, 4, 1, 5, 9, 2],\n",
    "    row_lengths=[4, 0, 2, 1]))\n",
    "\n",
    "# tf.RaggedTensor.from_row_splits\n",
    "print(tf.RaggedTensor.from_row_splits(\n",
    "    values=[3, 1, 4, 1, 5, 9, 2],\n",
    "    row_splits=[0, 4, 4, 6, 7]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### what you can store in a ragged tensor\n",
    "Tensor처럼 각각의 Element의 dtype은 일치하여야 하고 각 values의 depth(rank of tensor)가 일치하여야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor_s_2 value:  <tf.RaggedTensor [[b'Hi'], [b'How', b'are', b'you']]>\n",
      "tensor_s_2 type:  <dtype: 'string'>\n",
      "\n",
      "tensor_i_3 value:  <tf.RaggedTensor [[[1, 2], [3]], [[4, 5]]]>\n",
      "tensor_i_3 type:  <dtype: 'int32'>\n",
      "\n",
      "Multiple types exception\n",
      "Can't convert Python sequence with mixed types to Tensor.\n",
      "\n",
      "Multiple nesting depths exception\n",
      "all scalar values must have the same nesting depth\n"
     ]
    }
   ],
   "source": [
    "# Type = String, rank = 2\n",
    "tensor_s_2 = tf.ragged.constant([[\"Hi\"], [\"How\", \"are\", \"you\"]])\n",
    "print('tensor_s_2 value: ',tensor_s_2)\n",
    "print('tensor_s_2 type: ',tensor_s_2.dtype)\n",
    "print()\n",
    "\n",
    "# Type = int32, rank = 3\n",
    "tensor_i_3 = tf.ragged.constant([[[1, 2], [3]], [[4, 5]]])\n",
    "print('tensor_i_3 value: ',tensor_i_3)\n",
    "print('tensor_i_3 type: ',tensor_i_3.dtype)\n",
    "print()\n",
    "\n",
    "# Multiple types\n",
    "print('Multiple types exception')\n",
    "try:\n",
    "    tf.ragged.constant([[\"one\", \"two\"], [3, 4]])\n",
    "except ValueError as exception:\n",
    "    print(exception)\n",
    "print()\n",
    "\n",
    "# Multiple nesting depths\n",
    "print('Multiple nesting depths exception')\n",
    "try:\n",
    "    tf.ragged.constant([\"A\", [\"B\", \"C\"]])\n",
    "except ValueError as exception:\n",
    "    print(exception)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example use cases\n",
    "<code>tf.ragged.constant</code>가 사용되는 예시이다.  \n",
    "다음 과정은 Data의 전처리 과정으로서 Bigram과 Embedding을 통하여 단어를 Tensor로 바꾸는 과정이다. 전체적인 과정은 다음과 같다.  \n",
    "<img src=\"https://www.tensorflow.org/images/ragged_tensors/ragged_example.png\"><br>\n",
    "사진 참조: <a href=\"https://www.tensorflow.org/guide/ragged_tensor\">ragged tensor guide</a><br> \n",
    "Input tf.ragged.constant()가 어떻게 변하는지 상세하게 출력하였다.  \n",
    "아래 과정이 이해가 안되시면 다음 링크를 참조하자.\n",
    "- <a href=\"https://wjddyd66.github.io/dl/%EC%9E%90%EC%97%B0%EC%96%B4%EC%99%80-%EB%8B%A8%EC%96%B4%EC%9D%98-%EB%B6%84%EC%82%B0-%ED%91%9C%ED%98%84/\">자연어와 단어의 분산 표현</a>\n",
    "- <a href=\"https://wjddyd66.github.io/dl/word2vec/\">word2vec</a>\n",
    "- <a href=\"https://wjddyd66.github.io/dl/Fast-word2vec/\">Fast word2vec</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Table Value\n",
      "<tf.Variable 'Variable:0' shape=(1024, 4) dtype=float32, numpy=\n",
      "array([[-0.26984987,  0.33821583, -0.25303894,  0.01355879],\n",
      "       [-0.31692794,  0.2507733 , -0.04176231,  0.47682792],\n",
      "       [ 0.138528  ,  0.8105369 , -0.1459025 , -0.25926843],\n",
      "       ...,\n",
      "       [ 0.03473174,  0.71605283,  0.3699363 , -0.00614554],\n",
      "       [ 0.01484714, -0.49983358,  0.4236563 ,  0.83395946],\n",
      "       [ 0.29265305, -0.02712937,  0.13999745,  0.6439858 ]],\n",
      "      dtype=float32)>\n",
      "Embedding Table Max Value 0.99935836\n",
      "Embedding Table Min Value -0.9903946\n",
      "Embedding Table Std Value 0.44566667\n",
      "\n",
      "Wrod Buckets\n",
      "[b'Who' b'is' b'Dan' b'Smith'] -> [633 768 237 309]\n",
      "[b'Pause'] -> [28]\n",
      "[b'Who' b'is' b'Hwang' b'Jeong' b'Yong'] -> [633 768 872 282 283]\n",
      "\n",
      "Word Embedding\n",
      "[633 768 237 309] -> \n",
      "[[-0.20994134 -0.1857289   0.5846876   0.18298072]\n",
      " [-0.32397014  0.77367496  0.09295609 -0.7025036 ]\n",
      " [-0.44442365 -0.49605316 -0.23920043  0.35544553]\n",
      " [-0.15591177 -0.9721323  -0.24233624  0.3026163 ]]\n",
      "[28] -> \n",
      "[[ 0.75021434  0.2933693   0.39557642 -0.45455787]]\n",
      "[633 768 872 282 283] -> \n",
      "[[-0.20994134 -0.1857289   0.5846876   0.18298072]\n",
      " [-0.32397014  0.77367496  0.09295609 -0.7025036 ]\n",
      " [ 0.42836288  0.22043757  0.5645658  -0.17434597]\n",
      " [-0.0612235  -0.22269532 -0.05117381 -0.70351416]\n",
      " [ 0.53572744  0.5649934   0.7251783  -0.05632596]]\n",
      "\n",
      "Padded\n",
      "[b'Who' b'is' b'Dan' b'Smith'] -> [b'#' b'Who' b'is' b'Dan' b'Smith' b'#']\n",
      "[b'Pause'] -> [b'#' b'Pause' b'#']\n",
      "[b'Who' b'is' b'Hwang' b'Jeong' b'Yong'] -> [b'#' b'Who' b'is' b'Hwang' b'Jeong' b'Yong' b'#']\n",
      "\n",
      "Bigrams\n",
      "[b'#' b'Who' b'is' b'Dan' b'Smith' b'#'] -> [b'#+Who' b'Who+is' b'is+Dan' b'Dan+Smith' b'Smith+#']\n",
      "[b'#' b'Pause' b'#'] -> [b'#+Pause' b'Pause+#']\n",
      "[b'#' b'Who' b'is' b'Hwang' b'Jeong' b'Yong' b'#'] -> [b'#+Who' b'Who+is' b'is+Hwang' b'Hwang+Jeong' b'Jeong+Yong' b'Yong+#']\n",
      "\n",
      "Look up embeddings\n",
      "[b'#+Who' b'Who+is' b'is+Dan' b'Dan+Smith' b'Smith+#'] ->\n",
      "[[ 0.25747493  0.13615859  0.5341867  -0.85479945]\n",
      " [-0.3306522   0.01117908 -0.69294995 -0.18351139]\n",
      " [-0.83060795 -0.41425812  0.36051166 -0.9900611 ]\n",
      " [-0.21996655 -0.48888227 -0.2747723  -0.05589481]\n",
      " [-0.57145834  0.81615496 -0.31114626 -0.38524503]]\n",
      "[b'#+Pause' b'Pause+#'] ->\n",
      "[[ 0.60230803  0.13398379 -0.05554185 -0.285491  ]\n",
      " [ 0.0515134   0.1544675  -0.6966816   0.15531673]]\n",
      "[b'#+Who' b'Who+is' b'is+Hwang' b'Hwang+Jeong' b'Jeong+Yong' b'Yong+#'] ->\n",
      "[[ 0.25747493  0.13615859  0.5341867  -0.85479945]\n",
      " [-0.3306522   0.01117908 -0.69294995 -0.18351139]\n",
      " [ 0.8724997   0.62738943 -0.35298598  0.21889828]\n",
      " [-0.78824115 -0.7772786   0.30318534 -0.6767643 ]\n",
      " [-0.18517442  0.42661405  0.27245098 -0.44607124]\n",
      " [-0.22832447  0.99909246  0.52889496 -0.26612002]]\n",
      "\n",
      "Average Embedding\n",
      "tf.Tensor(\n",
      "[[-0.3143841  -0.09109858 -0.02089591 -0.25899696]\n",
      " [ 0.46801195  0.1939402  -0.11888235 -0.19491072]\n",
      " [-0.00304202  0.23398517  0.22809054 -0.33291608]], shape=(3, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "queries = tf.ragged.constant([['Who', 'is', 'Dan', 'Smith'],\n",
    "                              ['Pause'],\n",
    "                              ['Who', 'is', 'Hwang', 'Jeong', 'Yong']])\n",
    "\n",
    "# Create an embedding table.\n",
    "num_buckets = 1024\n",
    "embedding_size = 4\n",
    "embedding_table = tf.Variable(\n",
    "    tf.random.truncated_normal([num_buckets, embedding_size],\n",
    "                       stddev=1.0 / math.sqrt(embedding_size)))\n",
    "print('Embedding Table Value')\n",
    "print(embedding_table)\n",
    "print('Embedding Table Max Value',np.max(embedding_table.numpy()))\n",
    "print('Embedding Table Min Value',np.min(embedding_table.numpy()))\n",
    "print('Embedding Table Std Value',np.std(embedding_table.numpy()))\n",
    "print()\n",
    "\n",
    "# Look up the embedding for each word.\n",
    "word_buckets = tf.strings.to_hash_bucket_fast(queries, num_buckets)\n",
    "word_embeddings = tf.ragged.map_flat_values(\n",
    "    tf.nn.embedding_lookup, embedding_table, word_buckets)\n",
    "\n",
    "print('Wrod Buckets')\n",
    "for i,value in enumerate(word_buckets):\n",
    "    print('{} -> {}'.format(queries[i].numpy(), value.numpy()))\n",
    "print()\n",
    "\n",
    "print('Word Embedding')\n",
    "for i,value in enumerate(word_embeddings):\n",
    "    print('{} -> '.format(word_buckets[i].numpy()))\n",
    "    print(value.numpy())\n",
    "print()\n",
    "\n",
    "# Add markers to the beginning and end of each sentence.\n",
    "marker = tf.fill([queries.nrows(), 1], '#')\n",
    "padded = tf.concat([marker, queries, marker], axis=1)\n",
    "\n",
    "print('Padded')\n",
    "for i,value in enumerate(padded):\n",
    "    print('{} -> {}'.format(queries[i].numpy(), value.numpy()))\n",
    "print()\n",
    "\n",
    "# Build word bigrams & look up embeddings.\n",
    "bigrams = tf.strings.join([padded[:, :-1],\n",
    "                               padded[:, 1:]],\n",
    "                              separator='+')  \n",
    "\n",
    "print('Bigrams')\n",
    "for i,value in enumerate(bigrams):\n",
    "    print('{} -> {}'.format(padded[i].numpy(), value.numpy()))\n",
    "print()\n",
    "\n",
    "print('Look up embeddings')\n",
    "bigram_buckets = tf.strings.to_hash_bucket_fast(bigrams, num_buckets)\n",
    "bigram_embeddings = tf.ragged.map_flat_values(\n",
    "    tf.nn.embedding_lookup, embedding_table, bigram_buckets)\n",
    "\n",
    "for i,value in enumerate(bigram_embeddings):\n",
    "    print('{} ->'.format(bigrams[i].numpy()))\n",
    "    print(value.numpy())\n",
    "print()\n",
    "\n",
    "# Find the average embedding for each sentence\n",
    "print('Average Embedding')\n",
    "all_embeddings = tf.concat([word_embeddings, bigram_embeddings], axis=1)\n",
    "avg_embedding = tf.reduce_mean(all_embeddings, axis=1)\n",
    "print(avg_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RaggedTensor shape\n",
    "RaggedTensor의 Shape는 Tensor와 마찬가지 <code>.shape()</code>를 통하여 알 수 있다.  \n",
    "하지만 다음과 같은 Tensor가 정의되어있을때 shape를 생각해 보자.  \n",
    "<code>tf.ragged.constant([[\"Hi\"], [\"How\", \"are\", \"you\"]]).shape</code>  \n",
    "위의 Code는 Shape가 (2,1) 이라고 표현해야 하는가 (2,3)이라고 표현해야 하는지 정할 수 없다.  \n",
    "따라서 Tensorflow의 결과는 (2,None)으로서 출력된다.  \n",
    "**이러한 RaggedTensor 특성 때문에 <code>.bounding_shape()</code>를 사용하게 된다.**  \n",
    ".bounding_shape()의 출력 형태를 보면 다음과 같다.  \n",
    "<code>tf.Tensor([2 3], shape=(2,), dtype=int64)</code>\n",
    "- [2,3]: 최대 Shape\n",
    "- shape=(2,): 2차원 이나 나머지 차원은 다르다.\n",
    "- dtype: DType\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, None)\n",
      "tf.Tensor([2 3], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# Ragged Tensor 선언\n",
    "r_tensor = tf.ragged.constant([[\"Hi\"], [\"How\", \"are\", \"you\"]])\n",
    "# .shape 사용\n",
    "print(r_tensor.shape)\n",
    "#. bounding_shape 사용\n",
    "print(r_tensor.bounding_shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ragged vs sparse Tensor\n",
    "이런 특이한 Raggend Tensor를 합치는 방법은 2가지가 있다.  \n",
    "<code>tf.concat</code>으로서 tf.Ragged.Tensor + tf.Ragged.Tensor = tf.Ragged.Tensor으로 나타애는 방법과  \n",
    "<img src=\"https://www.tensorflow.org/images/ragged_tensors/ragged_concat.png\"><br>\n",
    "사진 참조: <a href=\"https://www.tensorflow.org/guide/ragged_tensor\">ragged tensor guide</a><br>\n",
    "<code>tf.sparse.concat</code>으로서 tf.SparseTensorCasting(tf.Ragged.Tensor) + tf.SparseTensorCasting(tf.Ragged.Tensor) = tf.Tensor로서 나타내는 방법이 존재한다.  \n",
    "<img src=\"https://www.tensorflow.org/images/ragged_tensors/sparse_concat.png\"><br>\n",
    "사진 참조: <a href=\"https://www.tensorflow.org/guide/ragged_tensor\">ragged tensor guide</a><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ragged_concat\n",
      "<tf.RaggedTensor [[b'John', b'fell', b'asleep'], [b'a', b'big', b'dog', b'barked'], [b'my', b'cat', b'is', b'fuzzy']]>\n",
      "\n",
      "sparse_concat\n",
      "SparseTensor(indices=tf.Tensor(\n",
      "[[0 0]\n",
      " [0 3]\n",
      " [0 4]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [1 2]\n",
      " [1 3]\n",
      " [2 0]\n",
      " [2 1]\n",
      " [2 3]\n",
      " [2 4]], shape=(11, 2), dtype=int64), values=tf.Tensor(\n",
      "[b'John' b'fell' b'asleep' b'a' b'big' b'dog' b'barked' b'my' b'cat' b'is'\n",
      " b'fuzzy'], shape=(11,), dtype=string), dense_shape=tf.Tensor([3 5], shape=(2,), dtype=int64))\n",
      "\n",
      "saprse -> dense\n",
      "tf.Tensor(\n",
      "[[b'John' b'' b'' b'fell' b'asleep']\n",
      " [b'a' b'big' b'dog' b'barked' b'']\n",
      " [b'my' b'cat' b'' b'is' b'fuzzy']], shape=(3, 5), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# ragged_concat\n",
    "ragged_x = tf.ragged.constant([[\"John\"], [\"a\", \"big\", \"dog\"], [\"my\", \"cat\"]])\n",
    "ragged_y = tf.ragged.constant([[\"fell\", \"asleep\"], [\"barked\"], [\"is\", \"fuzzy\"]])\n",
    "print('ragged_concat')\n",
    "print(tf.concat([ragged_x, ragged_y], axis=1))\n",
    "print()\n",
    "\n",
    "# sparse_concat\n",
    "# Spaese Tensor로 변경\n",
    "sparse_x = ragged_x.to_sparse()\n",
    "sparse_y = ragged_y.to_sparse()\n",
    "# sparse_concat수행\n",
    "sparse_result = tf.sparse.concat(sp_inputs=[sparse_x, sparse_y], axis=1)\n",
    "print('sparse_concat')\n",
    "print(sparse_result)\n",
    "print()\n",
    "\n",
    "# spase -> dense 수행\n",
    "print('saprse -> dense')\n",
    "print(tf.sparse.to_dense(sparse_result, ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overload operators\n",
    "Ragged Tensor를 계산하기 위해서는 Ragged Tensor끼리는 shape가 동일하여야 하고 하나의 Element와 Ragged Tensor가 연산을 수행하게 되면 모든 Ragged Tensor의 Element와 하나의 Element의 연산을 수행(Broad Casting)하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.ragged.constant + tf.ragged.constant\n",
      "x shape:  (3, None)\n",
      "y shape:  (3, None)\n",
      "x + y = :  <tf.RaggedTensor [[2, 3], [5], [7, 8, 9]]>\n",
      "\n",
      "tf.ragged.constant + Element\n",
      "<tf.RaggedTensor [[4, 5], [6], [7, 8, 9]]>\n"
     ]
    }
   ],
   "source": [
    "print('tf.ragged.constant + tf.ragged.constant')\n",
    "x = tf.ragged.constant([[1, 2], [3], [4, 5, 6]])\n",
    "y = tf.ragged.constant([[1, 1], [2], [3, 3, 3]])\n",
    "print('x shape: ',x.shape)\n",
    "print('y shape: ',x.shape)\n",
    "print('x + y = : ',x+y)\n",
    "print()\n",
    "\n",
    "print('tf.ragged.constant + Element')\n",
    "print(x + 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexing\n",
    "Ragged Tensor또한 indexing이 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queris[1] =  tf.Tensor([b'What' b'is' b'the' b'weather' b'tomorrow'], shape=(5,), dtype=string)\n",
      "queris[1,2] =  tf.Tensor(b'the', shape=(), dtype=string)\n",
      "queris[1:] =  <tf.RaggedTensor [[b'What', b'is', b'the', b'weather', b'tomorrow'], [b'Goodnight']]>\n",
      "queris[:,:3] =  <tf.RaggedTensor [[b'Who', b'is', b'George'], [b'What', b'is', b'the'], [b'Goodnight']]>\n",
      "queris[:,-2:] =  <tf.RaggedTensor [[b'George', b'Washington'], [b'weather', b'tomorrow'], [b'Goodnight']]>\n"
     ]
    }
   ],
   "source": [
    "queries = tf.ragged.constant(\n",
    "    [['Who', 'is', 'George', 'Washington'],\n",
    "     ['What', 'is', 'the', 'weather', 'tomorrow'],\n",
    "     ['Goodnight']])\n",
    "\n",
    "print('queris[1] = ',queries[1])\n",
    "\n",
    "# A single word\n",
    "print('queris[1,2] = ',queries[1,2])\n",
    "\n",
    "# Everything but the first row\n",
    "print('queris[1:] = ',queries[1:])\n",
    "\n",
    "# The first 3 words of each query\n",
    "print('queris[:,:3] = ',queries[:,:3])\n",
    "\n",
    "# The last 2 word of each query\n",
    "print('queris[:,-2:] = ',queries[:,-2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensor Type Conversion\n",
    "Ragged Tensor -> Tensor or Tensor -> Ragged Tensor or Sparse Tensor -> Ragged Tensor 등 Raggend Tensor또한 다양한 Type Conversion이 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ragged Tensor\n",
      "<tf.RaggedTensor [[b'Hi'], [b'Welcome', b'to', b'the', b'fair'], [b'Have', b'fun']]>\n",
      "\n",
      "Ragged Tensor -> Tensor\n",
      "Tensor\n",
      "tf.Tensor(\n",
      "[[b'Hi' b'' b'' b'']\n",
      " [b'Welcome' b'to' b'the' b'fair']\n",
      " [b'Have' b'fun' b'' b'']], shape=(3, 4), dtype=string)\n",
      "\n",
      "Ragged Tensor -> Sparse Tensor\n",
      "Sparse Tensor\n",
      "SparseTensor(indices=tf.Tensor(\n",
      "[[0 0]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [1 2]\n",
      " [1 3]\n",
      " [2 0]\n",
      " [2 1]], shape=(7, 2), dtype=int64), values=tf.Tensor([b'Hi' b'Welcome' b'to' b'the' b'fair' b'Have' b'fun'], shape=(7,), dtype=string), dense_shape=tf.Tensor([3 4], shape=(2,), dtype=int64))\n",
      "\n",
      "\n",
      "Tensor -> Ragged Tensor\n",
      "Tensor\n",
      "<tf.Variable 'Variable:0' shape=(3, 4) dtype=int32, numpy=\n",
      "array([[ 1,  3, -1, -1],\n",
      "       [ 2, -1, -1, -1],\n",
      "       [ 4,  5,  8,  9]], dtype=int32)>\n",
      "Ragged Tensor\n",
      "<tf.RaggedTensor [[1, 3], [2], [4, 5, 8, 9]]>\n",
      "\n",
      "\n",
      "Sparse Tensor -> Ragged Tensor\n",
      "Sparse Tensor\n",
      "SparseTensor(indices=tf.Tensor(\n",
      "[[0 0]\n",
      " [2 0]\n",
      " [2 1]], shape=(3, 2), dtype=int64), values=tf.Tensor([b'a' b'b' b'c'], shape=(3,), dtype=string), dense_shape=tf.Tensor([3 3], shape=(2,), dtype=int64))\n",
      "Ragged Tensor\n",
      "<tf.RaggedTensor [[b'a'], [], [b'b', b'c']]>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "ragged_sentences = tf.ragged.constant([\n",
    "    ['Hi'], ['Welcome', 'to', 'the', 'fair'], ['Have', 'fun']])\n",
    "\n",
    "print('Ragged Tensor')\n",
    "print(ragged_sentences)\n",
    "print()\n",
    "\n",
    "print('Ragged Tensor -> Tensor')\n",
    "print('Tensor')\n",
    "print(ragged_sentences.to_tensor(default_value=''))\n",
    "print()\n",
    "\n",
    "print('Ragged Tensor -> Sparse Tensor')\n",
    "print('Sparse Tensor')\n",
    "print(ragged_sentences.to_sparse())\n",
    "print()\n",
    "print()\n",
    "\n",
    "print('Tensor -> Ragged Tensor')\n",
    "x = tf.Variable([[1, 3, -1, -1], [2, -1, -1, -1], [4, 5, 8, 9]])\n",
    "print('Tensor')\n",
    "print(x)\n",
    "print('Ragged Tensor')\n",
    "print(tf.RaggedTensor.from_tensor(x, padding=-1))\n",
    "print()\n",
    "print()\n",
    "\n",
    "print('Sparse Tensor -> Ragged Tensor')\n",
    "st = tf.SparseTensor(indices=[[0, 0], [2, 0], [2, 1]],\n",
    "                     values=['a', 'b', 'c'],\n",
    "                     dense_shape=[3, 3])\n",
    "print('Sparse Tensor')\n",
    "print(st)\n",
    "print('Ragged Tensor')\n",
    "print(print(tf.RaggedTensor.from_sparse(st)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation ragged tensors\n",
    "**Ragged Tensors를 Tensor의 List(집합)** 이라고 표현하였다.  \n",
    "따라서 Ragged Tensor는 이전 Post <a href=\"https://wjddyd66.github.io/tnesorflow2.0/Tensorflow2.0(6)/\">Eager execution</a>에서 다루었던 Eager Tensor처럼 .numpy()로서 바로 Value를 확인하지 못한다.  \n",
    "Ragged Tensors는 Tensor의 List이므로 <code>.to_list()</code>로서 Python List형태로서 바꾼다.  \n",
    "또한 Ragged Tensors의 각각의 Element들은 EagerTensor이므로 <code>.numpy()</code>로서 값이 확인 가능하다.  \n",
    "또한 위에서 언급하였던 **Construction a ragged tensor**처럼 Ragged Tensor를 나타내는 다양한 방법을 통하여 특정 값을 확인 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ragged Tensor\n",
      "<tf.RaggedTensor [[1, 2], [3, 4, 5], [6], [], [7]]>\n",
      "\n",
      "Ragged Tensor -> List\n",
      "List Value: [[1, 2], [3, 4, 5], [6], [], [7]], List Type: <class 'list'>\n",
      "\n",
      "Ragged Tensor -> Indexing -> Eager Tensor\n",
      "Eager Tensor: [3 4 5], Eager Tensor: <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "\n",
      "Ragged Tensor Value[1 2 3 4 5 6 7]\n",
      "Ragged Tensor row_splits[0 2 5 6 6 7]\n"
     ]
    }
   ],
   "source": [
    "# Ragged Tensor 생성\n",
    "rt = tf.ragged.constant([[1, 2], [3, 4, 5], [6], [], [7]])\n",
    "print('Ragged Tensor')\n",
    "print(rt)\n",
    "print()\n",
    "\n",
    "# Ragged Tensor -> List\n",
    "rt_list = rt.to_list()\n",
    "print('Ragged Tensor -> List')\n",
    "print('List Value: {}, List Type: {}'.format(rt_list, type(rt_list)))\n",
    "print()\n",
    "\n",
    "# Ragged Tensor -> Indexing -> Eager Tensor\n",
    "rt_eager = rt[1]\n",
    "print('Ragged Tensor -> Indexing -> Eager Tensor')\n",
    "print('Eager Tensor: {}, Eager Tensor: {}'.format(rt_eager, type(rt_eager)))\n",
    "print()\n",
    "\n",
    "# Function\n",
    "print('Ragged Tensor Value{}'.format(rt.values))\n",
    "print('Ragged Tensor row_splits{}'.format(rt.row_splits))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
