{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess Data\n",
    "Tensorflow 2.0에 맞게 다시 Tensorflow를 살펴볼 필요가 있다고 느껴져서 <a href=\"https://www.tensorflow.org/?hl=ko\">Tensorflow 정식 홈페이지</a>에 나와있는 예제부터 전반적인 Tensorflow 사용법을 먼저 익히는 Post가 된다.  \n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 필요한 Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import functools\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.data\n",
    "<code>tf.data</code>는 Input Data에 대하여 복잡한 전처리 과정이나 Loading하는 과정을 쉽게 할 수 있게 지원하는 것 이다.  \n",
    "위의 과정으로 Input Data -> tf.data -> Input Tensor로서 편하게 Pipelines를 구출할 수 있는 것 이다.  \n",
    "tf.data API를 사용하면 많은 양의 데이터를 처리하고 서로 다른 데이터 형식에서 데이터를 읽고 복잡한 변환(전처리 과정)을 수행할 수 있다.  \n",
    "이러한 dataset을 구축하기 위해서는 다음과 같은 2가지의 조건이 필요하다.\n",
    "- Dataset은 Memory 혹은 File로서 저장되어있어야 한다.\n",
    "- Transformation은 tf.data.Dataset object에서 행해 진다. ex) Image의 경우 tf.data.Dataset으로 Object로 만들고 회전변환 혹은 Scaling등 다양한 Transformation을 실시한다.\n",
    "\n",
    "#### Basic mechanics\n",
    "기본적으로 <code>tf.data.Dataset</code>으로 만들기 위해서는 Source(원본 데이터)를 <code>tf.data.Dataset.from_tensors()</code>나 <code>tf.data.Dataset.from_tensor_slices()</code> 등으로서 선언하여 Dataset Object로서 변화 시켜야 한다.  \n",
    "\n",
    "이러한 바뀐 Dataset Object는 다음과 같은 특징을 가지게 된다.  \n",
    "**Dataset Object 특징**  \n",
    "- <code>Dataset.map()</code>과 같이 각각의 element에게 Function을 적용할 수 있다.\n",
    "- <code>Dataset.batch()</code>와 같이 다양한 element에게 Function을 적용할 수 있다.\n",
    "\n",
    "아래 Code는 간단한 [8, 3, 0, 8, 2, 1]을 Dataset Object로 변환시키고 결과를 확인한다.  \n",
    "**중요한 점은 TensorSliceDataset같이 Dataset Object는 Numpy의 <code>.take()</code>를 적용하여 가져올 수 있고 EagerTensor는 <code>.numpy()</code>로서 바로 결과를 확인 가능하다.**  \n",
    "위와 같은 작업을 Eager execution이라 하고 **바로 그래프를 생성하지 않고 계산값을 즉시 알려주는 Tensor로서 Model의 Debugging을 좀 더 쉽게 할 수 있는 기능이라고 한다.(이러한 기능은 나중에 다시 자세히 알아보기로 하자.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset shapes: (), types: tf.int32>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "8\n",
      "8\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "# Dataset Object 선언\n",
    "dataset = tf.data.Dataset.from_tensor_slices([8, 3, 0, 8, 2, 1])\n",
    "print(dataset)\n",
    "\n",
    "# 각각의 Eager Tensor 확인\n",
    "for elem in dataset.take(1):\n",
    "    print(type(elem))\n",
    "    print(elem.numpy())\n",
    "    \n",
    "# Dataset Object는 iter(), next()가 가능하다.\n",
    "it = next(iter(dataset))\n",
    "print(it.numpy())\n",
    "\n",
    "# Dataset Object에 Function 적용\n",
    "print(dataset.reduce(10, lambda state, value: state + value).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datastructure\n",
    "Dataset은 <a href=\"https://www.tensorflow.org/api_docs/python/tf/TypeSpec?version=stable\">tf.TypeSpec</a>이 표시할 수 있는 모든 구조를 포함할 수 있다.  \n",
    "<code>Dataset.element_spec</code>: Dataset안의 tf.TypeSpec Object를 확인할 수 있다.  \n",
    "<code>tf.TypeSpec.value_type</code>: tf.TypeSpec의 구조를 확인할 수 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Object 안의 Tensor 정보 확인\n",
      "dataset1:  TensorSpec(shape=(10,), dtype=tf.float32, name=None)\n",
      "dataset2:  (TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(100,), dtype=tf.int32, name=None))\n",
      "dataset3:  (TensorSpec(shape=(10,), dtype=tf.float32, name=None), (TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(100,), dtype=tf.int32, name=None)))\n",
      "dataset4:  SparseTensorSpec(TensorShape([3, 4]), tf.int32)\n",
      "\n",
      "Tensor 정보 확인\n",
      "dataset4:  <class 'tensorflow.python.framework.sparse_tensor.SparseTensor'>\n"
     ]
    }
   ],
   "source": [
    "# Dataset Object => Tensor로 이루워져 있다.\n",
    "dataset1 = tf.data.Dataset.from_tensor_slices(tf.random.uniform([4, 10]))\n",
    "\n",
    "# Dataset Object => Tensor로 이루워져 있다.\n",
    "dataset2 = tf.data.Dataset.from_tensor_slices(\n",
    "   (tf.random.uniform([4]),\n",
    "    tf.random.uniform([4, 100], maxval=100, dtype=tf.int32)))\n",
    "\n",
    "# Dataset Object => Tensor Array\n",
    "dataset3 = tf.data.Dataset.zip((dataset1, dataset2))\n",
    "\n",
    "# Dataset Object => Sparse Tensor로 이루워져 있다.\n",
    "dataset4 = tf.data.Dataset.from_tensors(tf.SparseTensor(indices=[[0, 0], [1, 2]], values=[1, 2], dense_shape=[3, 4]))\n",
    "\n",
    "# Dataset Object안의 ty.TypeSpec 구조의 자료 Tensor정보 확인\n",
    "print('Dataset Object 안의 Tensor 정보 확인')\n",
    "print('dataset1: ',dataset1.element_spec)\n",
    "print('dataset2: ',dataset2.element_spec)\n",
    "print('dataset3: ',dataset3.element_spec)\n",
    "print('dataset4: ',dataset4.element_spec)\n",
    "\n",
    "print()\n",
    "print('Tensor 정보 확인')\n",
    "print('dataset4: ',dataset4.element_spec.value_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CSV data\n",
    "Code 참조: <a href=\"https://www.tensorflow.org/tutorials/load_data/csv?hl=ko\">Tensorflow Core Load CSV data</a>\n",
    "<br>\n",
    "\n",
    "#### Setup\n",
    "<code>tf.data.Dataset</code>중 CSV data를 처리하는 방법에 대해서 알아보자.  \n",
    "먼저 <code>tf.keras.utils.get_file()</code>를 활용하여 URL에서 CSV Format Dataset을 가져오자.  \n",
    "\n",
    "**tf.keras.utils.get_file()**: Path to the downloaded file  \n",
    "```code\n",
    "tf.keras.utils.get_file(\n",
    "    fname,\n",
    "    origin,\n",
    "    untar=False,\n",
    "    md5_hash=None,\n",
    "    file_hash=None,\n",
    "    cache_subdir='datasets',\n",
    "    hash_algorithm='auto',\n",
    "    extract=False,\n",
    "    archive_format='auto',\n",
    "    cache_dir=None\n",
    ")\n",
    "```\n",
    "<br>\n",
    "앞으로의 Parameter는 전부 사용하지 않고 자주 사용하는 Parameter를 위주로 알아보자.  \n",
    "\n",
    "**parameter**\n",
    "- fname: File이 저장될 path 및 file 이름\n",
    "- origin: File의 URL\n",
    "- untar: Bool Type, File압축 해제 여부\n",
    "\n",
    "참조: <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/utils/get_file?version=stable\">tf.keras.utils.get_file() 설명</a>  \n",
    "\n",
    "<code>np.set_printoptions(precision=3, suppress=True)</code>: 부동 소수점 숫자, 배열 및 기타 NumPy 객체가 표시되는 방식을 결정\n",
    "- precision=3: Floating 데이터를 3자리 까지 나타낸다.\n",
    "- suppress=True: Floating point numbers를 위에서 정의한 precision에 맞게 항상 적용한다.\n",
    "\n",
    "참조: <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.set_printoptions.html\">np.set_printoptions() 설명</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_URL = \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\"\n",
    "TEST_DATA_URL = \"https://storage.googleapis.com/tf-datasets/titanic/eval.csv\"\n",
    "\n",
    "train_file_path = tf.keras.utils.get_file(\"train.csv\", TRAIN_DATA_URL)\n",
    "test_file_path = tf.keras.utils.get_file(\"eval.csv\", TEST_DATA_URL)\n",
    "\n",
    "# Make numpy values easier to read.\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data\n",
    "<code>!head {file_path}</code>: file path의 상위 10줄을 출력  \n",
    "<a href=\"https://wjddyd66.github.io/dataanalysis/Pandas/\">Pandas</a>를 활용하여서 Data를 Load하거나 처리하여 Numpy형태로 Tensorflow에 전달할 수 있다.  \n",
    "Data가 커질 경우에는 <code>tf.data.experimental.make_csv_dataset function</code>를 사용하게 된다.  \n",
    "\n",
    "**tf.data.experimental.make_csv_dataset()**: Read CSV files into a dataset  \n",
    "```code\n",
    "tf.data.experimental.make_csv_dataset(\n",
    "    file_pattern,\n",
    "    batch_size,\n",
    "    column_names=None,\n",
    "    column_defaults=None,\n",
    "    label_name=None,\n",
    "    select_columns=None,\n",
    "    field_delim=',',\n",
    "    use_quote_delim=True,\n",
    "    na_value='',\n",
    "    header=True,\n",
    "    num_epochs=None,\n",
    "    shuffle=True,\n",
    "    shuffle_buffer_size=10000,\n",
    "    shuffle_seed=None,\n",
    "    prefetch_buffer_size=dataset_ops.AUTOTUNE,\n",
    "    num_parallel_reads=1,\n",
    "    sloppy=False,\n",
    "    num_rows_for_inference=100,\n",
    "    compression_type=None,\n",
    "    ignore_errors=False\n",
    ")\n",
    "```\n",
    "<br>\n",
    "\n",
    "**parameter**\n",
    "- file_pattern: CSV records의 file path\n",
    "- batch_size: Dataset의 Batch\n",
    "- column_names: CSV records의 columns을 사용자가 원하는 이름으로 지정가능(Factor의 개수 동일해야 함)\n",
    "- label_name: Label, 즉 Dataset을 Label과 Factor로 분리하여 하나의 Dataset으로서 변형시킬 수 있다.\n",
    "- num_epochs: 반복할 횟수. 만약 None으로 지정시 무한 반복\n",
    "- shuffle: Dataset을 섞을지에 대한 option\n",
    "- na_value: NA/ NaN Value처리를 어떻게 할 것인지 지정\n",
    "- select_columns: CSV records에서 원하는 columns 선택 가능\n",
    "\n",
    "\n",
    "참조: <a href=\"https://www.tensorflow.org/api_docs/python/tf/data/experimental/make_csv_dataset?hl=ko&version=stable\">tf.data.experimental.make_csv_dataset() 설명</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "survived,sex,age,n_siblings_spouses,parch,fare,class,deck,embark_town,alone\n",
      "0,male,22.0,1,0,7.25,Third,unknown,Southampton,n\n",
      "1,female,38.0,1,0,71.2833,First,C,Cherbourg,n\n",
      "1,female,26.0,0,0,7.925,Third,unknown,Southampton,y\n",
      "1,female,35.0,1,0,53.1,First,C,Southampton,n\n",
      "0,male,28.0,0,0,8.4583,Third,unknown,Queenstown,y\n",
      "0,male,2.0,3,1,21.075,Third,unknown,Southampton,n\n",
      "1,female,27.0,0,2,11.1333,Third,unknown,Southampton,n\n",
      "1,female,14.0,1,0,30.0708,Second,unknown,Cherbourg,n\n",
      "1,female,4.0,1,1,16.7,Third,G,Southampton,n\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/experimental/ops/readers.py:498: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n"
     ]
    }
   ],
   "source": [
    "!head {train_file_path}\n",
    "\n",
    "LABEL_COLUMN = 'survived'\n",
    "LABELS = [0, 1]\n",
    "def get_dataset(file_path, **kwargs):\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        file_path,\n",
    "        batch_size=5, # Artificially small to make examples easier to show.\n",
    "        label_name=LABEL_COLUMN,\n",
    "        na_value=\"?\",\n",
    "        num_epochs=1,\n",
    "        ignore_errors=True, \n",
    "        **kwargs)\n",
    "    return dataset\n",
    "\n",
    "raw_train_data = get_dataset(train_file_path)\n",
    "raw_test_data = get_dataset(test_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset 확인\n",
    "위에서 batch_size=5, num_epochs=1로서 정의한 Dataset을 확인하기 위한 Function이다.  \n",
    "위의 Option으로 인하여 크기가 5인 Dataset이 만들어질 것이고, Label = survived값, columns는 나머지 Factor가 될 것이다.  \n",
    "\n",
    "**참고사항(<code>numpy.take()</code>)**  \n",
    "numpy에서 <code>take()</code>는 원하는 Index를 가져오는 Function이다.  \n",
    "예를 들면 다음과 같다.  \n",
    "```python\n",
    "a = np.array([4, 3, 5, 7, 6, 8])\n",
    "a.take(3) # 7\n",
    "```\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label               : [1 0 0 0 0]\n",
      "sex                 : [b'male' b'male' b'male' b'female' b'male']\n",
      "age                 : [28. 24. 16. 57. 34.]\n",
      "n_siblings_spouses  : [0 0 0 0 0]\n",
      "parch               : [0 0 0 0 0]\n",
      "fare                : [26.55   7.496  8.05  10.5    8.05 ]\n",
      "class               : [b'First' b'Third' b'Third' b'Second' b'Third']\n",
      "deck                : [b'unknown' b'unknown' b'unknown' b'E' b'unknown']\n",
      "embark_town         : [b'Southampton' b'Southampton' b'Southampton' b'Southampton'\n",
      " b'Southampton']\n",
      "alone               : [b'y' b'y' b'y' b'y' b'y']\n"
     ]
    }
   ],
   "source": [
    "def show_batch(dataset):\n",
    "    for batch, label in dataset.take(1):\n",
    "        print(\"{:20s}: {}\".format('Label',label.numpy()))\n",
    "        for key, value in batch.items():\n",
    "            print(\"{:20s}: {}\".format(key,value.numpy()))\n",
    "            \n",
    "show_batch(raw_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Columns 선택\n",
    "CSV File Format에서 <code>column_names</code>은 모든 Column의 개수와 같으나 사용자가 원하는 대로 Columns의 name을 변화시킬 수 있다.  \n",
    "CSV File Format에서 <code>select_columns</code>은 원하는 Column를 선택할 수 있다.  \n",
    "\n",
    "**단, Label로 정의한 Columns는 Dataset의 Factor로서 포함시킬 수 없다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV File Columns 변환\n",
      "Label               : [0 0 0 1 1]\n",
      "1                   : [b'male' b'male' b'male' b'female' b'female']\n",
      "2                   : [39. 31. 22. 14. 27.]\n",
      "3                   : [1 0 0 1 1]\n",
      "4                   : [5 0 0 0 0]\n",
      "5                   : [31.275 50.496  9.35  11.242 13.858]\n",
      "6                   : [b'Third' b'First' b'Third' b'Third' b'Second']\n",
      "7                   : [b'unknown' b'A' b'unknown' b'unknown' b'unknown']\n",
      "8                   : [b'Southampton' b'Southampton' b'Southampton' b'Cherbourg' b'Cherbourg']\n",
      "9                   : [b'n' b'y' b'y' b'n' b'n']\n",
      "------------------------------------------------------------\n",
      "CSV File 특정 Columns 선택\n",
      "Label               : [1 0 0 1 0]\n",
      "age                 : [28. 21. 27. 28. 28.]\n",
      "n_siblings_spouses  : [0 0 1 0 1]\n",
      "class               : [b'Third' b'Third' b'Second' b'First' b'First']\n",
      "deck                : [b'unknown' b'unknown' b'unknown' b'C' b'unknown']\n",
      "alone               : [b'y' b'y' b'n' b'y' b'n']\n"
     ]
    }
   ],
   "source": [
    "# CSV File Columns 변환\n",
    "print('CSV File Columns 변환')\n",
    "CSV_COLUMNS = ['survived', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "temp_dataset = get_dataset(train_file_path, column_names=CSV_COLUMNS)\n",
    "show_batch(temp_dataset)\n",
    "print('------------------------------------------------------------')\n",
    "\n",
    "# CSV File 특정 Columns 선택\n",
    "print('CSV File 특정 Columns 선택')\n",
    "SELECT_COLUMNS = ['survived', 'age', 'n_siblings_spouses', 'class', 'deck', 'alone']\n",
    "temp_dataset = get_dataset(train_file_path, select_columns=SELECT_COLUMNS)\n",
    "show_batch(temp_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continuous data\n",
    "<code>column_defaults = DEFAULTS</code>를 통하여 **Tensor에 없는 값을 치환하거나 DataType을 변화시킬 수 있다.**  \n",
    "아래 <code>DEFAULTS = [0, \"0\", 0.0, 0.0, 0.0]</code>를 살펴보게 되면  \n",
    "- 0: Int\n",
    "- \"0\": String\n",
    "- 0.0: float\n",
    "\n",
    "으로서 Columns의 Dtype을 변화시킬 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 Data\n",
      "Label               : [1 1 1 1 0]\n",
      "age                 : [33. 28. 34. 28. 25.]\n",
      "n_siblings_spouses  : [0 0 0 1 0]\n",
      "parch               : [2 0 0 0 0]\n",
      "fare                : [26.    7.75 26.55 15.5   7.05]\n",
      "------------------------------------------------------------\n",
      "Defaults option 적용\n",
      "Label               : [0 0 0 0 1]\n",
      "age                 : [b'22.0' b'47.0' b'28.0' b'28.0' b'35.0']\n",
      "n_siblings_spouses  : [0. 0. 0. 0. 0.]\n",
      "parch               : [0. 0. 0. 0. 0.]\n",
      "fare                : [ 10.517  25.587   7.896   7.75  512.329]\n"
     ]
    }
   ],
   "source": [
    "SELECT_COLUMNS = ['survived', 'age', 'n_siblings_spouses', 'parch', 'fare']\n",
    "DEFAULTS = [0, \"0\", 0.0, 0.0, 0.0]\n",
    "# 원본 Data\n",
    "print('원본 Data')\n",
    "temp_dataset = get_dataset(train_file_path, select_columns=SELECT_COLUMNS)\n",
    "show_batch(temp_dataset)\n",
    "print('------------------------------------------------------------')\n",
    "\n",
    "# Defaults option 적용\n",
    "print('Defaults option 적용')\n",
    "temp_dataset = get_dataset(train_file_path, select_columns=SELECT_COLUMNS,column_defaults = DEFAULTS)\n",
    "show_batch(temp_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pack together all the columns\n",
    "Dataset을 실제 Model에 넣기위한 Format으로 변형시키는 과정이다.  \n",
    "위의 과정은 다음과 같이 이루워 진다.  \n",
    "1. <code>get_dataset()</code>: Path에서 Dataset을 원한는 Option을 사용하여 가져온다.\n",
    "2. <code>next(iter(temp_dataset))</code>: 데이터를 Input, Label로서 나누는 작업을 한다. (만약 next와  iter에 대해서 모르시면 옆의 링크를 참조하시길 바랍니다. <a href=\"https://dojang.io/mod/page/view.php?id=2408\">코딩도장</a>)\n",
    "3. <code>pack()</code>: Feature들을 하나로 합치는 과정이다.\n",
    "4. <code>temp_dataset.map(pack)</code>: 준비한 Dataset에 map() Function을 적용한다. 즉, Dataset을 (Feature(합친것), Label)형태로서 나타낸다. (만약 map에 대해서 모르시면 옆의 링크를 참조하시길 바랍니다. <a href=\"https://bluese05.tistory.com/58\">물과같이 블로그</a>)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data\n",
      "Label               : [1 1 0 1 0]\n",
      "age                 : [34. 27. 21. 17. 22.]\n",
      "n_siblings_spouses  : [0. 0. 0. 1. 0.]\n",
      "parch               : [0. 0. 0. 0. 0.]\n",
      "fare                : [26.55 30.5  73.5  57.    9.35]\n",
      "----------------------------------------------------------------------\n",
      "Preprocess Data\n",
      "[[34.    0.    0.   26.55]\n",
      " [27.    0.    0.   30.5 ]\n",
      " [21.    0.    0.   73.5 ]\n",
      " [17.    1.    0.   57.  ]\n",
      " [22.    0.    0.    9.35]]\n",
      "\n",
      "[1 1 0 1 0]\n",
      "\n",
      "[[ 1.     4.     1.    39.688]\n",
      " [28.     2.     0.    23.25 ]\n",
      " [28.     0.     0.     8.113]\n",
      " [11.     4.     2.    31.275]\n",
      " [33.     0.     0.     5.   ]]\n",
      "\n",
      "[0 1 1 0 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DEFAULTS = [0, 0.0, 0.0, 0.0, 0.0]\n",
    "temp_dataset = get_dataset(train_file_path, select_columns=SELECT_COLUMNS,column_defaults = DEFAULTS)\n",
    "print('Original Data')\n",
    "show_batch(temp_dataset)\n",
    "\n",
    "example_batch, labels_batch = next(iter(temp_dataset)) \n",
    "\n",
    "def pack(features, label):\n",
    "    return tf.stack(list(features.values()), axis=-1), label\n",
    "\n",
    "packed_dataset = temp_dataset.map(pack)\n",
    "\n",
    "print('-'*70)\n",
    "print('Preprocess Data')\n",
    "for features, labels in packed_dataset.take(2):\n",
    "    print(features.numpy())\n",
    "    print()\n",
    "    print(labels.numpy())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General Preprocessor\n",
    "Numeric Feature를 하나로 합쳐서 Dataset을 구성하는 방법이다.  \n",
    "위의 과정은 다음과 같이 이루워 진다.  \n",
    "1. <code>NUMERIC_FEATURES</code>: CSV File에서 Numeric Features를 선언한다.\n",
    "2. <code>PackNumericFeatures()</code>: Numeric Features를 하나의 numeric이라는 Feature로 합쳐서 나타낸다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data\n",
      "Label               : [0 1 0 0 0]\n",
      "sex                 : [b'male' b'male' b'male' b'male' b'male']\n",
      "age                 : [29. 36. 46. 28. 22.]\n",
      "n_siblings_spouses  : [1 1 0 0 0]\n",
      "parch               : [0 2 0 0 0]\n",
      "fare                : [  7.046 120.     79.2    56.496   7.229]\n",
      "class               : [b'Third' b'First' b'First' b'Third' b'Third']\n",
      "deck                : [b'unknown' b'B' b'B' b'unknown' b'unknown']\n",
      "embark_town         : [b'Southampton' b'Southampton' b'Cherbourg' b'Southampton' b'Cherbourg']\n",
      "alone               : [b'n' b'n' b'y' b'y' b'y']\n",
      "----------------------------------------------------------------------\n",
      "Preprocess Data\n",
      "Label               : [1 0 0 0 0]\n",
      "sex                 : [b'male' b'male' b'male' b'female' b'male']\n",
      "class               : [b'First' b'Third' b'Third' b'Second' b'Third']\n",
      "deck                : [b'unknown' b'unknown' b'unknown' b'E' b'unknown']\n",
      "embark_town         : [b'Southampton' b'Southampton' b'Southampton' b'Southampton'\n",
      " b'Southampton']\n",
      "alone               : [b'y' b'y' b'y' b'y' b'y']\n",
      "numeric             : [[28.     0.     0.    26.55 ]\n",
      " [24.     0.     0.     7.496]\n",
      " [16.     0.     0.     8.05 ]\n",
      " [57.     0.     0.    10.5  ]\n",
      " [34.     0.     0.     8.05 ]]\n"
     ]
    }
   ],
   "source": [
    "CSV_COLUMNS = ['survived', 'sex', 'age', 'n_siblings_spouses', 'parch', 'fare','class','deck','embark_town','alone']\n",
    "temp_dataset = get_dataset(train_file_path, column_names=CSV_COLUMNS)\n",
    "print('Original Data')\n",
    "show_batch(temp_dataset)\n",
    "\n",
    "example_batch, labels_batch = next(iter(temp_dataset)) \n",
    "\n",
    "class PackNumericFeatures(object):\n",
    "    def __init__(self, names):\n",
    "        self.names = names\n",
    "        \n",
    "    def __call__(self, features, labels):\n",
    "        numeric_features = [features.pop(name) for name in self.names]\n",
    "        numeric_features = [tf.cast(feat, tf.float32) for feat in numeric_features]\n",
    "        numeric_features = tf.stack(numeric_features, axis=-1)\n",
    "        features['numeric'] = numeric_features\n",
    "\n",
    "        return features, labels\n",
    "\n",
    "print('-'*70)\n",
    "print('Preprocess Data')\n",
    "\n",
    "NUMERIC_FEATURES = ['age','n_siblings_spouses','parch', 'fare']\n",
    "\n",
    "packed_train_data = raw_train_data.map(\n",
    "    PackNumericFeatures(NUMERIC_FEATURES))\n",
    "\n",
    "packed_test_data = raw_test_data.map(\n",
    "    PackNumericFeatures(NUMERIC_FEATURES))\n",
    "\n",
    "show_batch(packed_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Normalization\n",
    "각각의 Feature의 범위와 크기가 다르기 때문에 Normalization이 필요하다.  \n",
    "Normalization은 간단하게 <span>$x \\leftarrow \\frac{x-mean}{std}$</span>로서 이루워진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>n_siblings_spouses</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>627.000000</td>\n",
       "      <td>627.000000</td>\n",
       "      <td>627.000000</td>\n",
       "      <td>627.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>29.631308</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.379585</td>\n",
       "      <td>34.385399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>12.511818</td>\n",
       "      <td>1.151090</td>\n",
       "      <td>0.792999</td>\n",
       "      <td>54.597730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>23.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.895800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.045800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>35.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.387500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              age  n_siblings_spouses       parch        fare\n",
       "count  627.000000          627.000000  627.000000  627.000000\n",
       "mean    29.631308            0.545455    0.379585   34.385399\n",
       "std     12.511818            1.151090    0.792999   54.597730\n",
       "min      0.750000            0.000000    0.000000    0.000000\n",
       "25%     23.000000            0.000000    0.000000    7.895800\n",
       "50%     28.000000            0.000000    0.000000   15.045800\n",
       "75%     35.000000            1.000000    0.000000   31.387500\n",
       "max     80.000000            8.000000    5.000000  512.329200"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numeric Feature의 4분위수 및 mean, std ... 출력\n",
    "desc = pd.read_csv(train_file_path)[NUMERIC_FEATURES].describe()\n",
    "\n",
    "# mean, std 선언\n",
    "MEAN = np.array(desc.T['mean'])\n",
    "STD = np.array(desc.T['std'])\n",
    "\n",
    "# Normalization Function 선언\n",
    "def normalize_numeric_data(data, mean, std):\n",
    "    # Center the data\n",
    "    return (data-mean)/std\n",
    "\n",
    "desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### functools.partial\n",
    "먼저 <code>functools.partial()</code>을 통하여 위에서 정의한 Function을 다시 재정의 한다.  \n",
    "partial이란 **lambda와 비슷하지만 lambda는 평가될때가 되서야 코드가 생성되고 partial에 의한 함수는 생성될 때 평가된다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "\n",
      "--------------------\n",
      "partial\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "funcs = []\n",
    "for i in range(5):\n",
    "    funcs.append(lambda : print(i))\n",
    "\n",
    "print('lambda')\n",
    "for f in funcs:\n",
    "    f()\n",
    "    \n",
    "print()\n",
    "print('-'*20)\n",
    "\n",
    "funcs = []\n",
    "for i in range(5):\n",
    "    funcs.append(functools.partial(print, i))\n",
    "\n",
    "print('partial')\n",
    "for f in funcs:\n",
    "    f()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization 2\n",
    "실질적으로 Data를 Normalization하는 Layer를 선언한다.  \n",
    "1. <code>functools.partial()</code>: 각각의 Data를 <span>$data \\leftarrow \\frac{data-mean}{std}$</span>적용\n",
    "2. <code>tf.feature_column.numeric_column()</code>: Data의 특정 Feature를 뽑아 낸 뒤 위에서 정의한 Normalization을 적용 시킨다.\n",
    "3. <code>tf.keras.layers.DenseFeatures(numeric_columns)</code>: 하나의 Normalization을 Layer로서 선언한다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumericColumn(key='numeric', shape=(4,), default_value=None, dtype=tf.float32, normalizer_fn=functools.partial(<function normalize_numeric_data at 0x7f0b90820bf8>, mean=array([29.631,  0.545,  0.38 , 34.385]), std=array([12.512,  1.151,  0.793, 54.598])))\n",
      "tf.Tensor(\n",
      "[[28.     0.     0.    26.55 ]\n",
      " [24.     0.     0.     7.496]\n",
      " [16.     0.     0.     8.05 ]\n",
      " [57.     0.     0.    10.5  ]\n",
      " [34.     0.     0.     8.05 ]], shape=(5, 4), dtype=float32)\n",
      "tf.Tensor([1 0 0 0 0], shape=(5,), dtype=int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.13 , -0.474, -0.479, -0.144],\n",
       "       [-0.45 , -0.474, -0.479, -0.493],\n",
       "       [-1.089, -0.474, -0.479, -0.482],\n",
       "       [ 2.187, -0.474, -0.479, -0.437],\n",
       "       [ 0.349, -0.474, -0.479, -0.482]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_batch, labels_batch = next(iter(packed_train_data))\n",
    "\n",
    "normalizer = functools.partial(normalize_numeric_data, mean=MEAN, std=STD)\n",
    "\n",
    "numeric_column = tf.feature_column.numeric_column('numeric', normalizer_fn=normalizer, shape=[len(NUMERIC_FEATURES)])\n",
    "numeric_columns = [numeric_column]\n",
    "print(numeric_column)\n",
    "\n",
    "print(example_batch['numeric'])\n",
    "print(labels_batch)\n",
    "\n",
    "numeric_layer = tf.keras.layers.DenseFeatures(numeric_columns)\n",
    "numeric_layer(example_batch).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical data\n",
    "Numeric하지 않고 Categorical data를 전처리 하는 과정이다.  \n",
    "One-Hot-Encoding 처럼 보이나 Categorical의 개수만큼 증가시키는 것 이다.  \n",
    "이러한 방법을 multi-hot representation이라고 한다.  \n",
    "아래 예시는 <a href=\"https://www.tensorflow.org/api_docs/python/tf/feature_column/indicator_column?version=stable\">Tensorflow tf.feature_column.indicator_cloumn 설명서</a>의 예시이다.  \n",
    "\n",
    "```python\n",
    "name = indicator_column(categorical_column_with_vocabulary_list(\n",
    "    'name', ['bob', 'george', 'wanda'])\n",
    "columns = [name, ...]\n",
    "features = tf.io.parse_example(..., features=make_parse_example_spec(columns))\n",
    "dense_tensor = input_layer(features, columns)\n",
    "\n",
    "dense_tensor == [[1, 0, 0]]  # If \"name\" bytes_list is [\"bob\"]\n",
    "dense_tensor == [[1, 0, 1]]  # If \"name\" bytes_list is [\"bob\", \"wanda\"]\n",
    "dense_tensor == [[2, 0, 0]]  # If \"name\" bytes_list is [\"bob\", \"bob\"]\n",
    "```\n",
    "<br>\n",
    "\n",
    "아래 Code는 위의 예시처럼 <code>tf.feature_column.indicator_cloumn()</code>을 활용하여 Categorical Feature들을 Multi Hot Representation형태로 나타내는 것 이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='sex', vocabulary_list=('male', 'female'), dtype=tf.string, default_value=-1, num_oov_buckets=0)), IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='class', vocabulary_list=('First', 'Second', 'Third'), dtype=tf.string, default_value=-1, num_oov_buckets=0)), IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='deck', vocabulary_list=('A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'), dtype=tf.string, default_value=-1, num_oov_buckets=0)), IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='embark_town', vocabulary_list=('Cherbourg', 'Southhampton', 'Queenstown'), dtype=tf.string, default_value=-1, num_oov_buckets=0)), IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='alone', vocabulary_list=('y', 'n'), dtype=tf.string, default_value=-1, num_oov_buckets=0))]\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/feature_column/feature_column_v2.py:2655: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/feature_column/feature_column_v2.py:4215: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/feature_column/feature_column_v2.py:4270: VocabularyListCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "[1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "CATEGORIES = {\n",
    "    'sex': ['male', 'female'],\n",
    "    'class' : ['First', 'Second', 'Third'],\n",
    "    'deck' : ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'],\n",
    "    'embark_town' : ['Cherbourg', 'Southhampton', 'Queenstown'],\n",
    "    'alone' : ['y', 'n']\n",
    "}\n",
    "\n",
    "categorical_columns = []\n",
    "for feature, vocab in CATEGORIES.items():\n",
    "    cat_col = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "        key=feature, vocabulary_list=vocab)\n",
    "    categorical_columns.append(tf.feature_column.indicator_column(cat_col))\n",
    "    \n",
    "print(categorical_columns)\n",
    "\n",
    "categorical_layer = tf.keras.layers.DenseFeatures(categorical_columns)\n",
    "print(categorical_layer(example_batch).numpy()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model\n",
    "위에서 선언한 Preprocess 과정을 거쳐서 실질적인 Model을 Training하는 과정이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 0.4971 - accuracy: 0.7390\n",
      "Epoch 2/20\n",
      "126/126 [==============================] - 0s 1ms/step - loss: 0.4216 - accuracy: 0.8365\n",
      "Epoch 3/20\n",
      "126/126 [==============================] - 0s 1ms/step - loss: 0.4030 - accuracy: 0.8501\n",
      "Epoch 4/20\n",
      "126/126 [==============================] - 0s 1ms/step - loss: 0.3903 - accuracy: 0.8467\n",
      "Epoch 5/20\n",
      "126/126 [==============================] - 0s 2ms/step - loss: 0.3813 - accuracy: 0.8477\n",
      "Epoch 6/20\n",
      "126/126 [==============================] - 0s 1ms/step - loss: 0.3733 - accuracy: 0.8478\n",
      "Epoch 7/20\n",
      "126/126 [==============================] - 0s 2ms/step - loss: 0.3662 - accuracy: 0.8466\n",
      "Epoch 8/20\n",
      "126/126 [==============================] - 0s 2ms/step - loss: 0.3598 - accuracy: 0.8487\n",
      "Epoch 9/20\n",
      "126/126 [==============================] - 0s 2ms/step - loss: 0.3544 - accuracy: 0.8474\n",
      "Epoch 10/20\n",
      "126/126 [==============================] - 0s 2ms/step - loss: 0.3482 - accuracy: 0.8517\n",
      "Epoch 11/20\n",
      "126/126 [==============================] - 0s 2ms/step - loss: 0.3436 - accuracy: 0.8513\n",
      "Epoch 12/20\n",
      "126/126 [==============================] - 0s 1ms/step - loss: 0.3389 - accuracy: 0.8519\n",
      "Epoch 13/20\n",
      "126/126 [==============================] - 0s 2ms/step - loss: 0.3348 - accuracy: 0.8507\n",
      "Epoch 14/20\n",
      "126/126 [==============================] - 0s 2ms/step - loss: 0.3303 - accuracy: 0.8544\n",
      "Epoch 15/20\n",
      "126/126 [==============================] - 0s 1ms/step - loss: 0.3264 - accuracy: 0.8547\n",
      "Epoch 16/20\n",
      "126/126 [==============================] - 0s 1ms/step - loss: 0.3239 - accuracy: 0.8545\n",
      "Epoch 17/20\n",
      "126/126 [==============================] - 0s 2ms/step - loss: 0.3192 - accuracy: 0.8559\n",
      "Epoch 18/20\n",
      "126/126 [==============================] - 0s 1ms/step - loss: 0.3157 - accuracy: 0.8552\n",
      "Epoch 19/20\n",
      "126/126 [==============================] - 0s 1ms/step - loss: 0.3117 - accuracy: 0.8572\n",
      "Epoch 20/20\n",
      "126/126 [==============================] - 0s 2ms/step - loss: 0.3080 - accuracy: 0.8614\n",
      "     53/Unknown - 0s 5ms/step - loss: 0.4609 - accuracy: 0.8485\n",
      "\n",
      "Test Loss 0.4608553098338955, Test Accuracy 0.8484848737716675\n",
      "Predicted survival: 5.91%  | Actual outcome:  DIED\n",
      "Predicted survival: 63.61%  | Actual outcome:  SURVIVED\n",
      "Predicted survival: 1.61%  | Actual outcome:  DIED\n",
      "Predicted survival: 69.31%  | Actual outcome:  SURVIVED\n",
      "Predicted survival: 7.14%  | Actual outcome:  DIED\n"
     ]
    }
   ],
   "source": [
    "preprocessing_layer = tf.keras.layers.DenseFeatures(categorical_columns+numeric_columns)\n",
    "model = tf.keras.Sequential([\n",
    "  preprocessing_layer,\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid'),\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "train_data = packed_train_data.shuffle(500)\n",
    "test_data = packed_test_data\n",
    "model.fit(train_data, epochs=20)\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(test_data)\n",
    "\n",
    "print('\\n\\nTest Loss {}, Test Accuracy {}'.format(test_loss, test_accuracy))\n",
    "\n",
    "predictions = model.predict(test_data)\n",
    "\n",
    "# Show some results\n",
    "for prediction, survived in zip(predictions[:10], list(test_data)[0][1][:10]):\n",
    "    print(\"Predicted survival: {:.2%}\".format(prediction[0]),\n",
    "          \" | Actual outcome: \",\n",
    "          (\"SURVIVED\" if bool(survived) else \"DIED\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Numpy data\n",
    "Code 참조: <a href=\"https://www.tensorflow.org/tutorials/load_data/numpy?hl=ko\">Tensorflow Core Load Numpy data</a><br>\n",
    "Numpy 참조: <a href=\"https://wjddyd66.github.io/dataanalysis/Numpy/\">Numpy</a><br>\n",
    "\n",
    "#### Setup\n",
    "<code>tf.data.Dataset</code>중 Numpy data를 처리하는 방법에 대해서 알아보자.  \n",
    "먼저 <code>tf.keras.utils.get_file()</code>를 활용하여 URL에서 Numpy Format Dataset을 가져오자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DATA_URL = 'https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz'\n",
    "\n",
    "path = tf.keras.utils.get_file('mnist.npz', DATA_URL)\n",
    "\n",
    "with np.load(path) as data:\n",
    "    train_examples = data['x_train']\n",
    "    train_labels = data['y_train']\n",
    "    test_examples = data['x_test']\n",
    "    test_labels = data['y_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mnist Numpy Dataset 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(len(train_labels))\n",
    "plt.figure(figsize=(6,3))\n",
    "\n",
    "ax_x_train = plt.subplot(121)\n",
    "ax_x_test = plt.subplot(122)\n",
    "\n",
    "ax_x_train.set_title('Train Label: '+ str(train_labels[0]))\n",
    "ax_x_train.imshow(train_examples[0],cmap='gray')\n",
    "ax_x_test.set_title('Train Label: '+ str(test_labels[0]))\n",
    "ax_x_test.imshow(test_examples[0],cmap='gray')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Numpy arrays with tf.data.Dataset\n",
    "- <code>tf.data.Dataset.from_tensor_slices()</code>: dataset과 labels를 하나로 묶음과 동시에 Tensor로서 변환시킨다. Argument인 dataset과 label은 Tuple형식으로 전달한다.\n",
    "- <code>train_dataset.shuffle()</code>: Dataset을 Shuffle한다. Argument로 주어지는 SUFFLE_BUFFER_SIZE는 Data의 개수보다 크거나 같은것이 좋다.\n",
    "- <code>test_dataset.batch()</code>: Input으로 넣을 Tensor를 Batch_size에 맞게 조절한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_examples, train_labels))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_examples, test_labels))\n",
    "\n",
    "# Hyperparameter 선언\n",
    "BATCH_SIZE = 64\n",
    "SHUFFLE_BUFFER_SIZE = 100\n",
    "\n",
    "# Train Dataset Shuffle\n",
    "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "# Batch Size로서 변경\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model\n",
    "위에서 준비한 Preprocessing Numpy Data로서 Model을 구성하고 결과를 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 8.3879 - sparse_categorical_accuracy: 0.4766\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 6.1293 - sparse_categorical_accuracy: 0.6166\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 4.8214 - sparse_categorical_accuracy: 0.6980\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 4.5923 - sparse_categorical_accuracy: 0.7128\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 4.5060 - sparse_categorical_accuracy: 0.7184\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 4.0114 - sparse_categorical_accuracy: 0.7490\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 3.1366 - sparse_categorical_accuracy: 0.8029\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 3.0169 - sparse_categorical_accuracy: 0.8107\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.9312 - sparse_categorical_accuracy: 0.8164\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.9276 - sparse_categorical_accuracy: 0.8165\n",
      "157/157 [==============================] - 0s 1ms/step - loss: 2.8658 - sparse_categorical_accuracy: 0.8209\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.8657501829657583, 0.8209]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "model.fit(train_dataset, epochs=10)\n",
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pandas data\n",
    "Code 참조: <a href=\"https://www.tensorflow.org/tutorials/load_data/pandas_dataframe?hl=ko\">Tensorflow Core Load a pandas.DataFrame</a><br>\n",
    "Pandas 참조: <a href=\"https://wjddyd66.github.io/dataanalysis/Pandas/\">Pandas</a><br>\n",
    "\n",
    "#### Setup\n",
    "<code>tf.data.Dataset</code>중 CSV data를 Pandas를 사용하여 처리하는 방법에 대해서 알아보자.  \n",
    "<code>tf.keras.utils.get_file()</code>를 활용하여 URL에서 CSV Format Dataset을 가져오자.  \n",
    "<code>pd.read_csv()</code>를 CSV Format Dataset을 Pandas로 변환하여 결과를 확인하자.  \n",
    "<code>df.head()</code>: 상위 5개 Data확인  \n",
    "<code>df.dtypes</code>: Factor의 Type확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>fixed</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>160</td>\n",
       "      <td>286</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>108</td>\n",
       "      <td>1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>normal</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>120</td>\n",
       "      <td>229</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>129</td>\n",
       "      <td>1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>reversible</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   1       145   233    1        2      150      0      2.3      3   \n",
       "1   67    1   4       160   286    0        2      108      1      1.5      2   \n",
       "2   67    1   4       120   229    0        2      129      1      2.6      2   \n",
       "3   37    1   3       130   250    0        0      187      0      3.5      3   \n",
       "4   41    0   2       130   204    0        2      172      0      1.4      1   \n",
       "\n",
       "   ca        thal  target  \n",
       "0   0       fixed       0  \n",
       "1   3      normal       1  \n",
       "2   2  reversible       0  \n",
       "3   0      normal       0  \n",
       "4   0      normal       0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CSV File 가져오기\n",
    "csv_file = tf.keras.utils.get_file('heart.csv', 'https://storage.googleapis.com/applied-dl/heart.csv')\n",
    "# CSV File을 Pandas로 변환\n",
    "df = pd.read_csv(csv_file)\n",
    "# 상위 5개 Data 확인\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age           int64\n",
       "sex           int64\n",
       "cp            int64\n",
       "trestbps      int64\n",
       "chol          int64\n",
       "fbs           int64\n",
       "restecg       int64\n",
       "thalach       int64\n",
       "exang         int64\n",
       "oldpeak     float64\n",
       "slope         int64\n",
       "ca            int64\n",
       "thal         object\n",
       "target        int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas Data Preprocessing\n",
    "위에서 각각의 Factor들의 Type을 확인해보면 **thal**이라는 Factor는 Numeric이 아니라 Object이다.  \n",
    "따라서 Object -> Numeric으로서 Type을 변환시켜야 한다.  \n",
    "<code>df.thal.cat.codes</code>를 통하여 Return Series of codes as well as the index즉, Index로서 변환시킨다.  \n",
    "아래 결과를 살펴보게 되면  \n",
    "Original Thal은 5개의 값으로서 이루워져 있다. (unique 5)  \n",
    "이러한 값을 Index로서 치환하게 되어 thal Factor는 0 ~ 4사이의 값으로 치환되게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Factor thal describe()\n",
      "count        303\n",
      "unique         5\n",
      "top       normal\n",
      "freq         168\n",
      "Name: thal, dtype: object\n",
      "\n",
      ".cat.codes thal describe()\n",
      "count    303.000000\n",
      "mean       3.303630\n",
      "std        0.625258\n",
      "min        0.000000\n",
      "25%        3.000000\n",
      "50%        3.000000\n",
      "75%        4.000000\n",
      "max        4.000000\n",
      "Name: thal, dtype: float64\n",
      "\n",
      "------------------------------\n",
      "Min thal:  0\n",
      "Max thal:  4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>160</td>\n",
       "      <td>286</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>108</td>\n",
       "      <td>1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>120</td>\n",
       "      <td>229</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>129</td>\n",
       "      <td>1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   1       145   233    1        2      150      0      2.3      3   \n",
       "1   67    1   4       160   286    0        2      108      1      1.5      2   \n",
       "2   67    1   4       120   229    0        2      129      1      2.6      2   \n",
       "3   37    1   3       130   250    0        0      187      0      3.5      3   \n",
       "4   41    0   2       130   204    0        2      172      0      1.4      1   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   0     2       0  \n",
       "1   3     3       1  \n",
       "2   2     4       0  \n",
       "3   0     3       0  \n",
       "4   0     3       0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Original thal Factor Check\n",
    "print('Original Factor thal describe()')\n",
    "print(df.thal.describe())\n",
    "\n",
    "# Convert thal Factor Object to Numeric\n",
    "df['thal'] = pd.Categorical(df['thal'])\n",
    "df['thal'] = df.thal.cat.codes\n",
    "print()\n",
    "print('.cat.codes thal describe()')\n",
    "print(df.thal.describe())\n",
    "print()\n",
    "print('-'*30)\n",
    "\n",
    "# Check min, max of thal Factor\n",
    "print('Min thal: ', min(df.thal))\n",
    "print('Max thal: ', max(df.thal))\n",
    "\n",
    "# 바뀐 DataFrame 확인\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data using tf.data.Dataset\n",
    "<code>tf.data.Dataset.from_tensor_slices()</code>을 확용하여 InputData, Label로서 Dataset을 구성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: [ 63.    1.    1.  145.  233.    1.    2.  150.    0.    2.3   3.    0.\n",
      "   2. ], Target: 0\n",
      "Features: [ 67.    1.    4.  160.  286.    0.    2.  108.    1.    1.5   2.    3.\n",
      "   3. ], Target: 1\n",
      "Features: [ 67.    1.    4.  120.  229.    0.    2.  129.    1.    2.6   2.    2.\n",
      "   4. ], Target: 0\n",
      "Features: [ 37.    1.    3.  130.  250.    0.    0.  187.    0.    3.5   3.    0.\n",
      "   3. ], Target: 0\n",
      "Features: [ 41.    0.    2.  130.  204.    0.    2.  172.    0.    1.4   1.    0.\n",
      "   3. ], Target: 0\n"
     ]
    }
   ],
   "source": [
    "train_df = df[:int(len(df)*0.8)]\n",
    "test_df = df[int(len(df)*0.8)+1:]\n",
    "\n",
    "train_target = train_df.pop('target')\n",
    "test_target = test_df.pop('target')\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((train_df.values, train_target.values))\n",
    "test = tf.data.Dataset.from_tensor_slices((test_df.values, test_target.values))\n",
    "\n",
    "for feat, targ in dataset.take(5):\n",
    "    print ('Features: {}, Target: {}'.format(feat, targ))\n",
    "\n",
    "train_dataset = dataset.shuffle(len(train_df)).batch(1)\n",
    "test_dataset = test.batch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model\n",
    "위에서 준비한 Preprocessing Pandas Data로서 Model을 구성하고 결과를 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 4.0793 - accuracy: 0.7762\n",
      "Epoch 2/15\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.0793 - accuracy: 0.7762\n",
      "Epoch 3/15\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.0793 - accuracy: 0.7762\n",
      "Epoch 4/15\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.0793 - accuracy: 0.7762\n",
      "Epoch 5/15\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.0793 - accuracy: 0.7762\n",
      "Epoch 6/15\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.0793 - accuracy: 0.7762\n",
      "Epoch 7/15\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.0793 - accuracy: 0.7762\n",
      "Epoch 8/15\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.0793 - accuracy: 0.7762\n",
      "Epoch 9/15\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.0793 - accuracy: 0.7762\n",
      "Epoch 10/15\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.0793 - accuracy: 0.7762\n",
      "Epoch 11/15\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.0793 - accuracy: 0.7762\n",
      "Epoch 12/15\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.0793 - accuracy: 0.7762\n",
      "Epoch 13/15\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.0793 - accuracy: 0.7762\n",
      "Epoch 14/15\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.0793 - accuracy: 0.7762\n",
      "Epoch 15/15\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.0793 - accuracy: 0.7762\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 4.8846 - accuracy: 0.6833\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4.884567085901896, 0.68333334]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_compiled_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(10, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = get_compiled_model()\n",
    "model.fit(train_dataset, epochs=15)\n",
    "model.evaluate(test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
