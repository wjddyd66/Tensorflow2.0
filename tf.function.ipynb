{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.function\n",
    "Tensorflow 2.0에 맞게 다시 Tensorflow를 살펴볼 필요가 있다고 느껴져서 <a href=\"https://www.tensorflow.org/?hl=ko\">Tensorflow 정식 홈페이지</a>에 나와있는 예제부터 전반적인 Tensorflow 사용법을 먼저 익히는 Post가 된다.  \n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 필요한 Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "import traceback\n",
    "import contextlib\n",
    "\n",
    "# Gpu 사용 가능 여부 출력\n",
    "print(tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is tf.function\n",
    "Tensorflow 2.0에서는 <a href=\"https://wjddyd66.github.io/tnesorflow2.0/Tensorflow2.0(6)/\">Eager execution</a>을 기본적으로 사용한다.  \n",
    "이러한 Eager execution사용으로 인하여 사용자는 쉽게 Debug하면서 Code를 사용할 수 있으나 **성능과 배포(Deployment)가 저하될 수 있다.**  \n",
    "\n",
    "**<code>tf.function</code>이란 위와같은 상황에서 성능을 향상시키고 Model Deployment를 할 수 있다. 또한 Python code와 함께 동작한다.**  \n",
    "단 Guide에서는 다음과 같은 주의사항이 있다고 설명하고 있다.  \n",
    ">- Don't rely on Python side effects like object mutation or list appends.\n",
    "- tf.function works best with TensorFlow ops, rather than NumPy ops or Python primitives.\n",
    "- When in doubt, use the for x in y idiom\n",
    ">\n",
    "\n",
    "위와 같은 주의사항과 왜 <code>tf.function</code>사용을 권장하는지 알아보자.  \n",
    "\n",
    "<br><br>\n",
    "\n",
    "### Basics\n",
    "<code>tf.function</code>은 Tensorflow operation과 같이 정의할 수 있다.  \n",
    "이렇게 정의된 Function은 Eager Execution과 <a href=\"https://wjddyd66.github.io/tnesorflow2.0/Tensorflow2.0(6)/#gradienttape2\">tf.GradientTape()</a>에서 바로 사용 가능하다. 또한 Functions inside Functions또한 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eager Execution Result\n",
      "[[2. 2.]\n",
      " [2. 2.]]\n",
      "\n",
      "Gradient Result\n",
      "1.0\n",
      "\n",
      "Functions inside Functions Result\n",
      "[[3. 3.]\n",
      " [3. 3.]\n",
      " [3. 3.]]\n"
     ]
    }
   ],
   "source": [
    "# tf.function 선언\n",
    "@tf.function\n",
    "def add(a,b):\n",
    "    return a+b\n",
    "\n",
    "# Eager Execution\n",
    "add_result = add(tf.ones([2,2]),tf.ones([2,2]))\n",
    "print('Eager Execution Result')\n",
    "print(add_result.numpy())\n",
    "print()\n",
    "\n",
    "# tf.GradientTape()\n",
    "v = tf.Variable(1.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    result = add(v,1.0)\n",
    "gradient_result = tape.gradient(result,v)\n",
    "print('Gradient Result')\n",
    "print(gradient_result.numpy())\n",
    "print()\n",
    "\n",
    "# Functions inside Functions\n",
    "@tf.function\n",
    "def dense_layer(x,w,b):\n",
    "    return add(tf.matmul(x,w),b)\n",
    "function_inside_function_result = dense_layer(tf.ones([3,2]),\n",
    "                                              tf.ones([2,2]),\n",
    "                                              tf.ones([2]))\n",
    "print('Functions inside Functions Result')\n",
    "print(function_inside_function_result.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracing and polymorphism\n",
    "#### polymorphism\n",
    "Python은 dynamic typing이다. 한가지 예를 들어보면 Python에는 변수선언을 할때 int나 double, string 등으로 변수의 Type을 지정하지 않는다.  \n",
    "Python 내부에서 알아서 알맞게 지정하기 때문이다.  \n",
    "\n",
    "Tensorflow의 기초가 되는 <a href=\"https://wjddyd66.github.io/tnesorflow2.0/Tensorflow2.0(7)/\">Tensor</a> 는 정의할때 반드시 Shape(Dimension)과 자료형(Dtype)이 필요로 하다.  \n",
    "위와 같은 python의 특징을 사용하면 하나의 Function에서 서로다른 Dtype을 받아들이고 처리하는 Function을 정의할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracing with Tensor(\"a:0\", shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "\n",
      "Tracing with Tensor(\"a:0\", shape=(), dtype=float32)\n",
      "tf.Tensor(2.2, shape=(), dtype=float32)\n",
      "\n",
      "Tracing with Tensor(\"a:0\", shape=(), dtype=string)\n",
      "tf.Tensor(b'aa', shape=(), dtype=string)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Polymorphism\n",
    "@tf.function\n",
    "def double(a):\n",
    "    print(\"Tracing with\", a)\n",
    "    return a + a\n",
    "\n",
    "print(double(tf.constant(1)))\n",
    "print()\n",
    "print(double(tf.constant(1.1)))\n",
    "print()\n",
    "print(double(tf.constant(\"a\")))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tracing\n",
    "위에서 <code>tf.function</code>으로서 Polymorphism을 확인하였다.  \n",
    "Tracing이란 <code>tf.function</code>로서 원하는 Assert 구문을 만들어서 원하는 형태로 흘러가는지 확인하는 것이다.  \n",
    "Tracing의 순서는 다음과 같다.  \n",
    "- <code>tf.function</code> 선언\n",
    "- <code>get_concrete_function</code>으로서 Specific Trace 정의\n",
    "- <code>input_signature</code>으로서 tf.function이 호출될때 Error 확인\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracing with Tensor(\"a:0\", dtype=string)\n",
      "Executing traced function\n",
      "tf.Tensor(b'aa', shape=(), dtype=string)\n",
      "tf.Tensor(b'bb', shape=(), dtype=string)\n",
      "\n",
      "Check InvalidArgumentError\n",
      "Caught expected exception \n",
      "  <class 'tensorflow.python.framework.errors_impl.InvalidArgumentError'>:\n",
      "\n",
      "Tracing with Tensor(\"x:0\", shape=(None,), dtype=int32)\n",
      "tf.Tensor([4 1], shape=(2,), dtype=int32)\n",
      "Check ValueError\n",
      "Caught expected exception \n",
      "  <class 'ValueError'>:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-4-17daa06afe8b>\", line 5, in assert_raises\n",
      "    yield\n",
      "  File \"<ipython-input-4-17daa06afe8b>\", line 26, in <module>\n",
      "    double_strings(tf.constant(1))\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: cannot compute __inference_double_90 as input #0(zero-based) was expected to be a string tensor but is a int32 tensor [Op:__inference_double_90]\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-4-17daa06afe8b>\", line 5, in assert_raises\n",
      "    yield\n",
      "  File \"<ipython-input-4-17daa06afe8b>\", line 40, in <module>\n",
      "    next_collatz(tf.constant([[1, 2], [3, 4]]))\n",
      "ValueError: Python inputs incompatible with input_signature:\n",
      "  inputs: (\n",
      "    tf.Tensor(\n",
      "[[1 2]\n",
      " [3 4]], shape=(2, 2), dtype=int32))\n",
      "  input_signature: (\n",
      "    TensorSpec(shape=(None,), dtype=tf.int32, name=None))\n"
     ]
    }
   ],
   "source": [
    "# Error 발생시 지정한 Error_class면 출력\n",
    "@contextlib.contextmanager\n",
    "def assert_raises(error_class):\n",
    "    try:\n",
    "        yield\n",
    "    except error_class as e:\n",
    "        print('Caught expected exception \\n  {}:'.format(error_class))\n",
    "        traceback.print_exc(limit=2)\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "    else:\n",
    "        raise Exception('Expected {} to be raised but no error was raised!'.format(\n",
    "            error_class))\n",
    "\n",
    "# get_concrete_function으로서 Specific Trace정의\n",
    "# Input Tensor의 Dtype이 String인지 확인한다.\n",
    "double_strings = double.get_concrete_function(tf.TensorSpec(shape=None, dtype=tf.string))\n",
    "\n",
    "print('Executing traced function')\n",
    "print(double_strings(tf.constant('a')))\n",
    "print(double_strings(tf.constant('b')))\n",
    "print()\n",
    "# InvalidArgumentError 발생시 Error Message 확인\n",
    "print('Check InvalidArgumentError')\n",
    "with assert_raises(tf.errors.InvalidArgumentError):\n",
    "    double_strings(tf.constant(1))\n",
    "print()\n",
    "\n",
    "# input_signature로서 Tensor가 1Dimension인지 확인\n",
    "@tf.function(input_signature=(tf.TensorSpec(shape=[None], dtype=tf.int32),))\n",
    "def next_collatz(x):\n",
    "    print(\"Tracing with\", x)\n",
    "    return tf.where(x % 2 == 0, x // 2, 3 * x + 1)\n",
    "\n",
    "# 1 Dimension이므로 Error 발생 X\n",
    "print(next_collatz(tf.constant([1, 2])))\n",
    "# 2 Dimension이므로 Error 발생 O\n",
    "print('Check ValueError')\n",
    "with assert_raises(ValueError):\n",
    "    next_collatz(tf.constant([[1, 2], [3, 4]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python or Tensor args?\n",
    "기본적으로 Hyperparameter를 Python의 Argument를 사용하여 지정하였다. 예를들어, dropout_ratio=0.1, learning_rate=0.2처럼 선언하였다.  \n",
    "이러한 Argument가 바뀌게 되면 Static한 Tensorflow Graph를 재정의 하고 Retrace해야 하므로 비효율적이다.(단, Argument가 같으면 새로운 Graph생성 X)  \n",
    "**Tensor args를 활용하여 Graph의 Hyperparameter를 정의하고 사용하게 되면 Tensorflow의 Graph인 AutoGraph는 Dynamical unroll이 될 것이고 다양한 traces에도 불고하고 Graph는 즉시 정의될 것이다.**  \n",
    "아래 예시를 살펴보면 매우 잘 와닿을 수 있다.  \n",
    "Python Argument를 사용하여 Graph를 2번 정의했을 경우 2개의 Graph가 생성되는 것을 볼 수 있다.  \n",
    "Tensor Argument를 사용하면 Graph를 2번 정의하는 것이 아닌 마지막의 TensorArgument를 통하여 정의되게 된다.  \n",
    "Tensor의 특징이다. Tensor Argument를 통하여 Tensorflow의 Graph를 생성하게 되면 하나의 Graph에서 Tensor를 올려두기 때문에 올려둔 Tensor의 값만 바꾸는 거지 Graph를 새롭게 그리는 것이 아니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Argument\n",
      "<class 'int'>\n",
      "Tracing with num_steps=10\n",
      "10\n",
      "10\n",
      "<class 'int'>\n",
      "Tracing with num_steps=20\n",
      "20\n",
      "\n",
      "Tensor Argument\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "Tracing with num_steps=Tensor(\"num_steps:0\", shape=(), dtype=int32)\n",
      "10\n",
      "10\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "def train_one_step():\n",
    "    pass\n",
    "\n",
    "@tf.function\n",
    "def train(num_steps):\n",
    "    print(type(num_steps))\n",
    "    tf.print(num_steps)\n",
    "    print('Tracing with num_steps={}'.format(num_steps))\n",
    "    for _ in tf.range(num_steps):\n",
    "        train_one_step()\n",
    "\n",
    "# Python Argument 사용\n",
    "print('Python Argument')\n",
    "train(num_steps=10)\n",
    "# 값이 같은 Argument사용시 Graph생성 X\n",
    "train(num_steps=10)\n",
    "train(num_steps=20)\n",
    "print()\n",
    "\n",
    "# Tensor Argument 사용\n",
    "# Python Argument를 Tensor로 Casting하여 사용\n",
    "print('Tensor Argument')\n",
    "# Graph는 한번만 생성되고 Graph안에서의 Tensor값만 바뀌게 된다.\n",
    "train(num_steps=tf.constant(10))\n",
    "train(num_steps=tf.constant(10))\n",
    "train(num_steps=tf.constant(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beware of Python state\n",
    "Generator나 iterator아 같은 연산자들은 Python runtime에만 의존하게 된다. 따라서 **Eager Execution으로 살펴보는 경우 문제가 발생하지 않지만 실제 <code>tf.function</code>안에서 사용하는 예상치못한 문제가 발생할 수 있다.**  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.function iterator\n",
      "Value of external_var: 0\n",
      "Value of external_var: 0\n",
      "Value of external_var: 0\n",
      "\n",
      "Python iterator\n",
      "Value of external_var: 1\n",
      "Value of external_var: 3\n",
      "Value of external_var: 6\n"
     ]
    }
   ],
   "source": [
    "external_var = tf.Variable(0)\n",
    "\n",
    "@tf.function\n",
    "def buggy_consume_next(iterator):\n",
    "    external_var.assign_add(next(iterator))\n",
    "    tf.print(\"Value of external_var:\", external_var)\n",
    "\n",
    "iterator = iter([0, 1, 2, 3])\n",
    "\n",
    "print('tf.function iterator')\n",
    "buggy_consume_next(iterator)\n",
    "# This reuses the first value from the iterator, rather than consuming the next value.\n",
    "buggy_consume_next(iterator)\n",
    "buggy_consume_next(iterator)\n",
    "print()\n",
    "\n",
    "def buggy_consume_next(iterator):\n",
    "    external_var.assign_add(next(iterator))\n",
    "    tf.print(\"Value of external_var:\", external_var)\n",
    "\n",
    "print('Python iterator')\n",
    "buggy_consume_next(iterator)\n",
    "buggy_consume_next(iterator)\n",
    "buggy_consume_next(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iterator가 <code>tf.function</code>안에서 생성되고 수행되어지면 정확히 작동 될 것이다. 따라서 iterator를 구현하기 위해서 Function안에서 <code>x in y</code> 처럼 계속해서 반복하는 수 밖에 없고 그렇게 되면 **Large in-memory Dataset**이 되면서 감당할 수 없을 것이다.  \n",
    "따라서 이러한 Iterator형식의 Dataset을 사용하기 위해서는 **<code>tf.data.Dataset.from_generator()</code>** 을 사용하여 Python data를 wrap해야 한다.  \n",
    "아래 Code와 결과를 살펴보면 쉽게 이해 된다.  \n",
    "간단한 train(), train2() <code>tf.function</code>은 dummy computation을 실시하게 된다.  \n",
    "**Wrap하지 않은 연산자는 Data의 Size가 커지거나 연산이 추가될 수록 지속적으로 Node가 생성되면서 Graph가 커지는 반면 <code>tf.data.Dataset.from_generator()</code>Wrap을 실시할 시 Graph의 Size는 일정한 것을 확인할 수 있다.**  \n",
    "\n",
    "**참조(tf.data.Dataset.from_generator)**  \n",
    ">The generator argument must be a callable object that returns an object that supports the iter() protocol (e.g. a generator function). The elements generated by generator must be compatible with the given output_types and (optional) output_shapes arguments.\n",
    "\n",
    "한 Object를 받아서 iter()을 적용할 수 있다.\n",
    "\n",
    "참조: <a href=\"https://www.tensorflow.org/api_docs/python/tf/data/Dataset?version=stable#__init__\">tf.data 설명서</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Wrap - train()\n",
      "train([(1, 1), (1, 1)]) contains 8 nodes in its graph\n",
      "train([(1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1)]) contains 32 nodes in its graph\n",
      "\n",
      "No Wrap - train2()\n",
      "train2([(1, 1), (1, 1)]) contains 18 nodes in its graph\n",
      "train2([(1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1)]) contains 82 nodes in its graph\n",
      "\n",
      "Wrap - train()\n",
      "train(<DatasetV1Adapter shapes: (<unknown>, <unknown>), types: (tf.int32, tf.int32)>) contains 5 nodes in its graph\n",
      "train(<DatasetV1Adapter shapes: (<unknown>, <unknown>), types: (tf.int32, tf.int32)>) contains 5 nodes in its graph\n",
      "\n",
      "Wrap - train2()\n",
      "train2(<DatasetV1Adapter shapes: (<unknown>, <unknown>), types: (tf.int32, tf.int32)>) contains 5 nodes in its graph\n",
      "train2(<DatasetV1Adapter shapes: (<unknown>, <unknown>), types: (tf.int32, tf.int32)>) contains 5 nodes in its graph\n"
     ]
    }
   ],
   "source": [
    "def measure_graph_size(f, *args):\n",
    "    g = f.get_concrete_function(*args).graph\n",
    "    print(\"{}({}) contains {} nodes in its graph\".format(\n",
    "        f.__name__, ', '.join(map(str, args)), \n",
    "        len(g.as_graph_def().node)))\n",
    "\n",
    "@tf.function\n",
    "def train(dataset):\n",
    "    loss = tf.constant(0)\n",
    "    for x, y in dataset:\n",
    "        loss += tf.abs(y - x) # Some dummy computation.\n",
    "    return loss\n",
    "\n",
    "@tf.function\n",
    "def train2(dataset):\n",
    "    loss = tf.constant(0)\n",
    "    for x, y in dataset:\n",
    "        loss += tf.abs(y - x) # Some dummy computation.\n",
    "        loss += tf.abs(y - tf.constant(0)) # Some dummy computation.\n",
    "    return loss\n",
    "\n",
    "small_data = [(1, 1)] * 2\n",
    "big_data = [(1, 1)] * 10\n",
    "\n",
    "print('No Wrap - train()')\n",
    "measure_graph_size(train, small_data)\n",
    "measure_graph_size(train, big_data)\n",
    "print()\n",
    "\n",
    "print('No Wrap - train2()')\n",
    "measure_graph_size(train2, small_data)\n",
    "measure_graph_size(train2, big_data)\n",
    "print()\n",
    "\n",
    "print('Wrap - train()')\n",
    "measure_graph_size(train, tf.data.Dataset.from_generator(\n",
    "    lambda: small_data, (tf.int32, tf.int32)))\n",
    "measure_graph_size(train, tf.data.Dataset.from_generator(\n",
    "    lambda: big_data, (tf.int32, tf.int32)))\n",
    "print()\n",
    "\n",
    "print('Wrap - train2()')\n",
    "measure_graph_size(train2, tf.data.Dataset.from_generator(\n",
    "    lambda: small_data, (tf.int32, tf.int32)))\n",
    "measure_graph_size(train2, tf.data.Dataset.from_generator(\n",
    "    lambda: big_data, (tf.int32, tf.int32)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Control Dependencies\n",
    "동일한 변수에 대하여 여러번 읽고 쓰는 것은 사용자가 원하는 flow대로 흐르지 않을 수 있다.  \n",
    "tf.function을 사용하면 의도한 flow대로 자연스럽게 Code를 실행하는 것을 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: 1.0, y: 2.0\n",
      "a:  4 b:  6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=609, shape=(), dtype=float32, numpy=10.0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Automatic control dependencies\n",
    "\n",
    "a = tf.Variable(1.0)\n",
    "b = tf.Variable(2.0)\n",
    "\n",
    "@tf.function\n",
    "def f(x, y):\n",
    "    print('x: {}, y: {}'.format(x,y))\n",
    "    a.assign(y * b) # 2*2\n",
    "    b.assign_add(x * a) # 2 + 1*4\n",
    "    tf.print('a: ',a,'b: ',b)\n",
    "    return a + b # 4+6\n",
    "\n",
    "f(1.0, 2.0)  # 10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables\n",
    "tf.function안에 tf.Variable을 생성할 경우 tf.function에서는 호출될 시 같은 variable로서 reuse되겠지만, eager mode에서는 호출될 시 각각 새로운 변수를 생성하게 될 것이다.  \n",
    "tf.function은 사전에 이러한 Ambiguous한 구문을 사용하지 못하도록 막고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambiguous Code\n",
      "WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Caught expected exception \n",
      "  <class 'ValueError'>:\n",
      "\n",
      "Non-ambiguous code\n",
      "tf.Tensor(2.0, shape=(), dtype=float32)\n",
      "tf.Tensor(4.0, shape=(), dtype=float32)\n",
      "\n",
      "Object Variable\n",
      "tf.Tensor(2.0, shape=(), dtype=float32)\n",
      "tf.Tensor(4.0, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-4-17daa06afe8b>\", line 5, in assert_raises\n",
      "    yield\n",
      "  File \"<ipython-input-9-bde50482edbb>\", line 10, in <module>\n",
      "    f(1.0)\n",
      "ValueError: in converted code:\n",
      "\n",
      "    <ipython-input-9-bde50482edbb>:5 f  *\n",
      "        v = tf.Variable(1.0)\n",
      "    /root/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py:260 __call__\n",
      "        return cls._variable_v2_call(*args, **kwargs)\n",
      "    /root/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py:254 _variable_v2_call\n",
      "        shape=shape)\n",
      "    /root/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py:65 getter\n",
      "        return captured_getter(captured_previous, **kwargs)\n",
      "    /root/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py:413 invalid_creator_scope\n",
      "        \"tf.function-decorated function tried to create \"\n",
      "\n",
      "    ValueError: tf.function-decorated function tried to create variables on non-first call.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ambiguous Code\n",
    "print('Ambiguous Code')\n",
    "@tf.function\n",
    "def f(x):\n",
    "    v = tf.Variable(1.0)\n",
    "    v.assign_add(x)\n",
    "    return v\n",
    "\n",
    "with assert_raises(ValueError):\n",
    "    f(1.0)\n",
    "print()\n",
    "\n",
    "# Non-ambiguous code\n",
    "print('Non-ambiguous code')\n",
    "v = tf.Variable(1.0)\n",
    "@tf.function\n",
    "def f(x):\n",
    "    return v.assign_add(x)\n",
    "print(f(1.0))\n",
    "print(f(2.0))\n",
    "print()\n",
    "\n",
    "# 하나의 Object의 변수로서 tf.Variable 사용 가능\n",
    "print('Object Variable')\n",
    "class C:\n",
    "    pass\n",
    "obj = C()\n",
    "obj.v = None\n",
    "\n",
    "@tf.function\n",
    "def g(x):\n",
    "    if obj.v is None:\n",
    "        obj.v = tf.Variable(1.0)\n",
    "    return obj.v.assign_add(x)\n",
    "print(g(1.0))\n",
    "print(g(2.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using AutoGraph\n",
    "AutoGraph와 관련된 라이브러리는 tf.function과 완전히 통일되어있다.  \n",
    "Tensorflow의 Graph안에서의 반복이나 조건은 tf.wile_loop나 tf.cond로서 작성하는 것이 맞으나 이럴 경우 매우 복잡해지고 익숙한 제어와 조건문을 사용하길 원할 것 이다.  \n",
    "**AutoGraph는 이러한 if같이 조건이나 for같이 반복문을 자동으로 Convert해서 Code를 실행한다.**  \n",
    "<code>tf.autograph.to_code(function)</code>: 기존 Python Function을 tf.function으로 바꿀시 만드는 Code => tf.function이 어떻게 정의되어지는지 이해하기 편하라고 만들어둔 기능이나, 알아보기 매우 힘들다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.520907283 0.82572329 0.908115149 0.760388494 0.711923599]\n",
      "[0.478399932 0.678172946 0.720226347 0.641305745 0.611881673]\n",
      "[0.44496125 0.590330303 0.617049456 0.565788 0.54545027]\n",
      "[0.417748362 0.530133128 0.549070358 0.512259245 0.497102529]\n",
      "[0.395031869 0.485482842 0.499823153 0.471703619 0.45983538]\n",
      "[0.375690043 0.45062387 0.461978048 0.439574778 0.429950058]\n",
      "[0.358958662 0.42241171 0.431695 0.413291931 0.405279577]\n",
      "[0.344296455 0.398960233 0.40673691 0.39126426 0.38445729]\n",
      "[0.331307679 0.379058957 0.385698527 0.372449636 0.366572112]\n",
      "[0.319695294 0.361889958 0.367646068 0.356132507 0.350989729]\n",
      "[0.309231371 0.346877664 0.351931036 0.341802895 0.337252975]\n",
      "[0.299737662 0.333603591 0.338086963 0.329085976 0.325022757]\n",
      "[0.291072518 0.321755022 0.325768441 0.317699224 0.314041436]\n",
      "[0.283121645 0.311092943 0.314713418 0.307425052 0.304109246]\n",
      "[0.275791764 0.30143106 0.304718971 0.298092753 0.295068622]\n",
      "[0.269005805 0.292621672 0.29562515 0.289566249 0.286793262]\n",
      "[0.262699485 0.284546 0.287303925 0.281735539 0.279180646]\n",
      "[0.256818712 0.277107239 0.279651463 0.274510592 0.272146583]\n",
      "[0.251317561 0.270225644 0.272582471 0.267816931 0.265621096]\n",
      "[0.246156782 0.263834774 0.266026169 0.261592329 0.259545565]\n",
      "[0.241302595 0.25787881 0.259923309 0.255784273 0.253870428]\n",
      "[0.236725733 0.252310455 0.254223794 0.25034821 0.248553455]\n",
      "[0.232400686 0.247089297 0.248884961 0.245245948 0.243558407]\n",
      "[0.228305146 0.242180616 0.243870214 0.240444601 0.238853991]\n",
      "[0.224419475 0.237554371 0.239147976 0.235915646 0.234413]\n",
      "[0.220726296 0.233184427 0.234690815 0.231634215 0.23021169]\n",
      "[0.217210203 0.229047909 0.23047477 0.227578506 0.226229221]\n",
      "[0.213857457 0.225124717 0.226478815 0.223729312 0.222447187]\n",
      "[0.210655764 0.221397072 0.222684413 0.220069662 0.218849286]\n",
      "[0.207594097 0.21784924 0.219075143 0.216584459 0.215421021]\n",
      "[0.204662517 0.214467183 0.215636387 0.213260248 0.212149441]\n",
      "[0.201852053 0.211238354 0.212355107 0.21008499 0.209022954]\n",
      "[0.199154571 0.208151504 0.20921962 0.20704785 0.206031114]\n",
      "[0.196562693 0.205196515 0.206219435 0.204139084 0.203164518]\n",
      "[0.194069698 0.202364236 0.20334506 0.201349899 0.200414658]\n",
      "def tf__f(x):\n",
      "  do_return = False\n",
      "  retval_ = ag__.UndefinedReturnValue()\n",
      "  with ag__.FunctionScope('f', 'f_scope', ag__.ConversionOptions(recursive=True, user_requested=True, optional_features=(), internal_convert_user_code=True)) as f_scope:\n",
      "\n",
      "    def get_state():\n",
      "      return ()\n",
      "\n",
      "    def set_state(_):\n",
      "      pass\n",
      "\n",
      "    def loop_body(x):\n",
      "      ag__.converted_call(tf.print, f_scope.callopts, (x,), None, f_scope)\n",
      "      x = ag__.converted_call(tf.tanh, f_scope.callopts, (x,), None, f_scope)\n",
      "      return x,\n",
      "\n",
      "    def loop_test(x):\n",
      "      return ag__.converted_call(tf.reduce_sum, f_scope.callopts, (x,), None, f_scope) > 1\n",
      "    x, = ag__.while_stmt(loop_test, loop_body, get_state, set_state, (x,), ('x',), ())\n",
      "    do_return = True\n",
      "    retval_ = f_scope.mark_return_value(x)\n",
      "  do_return,\n",
      "  return ag__.retval(retval_)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Simple loop\n",
    "\n",
    "@tf.function\n",
    "def f(x):\n",
    "    while tf.reduce_sum(x) > 1:\n",
    "        tf.print(x)\n",
    "        x = tf.tanh(x)\n",
    "    return x\n",
    "\n",
    "f(tf.random.uniform([5]))\n",
    "\n",
    "def f(x):\n",
    "    while tf.reduce_sum(x) > 1:\n",
    "        tf.print(x)\n",
    "        x = tf.tanh(x)\n",
    "    return x\n",
    "\n",
    "print(tf.autograph.to_code(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AutoGraph: Conditionals\n",
    "<a href=\"https://www.tensorflow.org/api_docs/python/tf/cond?version=stable\">tf.cond</a>를 살펴보게 되면 **Condition을 비교하여 각각의 Function을 적용할 수 있다.**  \n",
    "아래 tf.cond() Example을 살펴보게 되면 x < y 이기 때문에 True가 되고 따라서 f1이 실행됨에 따라서 2 * 17의 결과인 37을 출력하게 된다.  \n",
    "\n",
    "또한 Tensor로 Casting된 True는 tf.cond로서 비교하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.cond() Example\n",
      "34\n",
      "\n",
      "if vs tf.cond\n",
      "Use Parameter\n",
      "dropout(tf.Tensor([1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], shape=(10,), dtype=float32), True) executes normally.\n",
      "  result:  [2. 2. 0. 2. 0. 2. 2. 2. 0. 0.]\n",
      "\n",
      "Use Tensor(tf.constant(True))\n",
      "dropout(tf.Tensor([1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], shape=(10,), dtype=float32), tf.Tensor(True, shape=(), dtype=bool)) uses tf.cond.\n",
      "  result:  [0. 2. 0. 2. 0. 0. 2. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# tf.cond() Example\n",
    "print('tf.cond() Example')\n",
    "x = tf.constant(2)\n",
    "y = tf.constant(5)\n",
    "def f1(): return tf.multiply(x, 17)\n",
    "def f2(): return tf.add(y, 23)\n",
    "r = tf.cond(tf.less(x, y), f1, f2)\n",
    "print(r.numpy())\n",
    "print()\n",
    "\n",
    "# Python IF 문인지 tf.cond인지 판단하는 Function\n",
    "def test_tf_cond(f, *args):\n",
    "    g = f.get_concrete_function(*args).graph\n",
    "    if any(node.name == 'cond' for node in g.as_graph_def().node):\n",
    "        print(\"{}({}) uses tf.cond.\".format(\n",
    "            f.__name__, ', '.join(map(str, args))))\n",
    "    else:\n",
    "        print(\"{}({}) executes normally.\".format(\n",
    "            f.__name__, ', '.join(map(str, args))))\n",
    "\n",
    "    print(\"  result: \",f(*args).numpy())\n",
    "    \n",
    "@tf.function\n",
    "def dropout(x, training=True):\n",
    "    if training:\n",
    "        x = tf.nn.dropout(x, rate=0.5)\n",
    "    return x\n",
    "\n",
    "print('if vs tf.cond')\n",
    "print('Use Parameter')\n",
    "test_tf_cond(dropout, tf.ones([10], dtype=tf.float32), True)\n",
    "print()\n",
    "print('Use Tensor(tf.constant(True))')\n",
    "test_tf_cond(dropout, tf.ones([10], dtype=tf.float32), tf.constant(True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">tf.function</span>에서 매우 중요한 부분이다.  \n",
    "\n",
    "**<code>tf.function</code>에서 비교하는 값이 Tensor이게 되면 if나 else와 같은 조건문을 tf.cond로서 판별하게 된다.**  \n",
    "\n",
    "**중요한점은 이러한 tf.cond는 일반적인 Python처럼 하나의 조건(if or else)만 살펴보는 것이 아니라 양쪽 다 살펴본 뒤 하나를 선택하게 된다.**  \n",
    "\n",
    "<span style=\"color:red\">**따라서 tf.constant(True)와 같이 Tensor로 감싼 True를 조건문으로서 판단하게 되면 꼭 if, else를 같이 사용하여야 한다.**</span>  \n",
    "\n",
    "**마지막으로 Tensor로 감싼 True는 Python에서 True와 다르기 때문에 bool()같은 Type으로서 Casting시 TypeError가 발생한다.**  \n",
    "\n",
    "기본적인 Python 문법과 같다고 생각하고 Code를 작성하게 되면 값은 일정하나 Code안에서 print()와 같이 즉시 시행되는 것에서 예상치 못하는 Error가 발생할 수 있다.  \n",
    "최종적으로 정리하면 다음과 같다.\n",
    "- 비교하는 값이 Tensor이게 되면 if나 else와 같은 조건문을 tf.cond로서 판별하게 된다.\n",
    "- tf.cond는 일반적인 Python처럼 하나의 조건(if or else)만 살펴보는 것이 아니라 양쪽 다 살펴본 뒤 하나를 선택하게 된다.\n",
    "- Tensor로 감싼 True는 Python에서 True와 다르기 때문에 bool()같은 Type으로서 Casting시 TypeError가 발생한다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use Python Parameter\n",
      "Tracing `else` branch\n",
      "-2.0\n",
      "Tracing `then` branch\n",
      "2.0\n",
      "\n",
      "Use Tensor Parameter\n",
      "Tracing `then` branch\n",
      "Tracing `else` branch\n",
      "2.0\n",
      "\n",
      "Use If & Else\n",
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "\n",
      "Use only If\n",
      "Caught expected exception \n",
      "  <class 'ValueError'>:\n",
      "\n",
      "Bool Casting\n",
      "Using True or False\n",
      "Tracing `then` branch\n",
      "1.0\n",
      "Tracing `else` branch\n",
      "-1.0\n",
      "\n",
      "Using tf.constant(True)\n",
      "Caught expected exception \n",
      "  <class 'TypeError'>:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-4-17daa06afe8b>\", line 5, in assert_raises\n",
      "    yield\n",
      "  File \"<ipython-input-12-d239ff66395f>\", line 43, in <module>\n",
      "    f()\n",
      "ValueError: in converted code:\n",
      "\n",
      "    <ipython-input-12-d239ff66395f>:36 f  *\n",
      "        if tf.constant(True):\n",
      "    /root/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/autograph/operators/control_flow.py:893 if_stmt\n",
      "        basic_symbol_names, composite_symbol_names)\n",
      "    /root/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/autograph/operators/control_flow.py:931 tf_if_stmt\n",
      "        error_checking_orelse)\n",
      "    /root/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py:507 new_func\n",
      "        return func(*args, **kwargs)\n",
      "    /root/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/control_flow_ops.py:1174 cond\n",
      "        return cond_v2.cond_v2(pred, true_fn, false_fn, name)\n",
      "    /root/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/cond_v2.py:91 cond_v2\n",
      "        op_return_value=pred)\n",
      "    /root/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py:915 func_graph_from_py_func\n",
      "        func_outputs = python_func(*func_args, **func_kwargs)\n",
      "    /root/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/autograph/operators/control_flow.py:924 error_checking_orelse\n",
      "        result[orelse_branch] = orelse()\n",
      "    /root/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/autograph/operators/control_flow.py:962 wrapper\n",
      "        new_vars = func()\n",
      "    /root/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/autograph/operators/control_flow.py:988 wrapper\n",
      "        tuple(s.symbol_name for s in undefined)))\n",
      "\n",
      "    ValueError: The following symbols must also be initialized in the else branch: ('x',). Alternatively, you may initialize them before the if statement.\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-4-17daa06afe8b>\", line 5, in assert_raises\n",
      "    yield\n",
      "  File \"<ipython-input-12-d239ff66395f>\", line 65, in <module>\n",
      "    f(tf.constant(True), 0.0)\n",
      "tensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: in converted code:\n",
      "\n",
      "    <ipython-input-12-d239ff66395f>:49 f  *\n",
      "        if bool(x):\n",
      "    /root/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py:396 converted_call\n",
      "        return py_builtins.overload_of(f)(*args)\n",
      "    /root/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:765 __bool__\n",
      "        self._disallow_bool_casting()\n",
      "    /root/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:531 _disallow_bool_casting\n",
      "        \"using a `tf.Tensor` as a Python `bool`\")\n",
      "    /root/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:518 _disallow_when_autograph_enabled\n",
      "        \" decorating it directly with @tf.function.\".format(task))\n",
      "\n",
      "    OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed: AutoGraph did not convert this function. Try decorating it directly with @tf.function.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def f(x):\n",
    "    if x > 0:\n",
    "        x = x + 1.\n",
    "        print(\"Tracing `then` branch\")\n",
    "    else:\n",
    "        x = x - 1.\n",
    "        print(\"Tracing `else` branch\")\n",
    "    return x\n",
    "\n",
    "print('Use Python Parameter')\n",
    "print(f(-1.0).numpy())\n",
    "print(f(1.0).numpy())\n",
    "print()\n",
    "\n",
    "print('Use Tensor Parameter')\n",
    "print(f(tf.constant(1.0)).numpy())\n",
    "print()\n",
    "\n",
    "@tf.function\n",
    "def f():\n",
    "    x = None\n",
    "    if tf.constant(True):\n",
    "        x = tf.ones([3, 3])\n",
    "    else:\n",
    "        x = tf.ones([2,2])\n",
    "    return x\n",
    "\n",
    "result = f()\n",
    "print('Use If & Else')\n",
    "print(result.numpy())\n",
    "print()\n",
    "    \n",
    "@tf.function\n",
    "def f():\n",
    "    if tf.constant(True):\n",
    "        x = tf.ones([3, 3])\n",
    "    return x\n",
    "\n",
    "# Throws an error because both branches need to define `x`.\n",
    "print('Use only If')\n",
    "with assert_raises(ValueError):\n",
    "    f()\n",
    "print()\n",
    "\n",
    "# Bool Casting\n",
    "@tf.function\n",
    "def f(x, y):\n",
    "    if bool(x):\n",
    "        y = y + 1.\n",
    "        print(\"Tracing `then` branch\")\n",
    "    else:\n",
    "        y = y - 1.\n",
    "        print(\"Tracing `else` branch\")\n",
    "    return y\n",
    "\n",
    "print('Bool Casting')\n",
    "print('Using True or False')\n",
    "print(f(True, 0).numpy())\n",
    "print(f(False, 0).numpy())\n",
    "print()\n",
    "\n",
    "print('Using tf.constant(True)')\n",
    "with assert_raises(TypeError):\n",
    "    f(tf.constant(True), 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AutoGraph: Loop\n",
    "AutoGraph는 Loop구문을 다음과 같이 Convert한다.\n",
    "- for: Convert if the iterable is a tensor\n",
    "- while: Convert if the while condition depends on a tensor\n",
    "\n",
    "위의 Convert를 자세히 알아보면 다음과 같다.  \n",
    "- 반복문의 범위가 Tensor로 주어진다. -> <a href=\"https://www.tensorflow.org/api_docs/python/tf/while_loop?version=stable\">tf.while_loop</a>사용\n",
    "- for x in tf.data.Dataset 으로 주어딘다. -> <a href=\"https://www.tensorflow.org/api_docs/python/tf/data/Dataset?version=stable#reduce\">tf.data.Dataset,redyce</a>사용\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기본적인 반복문 사용\n",
      "for_in_range() gets unrolled.\n",
      "\n",
      "반복문의 범위가 Tensor로서 사용\n",
      "for_in_tfrange() uses tf.while_loop.\n",
      "\n",
      "for x in tf.data.Dataset 형태\n",
      "for_in_tfdataset() uses tf.data.Dataset.reduce.\n",
      "\n",
      "반복문 + 조건문\n",
      "while_py_cond() gets unrolled.\n",
      "\n",
      "반복문(범위 Tensor) + 조건문(변수 Tensor)\n",
      "while_tf_cond() uses tf.while_loop.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# AutoGraph에서 Convert하는 Loop구문 확인\n",
    "def test_dynamically_unrolled(f,*args):\n",
    "    g = f.get_concrete_function(*args).graph\n",
    "    if any(node.name == 'while' for node in g.as_graph_def().node):\n",
    "        # 반복문의 범위가 Tensor로서 주어짐\n",
    "        print(\"{}({}) uses tf.while_loop.\".format(\n",
    "            f.__name__, ', '.join(map(str, args))))\n",
    "    elif any(node.name == 'ReduceDataset' for node in g.as_graph_def().node):\n",
    "        # for x in tf.data.Dataset \n",
    "        print(\"{}({}) uses tf.data.Dataset.reduce.\".format(\n",
    "            f.__name__, ', '.join(map(str, args))))\n",
    "    else:\n",
    "        # 기본적인 반복문 사용\n",
    "        print(\"{}({}) gets unrolled.\".format(\n",
    "            f.__name__, ', '.join(map(str, args))))\n",
    "    print()\n",
    "        \n",
    "# 기본적인 반복문 사용\n",
    "@tf.function\n",
    "def for_in_range():\n",
    "    x = 0\n",
    "    for i in range(5):\n",
    "        x += i\n",
    "    return x\n",
    "print('기본적인 반복문 사용')\n",
    "test_dynamically_unrolled(for_in_range)\n",
    "\n",
    "# 반복문의 범위가 Tensor로서 사용\n",
    "@tf.function\n",
    "def for_in_tfrange():\n",
    "    x = tf.constant(0, dtype=tf.int32)\n",
    "    for i in tf.range(5):\n",
    "        x += i\n",
    "    return x\n",
    "print('반복문의 범위가 Tensor로서 사용')\n",
    "test_dynamically_unrolled(for_in_tfrange)\n",
    "\n",
    "# for x in tf.data.Dataset 형태\n",
    "@tf.function\n",
    "def for_in_tfdataset():\n",
    "    x = tf.constant(0, dtype=tf.int64)\n",
    "    for i in tf.data.Dataset.range(5):\n",
    "        x += i\n",
    "    return x\n",
    "print('for x in tf.data.Dataset 형태')\n",
    "test_dynamically_unrolled(for_in_tfdataset)\n",
    "\n",
    "# 반복문 + 조건문\n",
    "@tf.function\n",
    "def while_py_cond():\n",
    "    x = 5\n",
    "    while x > 0:\n",
    "        x -= 1\n",
    "    return x\n",
    "print('반복문 + 조건문')\n",
    "test_dynamically_unrolled(while_py_cond)\n",
    "\n",
    "# 반복문(범위 Tensor) + 조건문(변수 Tensor)\n",
    "@tf.function\n",
    "def while_tf_cond():\n",
    "    x = tf.constant(5)\n",
    "    while x > 0:\n",
    "        x -= 1\n",
    "    return x\n",
    "print('반복문(범위 Tensor) + 조건문(변수 Tensor)')\n",
    "test_dynamically_unrolled(while_tf_cond)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
